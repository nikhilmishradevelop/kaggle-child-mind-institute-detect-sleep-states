{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33b94e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.metric_fast' from '/home/sleep-kaggle/kaggle_submit_solution/nbs/../src/metric_fast.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src import metric_fast\n",
    "import joblib\n",
    "import time\n",
    "import importlib\n",
    "importlib.reload(metric_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e630b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip freeze > ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f58ab388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    samp_freq=1\n",
    "    gaussian_overlay_len = 60\n",
    "    std_dev_num = 2400\n",
    "    ver='fm-v13-final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee9a3116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_events_path': '../data/train_events.csv', 'train_series_path': '../data/train_series.parquet', 'processed_data_path': '../data_processed_models', 'models_path': '../outputs'}\n",
      "{'__module__': '__main__', 'samp_freq': 1, 'gaussian_overlay_len': 60, 'std_dev_num': 2400, 'output_dir': '../outputs', 'ver': 'fm-v13-final', '__dict__': <attribute '__dict__' of 'cfg' objects>, '__weakref__': <attribute '__weakref__' of 'cfg' objects>, '__doc__': None, 'train_events_path': '../data/train_events.csv', 'train_series_path': '../data/train_series.parquet', 'processed_data_path': '../data_processed_models', 'models_path': '../outputs'}\n"
     ]
    }
   ],
   "source": [
    "settings_json = json.load(open('../settings.json', 'r'))\n",
    "print(settings_json)\n",
    "\n",
    "for k,v in settings_json.items():\n",
    "    setattr(cfg, k, v)\n",
    "    \n",
    "print(cfg.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f380211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 3.3 ms, total: 3.3 ms\n",
      "Wall time: 3.28 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not mmap compressed IPC file, defaulting to normal read. Toggle off 'memory_map' to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_events = pl.read_ipc(os.path.join(cfg.processed_data_path, 'train_events.ipc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a58c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_df = train_events[['series_id']].unique(maintain_order=True)\n",
    "splits_df = splits_df.to_pandas()\n",
    "\n",
    "for n_splits in [5, 7, 10]:\n",
    "    folds = KFold(n_splits, shuffle=True, random_state=5)\n",
    "\n",
    "    splits_df[f'{n_splits}_fold'] = 0\n",
    "    for i, (trn_idx, val_idx) in enumerate(folds.split(splits_df['series_id'], splits_df['series_id'])):\n",
    "        \n",
    "        splits_df.loc[val_idx, f'{n_splits}_fold'] = i+1\n",
    "        \n",
    "        \n",
    "splits_df = pl.DataFrame(splits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deb1b8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (264, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>series_id</th><th>5_fold</th><th>7_fold</th><th>10_fold</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;038441c925bb&quot;</td><td>5</td><td>6</td><td>9</td></tr><tr><td>&quot;03d92c9f6f8a&quot;</td><td>2</td><td>3</td><td>4</td></tr><tr><td>&quot;0402a003dae9&quot;</td><td>5</td><td>6</td><td>9</td></tr><tr><td>&quot;04f547b8017d&quot;</td><td>1</td><td>1</td><td>2</td></tr><tr><td>&quot;05e1944c3818&quot;</td><td>3</td><td>4</td><td>6</td></tr><tr><td>&quot;062cae666e2a&quot;</td><td>5</td><td>6</td><td>9</td></tr><tr><td>&quot;062dbd4c95e6&quot;</td><td>1</td><td>1</td><td>2</td></tr><tr><td>&quot;08db4255286f&quot;</td><td>5</td><td>7</td><td>10</td></tr><tr><td>&quot;0a96f4993bd7&quot;</td><td>5</td><td>7</td><td>10</td></tr><tr><td>&quot;0cd1e3d0ed95&quot;</td><td>4</td><td>5</td><td>7</td></tr><tr><td>&quot;0cfc06c129cc&quot;</td><td>3</td><td>4</td><td>6</td></tr><tr><td>&quot;0d0ad1e77851&quot;</td><td>4</td><td>6</td><td>8</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;f2c2436cf7b7&quot;</td><td>2</td><td>3</td><td>3</td></tr><tr><td>&quot;f564985ab692&quot;</td><td>1</td><td>1</td><td>2</td></tr><tr><td>&quot;f56824b503a0&quot;</td><td>5</td><td>6</td><td>9</td></tr><tr><td>&quot;f6d2cc003183&quot;</td><td>4</td><td>6</td><td>8</td></tr><tr><td>&quot;f7eb179216c2&quot;</td><td>1</td><td>1</td><td>1</td></tr><tr><td>&quot;f88e18cb4100&quot;</td><td>5</td><td>7</td><td>10</td></tr><tr><td>&quot;f981a0805fd0&quot;</td><td>2</td><td>3</td><td>4</td></tr><tr><td>&quot;fa149c3c4bde&quot;</td><td>4</td><td>5</td><td>7</td></tr><tr><td>&quot;fb223ed2278c&quot;</td><td>2</td><td>3</td><td>4</td></tr><tr><td>&quot;fbf33b1a2c10&quot;</td><td>4</td><td>5</td><td>7</td></tr><tr><td>&quot;fcca183903b7&quot;</td><td>4</td><td>5</td><td>7</td></tr><tr><td>&quot;fe90110788d2&quot;</td><td>5</td><td>6</td><td>9</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (264, 4)\n",
       "┌──────────────┬────────┬────────┬─────────┐\n",
       "│ series_id    ┆ 5_fold ┆ 7_fold ┆ 10_fold │\n",
       "│ ---          ┆ ---    ┆ ---    ┆ ---     │\n",
       "│ str          ┆ i64    ┆ i64    ┆ i64     │\n",
       "╞══════════════╪════════╪════════╪═════════╡\n",
       "│ 038441c925bb ┆ 5      ┆ 6      ┆ 9       │\n",
       "│ 03d92c9f6f8a ┆ 2      ┆ 3      ┆ 4       │\n",
       "│ 0402a003dae9 ┆ 5      ┆ 6      ┆ 9       │\n",
       "│ 04f547b8017d ┆ 1      ┆ 1      ┆ 2       │\n",
       "│ …            ┆ …      ┆ …      ┆ …       │\n",
       "│ fb223ed2278c ┆ 2      ┆ 3      ┆ 4       │\n",
       "│ fbf33b1a2c10 ┆ 4      ┆ 5      ┆ 7       │\n",
       "│ fcca183903b7 ┆ 4      ┆ 5      ┆ 7       │\n",
       "│ fe90110788d2 ┆ 5      ┆ 6      ┆ 9       │\n",
       "└──────────────┴────────┴────────┴─────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68262be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02d23411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory):\n",
    "    X_s = []\n",
    "    y_s = []\n",
    "    series_ids = []\n",
    "\n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        if filename.endswith('_X.npy'):\n",
    "            series_id = filename.split('_X.npy')[0]\n",
    "            X = np.load(os.path.join(directory, filename))\n",
    "            y = np.load(os.path.join(directory, f'{series_id}_y.npy'))\n",
    "\n",
    "            X_s.append(X)\n",
    "            y_s.append(y)\n",
    "            series_ids.append(series_id)\n",
    "\n",
    "    return X_s, y_s, series_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0ad2da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_y(y):\n",
    "    \n",
    "    for i in range(y.shape[1]):\n",
    "        \n",
    "        mean = y[:,i].mean()\n",
    "        std = y[:,i].std()\n",
    "        y[:,i] = (y[:,i]-mean)/(std+1e-16)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c515709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_pad(X, y=None, split_length=1440, stride=1440):\n",
    "    \"\"\"\n",
    "    Splits and pads the arrays X and y using a sliding window.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.array): Array with shape (timesteps, features)\n",
    "    - y (np.array): Array with shape (timesteps, 2)\n",
    "    - split_length (int): Desired timestep length for the resulting arrays.\n",
    "    - stride (int): Step size for sliding window.\n",
    "\n",
    "    Returns:\n",
    "    - List of arrays for X and y, each with shape (split_length, features)\n",
    "    \"\"\"\n",
    "    \n",
    "    if y is not None and len(X) != len(y):\n",
    "        raise ValueError(\"X and y should have the same number of timesteps.\")\n",
    "    \n",
    "    timesteps, features = X.shape\n",
    "    \n",
    "    # Create empty lists to store split segments\n",
    "    X_splits = []\n",
    "    y_splits = []\n",
    "    starts = []\n",
    "    \n",
    "    # Use sliding window to extract segments\n",
    "    \n",
    "    for start in range(0, timesteps, stride):\n",
    "        end = start + split_length\n",
    "        if end <= timesteps:\n",
    "            starts.append(start)\n",
    "            X_splits.append(X[start:end].copy())\n",
    "            if y is not None:\n",
    "                y_splits.append(y[start:end].copy())\n",
    "        else:\n",
    "            # If the segment is shorter than split_length, pad it\n",
    "            starts.append(start)\n",
    "            padding_length = end - timesteps\n",
    "            X_segment_padded = np.pad(X[start:], ((0, padding_length), (0, 0)), mode='constant', constant_values=-9)\n",
    "            X_splits.append(X_segment_padded)\n",
    "            \n",
    "            if y is not None:\n",
    "                y_segment_padded = np.pad(y[start:], ((0, padding_length), (0, 0)), mode='constant', constant_values=0)\n",
    "                y_splits.append(y_segment_padded)\n",
    "                \n",
    "            break\n",
    "            \n",
    "            \n",
    "    if y is not None:\n",
    "        return X_splits, y_splits, starts\n",
    "    \n",
    "    return X_splits, starts\n",
    "\n",
    "\n",
    "\n",
    "class SleepDataset:\n",
    "    \n",
    "    def __init__(self, X_s, y_s=None, series_ids=None, samp_freq=None, remove_no_dets=True, is_train=False, split_factor=1, norm_params=None):\n",
    "        \n",
    "        self.split_len = (24*60*12) // cfg.samp_freq\n",
    "        self.split_strides = self.split_len\n",
    "        self.remove_no_dets = remove_no_dets\n",
    "        self.is_train = is_train\n",
    "\n",
    "        print(f'Using a split len of {self.split_len}')\n",
    "        self.create_dataset(X_s, y_s, series_ids)\n",
    "\n",
    "        if self.is_train:\n",
    "            self.norm_params = self.calculate_norm_params()\n",
    "        else:\n",
    "            if norm_params is None:\n",
    "                raise ValueError(\"Normalization parameters must be provided for non-training data.\")\n",
    "            self.norm_params = norm_params\n",
    "\n",
    "        self.normalize_data()\n",
    "\n",
    "    def calculate_norm_params(self):\n",
    "        mean = np.mean(self.X, axis=(0, 1))\n",
    "        std = np.std(self.X, axis=(0, 1))\n",
    "        return {'mean': mean, 'std': std}\n",
    "\n",
    "    def normalize_data(self):\n",
    "        self.X = (self.X - self.norm_params['mean']) / (1e-6 + self.norm_params['std'])\n",
    "        \n",
    "        \n",
    "    def create_dataset(self, X_s, y_s=None, series_ids=None):\n",
    "        X_s_splits, y_s_splits, series_splits, starts_splits = [], [], [], []\n",
    "\n",
    "        for i in tqdm(range(len(X_s))):\n",
    "            x_splits, starts = split_and_pad(X_s[i].copy(), split_length = self.split_len, stride=self.split_strides)\n",
    "            X_s_splits.extend(x_splits)\n",
    "            starts_splits.extend(starts)\n",
    "\n",
    "            if y_s is not None:\n",
    "                _, y_splits, _ = split_and_pad(X_s[i].copy(), y_s[i].copy(), split_length=self.split_len, stride=self.split_strides)\n",
    "                y_s_splits.extend(y_splits)\n",
    "\n",
    "            if series_ids is not None:\n",
    "                series_splits.extend([series_ids[i] for _ in range(len(x_splits))])\n",
    "            \n",
    "        self.X = np.array(X_s_splits)\n",
    "        self.starts_splits = np.array(starts_splits)\n",
    "        \n",
    "        if y_s is not None:\n",
    "            self.y = np.array(y_s_splits)\n",
    "\n",
    "            if self.remove_no_dets:\n",
    "                fltr = (self.y[:, :, 1].sum(axis=1) + self.y[:, :, 0].sum(axis=1)) != 0\n",
    "                self.X = self.X[fltr]\n",
    "                self.y = self.y[fltr]\n",
    "                if series_ids is not None:\n",
    "                    self.series_ids = np.array(series_splits)[fltr]\n",
    "                else:\n",
    "                    self.series_ids = None\n",
    "            \n",
    "            self.y = np.array([normalize_y(yts) for yts in self.y])\n",
    "\n",
    "        else:\n",
    "            self.y = None\n",
    "\n",
    "        if series_ids is not None:\n",
    "            if not hasattr(self, 'series_ids'):\n",
    "                self.series_ids = np.array(series_splits)\n",
    "            print(f'X: {self.X.shape}, y: {self.y.shape if self.y is not None else \"Not provided\"}, series_ids: {self.series_ids.shape}')\n",
    "        else:\n",
    "            print(f'X: {self.X.shape}, y: {self.y.shape if self.y is not None else \"Not provided\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20edbd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_nikhil(preds_orig, k_orig=125, max_thresh=0.05, max_count=1000):\n",
    "    \n",
    "    preds=preds_orig.copy()\n",
    "\n",
    "    count = 0\n",
    "    base=6.75\n",
    "    \n",
    "    k = k_orig\n",
    "    prev_max = None\n",
    "\n",
    "    scores = []\n",
    "    indices = []\n",
    "    \n",
    "    while True:\n",
    "            \n",
    "        curr_max_idx = np.argmax(preds)\n",
    "        curr_max = preds[curr_max_idx] \n",
    "        \n",
    "        if (curr_max < max_thresh) or count > max_count:\n",
    "            break\n",
    "        \n",
    "        indices.append(curr_max_idx)\n",
    "        scores.append(curr_max)\n",
    "        \n",
    "        preds[curr_max_idx] = 0\n",
    "        \n",
    "        supress_rates = np.logspace(start=0, stop=1, num=k, base=base)/base\n",
    "\n",
    "        preds[max(curr_max_idx-k, 0):curr_max_idx] *= supress_rates[:min(k, curr_max_idx-0)][::-1]\n",
    "        preds[curr_max_idx+1:min(curr_max_idx+k+1, preds.shape[0])] *= supress_rates[:min(k, preds.shape[0]-curr_max_idx-1)]\n",
    "\n",
    "        prev_max = curr_max\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    return indices, scores\n",
    "\n",
    "def get_actual_preds(val_preds, val_series_ids, val_steps, type_):\n",
    "    times = []\n",
    "    series_ids = []\n",
    "    scores = []\n",
    "    scores_y = []\n",
    "    \n",
    "    for i in np.arange(len(val_preds)):\n",
    "        \n",
    "        vp_i = val_preds[i]\n",
    "        ser_id = val_series_ids[i]\n",
    "        \n",
    "        col_index = 0 if type_ == \"onset\" else 1\n",
    "        other_col_index = 1 if type_ == \"onset\" else 0\n",
    "        \n",
    "        preds = vp_i[:, col_index]\n",
    "        preds_other = vp_i[:, other_col_index]\n",
    "\n",
    "        height_thresh = 0.05\n",
    "        \n",
    "        peaks, peak_scores = nms_nikhil(preds)\n",
    "\n",
    "        times.extend(val_steps[i][peaks])\n",
    "        scores.extend(list(peak_scores))\n",
    "        series_ids.extend([ser_id] * len(peaks))\n",
    "\n",
    "    return np.array(series_ids), np.array(times), np.array(scores)\n",
    "\n",
    "\n",
    "def post_process_preds(val_events_df, val_preds, val_series_ids, val_starts_splits, samp_freq, get_score=False):\n",
    "    \n",
    "#     val_steps = np.concatenate([val_starts_splits[idx] + np.arange(len(val_preds[idx])) for idx in range(val_preds.shape[0])])\n",
    "\n",
    "    val_res = []\n",
    "    \n",
    "    prev_ser_id = None\n",
    "    \n",
    "    res_steps = []\n",
    "    res_preds = []\n",
    "    res_series_ids = []\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(val_preds):\n",
    "        \n",
    "        end = start+1\n",
    "        while end < len(val_preds) and val_series_ids[end] == val_series_ids[start]:\n",
    "            end += 1\n",
    "            \n",
    "        preds = val_preds[start:end]\n",
    "        \n",
    "        steps = np.concatenate([val_starts_splits[idx] + np.arange(len(val_preds[idx])) for idx in range(start, end)])\n",
    "        preds = preds.reshape((preds.shape[0]*preds.shape[1]), 2)\n",
    "        \n",
    "        res_preds.append(preds)\n",
    "        res_steps.append(steps)\n",
    "        res_series_ids.append(val_series_ids[start])\n",
    "        \n",
    "        start=end\n",
    "        \n",
    "        \n",
    "#     print(len(res_series_ids), len(res_steps), len(res_preds))\n",
    "\n",
    "    series_ids_onsets, onsets,  scores_onsets = get_actual_preds(res_preds, res_series_ids, res_steps, 'onset')\n",
    "    series_ids_wakeups, wakeups,  scores_wakeups =get_actual_preds(res_preds, res_series_ids, res_steps, 'wakeup')\n",
    "    \n",
    "    \n",
    "    onset_preds = pl.DataFrame().with_columns([pl.Series(series_ids_onsets).alias('series_id'),\n",
    "                                           pl.Series(onsets).cast(pl.Int64).alias('step'),\n",
    "                                           pl.lit('onset').alias('event'),\n",
    "                                           pl.Series(scores_onsets).alias('score')])\n",
    "\n",
    "    wakeup_preds = pl.DataFrame().with_columns([pl.Series(series_ids_wakeups).alias('series_id'),\n",
    "                                               pl.Series(wakeups).cast(pl.Int64).cast(pl.Int64).alias('step'),\n",
    "                                               pl.lit('wakeup').alias('event'),\n",
    "                                               pl.Series(scores_wakeups).alias('score')])\n",
    "    \n",
    "    val_preds_df = pl.concat([onset_preds, wakeup_preds]).sort(by=['series_id', 'step'])\n",
    "    \n",
    "    if get_score:\n",
    "        toleranaces = {'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]}\n",
    "        comp_score = metric_fast.comp_scorer(\n",
    "        val_events_df,\n",
    "        val_preds_df,\n",
    "        tolerances = toleranaces,\n",
    "        )\n",
    "        return comp_score\n",
    "    \n",
    "    else:\n",
    "        return val_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a630548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_exponential_lr(start_lr, end_lr, num_steps, decay_rate=None):\n",
    "    \"\"\"\n",
    "    Calculate the exponentially decreasing learning rates.\n",
    "\n",
    "    Parameters:\n",
    "    start_lr (float): Initial learning rate.\n",
    "    end_lr (float): Final learning rate.\n",
    "    num_steps (int): Total number of steps over which the learning rate should decay.\n",
    "    decay_rate (float): Decay rate per step. If None, it will be computed based on start_lr, end_lr, and num_steps.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing the learning rate for each step.\n",
    "    \"\"\"\n",
    "    if decay_rate is None:\n",
    "        # Calculate decay rate based on the start_lr, end_lr, and num_steps\n",
    "        decay_rate = (end_lr / start_lr) ** (1 / (num_steps - 1))\n",
    "\n",
    "    learning_rates = [start_lr * (decay_rate ** step) for step in range(num_steps)]\n",
    "    return learning_rates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cf0fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntervalEvaluation(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_ds, val_events_df, samp_freq, n_steps, start_lr, end_lr):\n",
    "\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.val_ds = val_ds\n",
    "        self.val_events_df = val_events_df\n",
    "        self.samp_freq = samp_freq\n",
    "        \n",
    "        warmup_pct_steps = 0.25\n",
    "        warmup_steps = int(n_steps * warmup_pct_steps)\n",
    "        self.learning_rates = [start_lr] * (warmup_steps) + calculate_exponential_lr(start_lr, end_lr, n_steps-warmup_steps)\n",
    "        self.best_score = -np.inf\n",
    "        self.best_model = None\n",
    "        self.step_count=0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        if epoch == 0:\n",
    "            self.first_epoch_start_time = self.start_time\n",
    "        \n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        if self.step_count < len(self.learning_rates):\n",
    "\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, self.learning_rates[self.step_count])\n",
    "            self.curr_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        self.step_count += 1\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        val_preds = self.model.predict(self.val_ds.X, batch_size=32, verbose=0)[:, :, :2]\n",
    "        val_score = post_process_preds(self.val_events_df, val_preds, self.val_ds.series_ids, self.val_ds.starts_splits, self.samp_freq, get_score=True)\n",
    "        \n",
    "        if val_score > self.best_score:\n",
    "            self.best_score = val_score\n",
    "            self.best_model = tf.keras.models.clone_model(self.model)\n",
    "            self.best_model.set_weights(self.model.get_weights()) \n",
    "        \n",
    "        total_time = round(time.time() - self.start_time, 2)\n",
    "        total_seconds_till_now = round(time.time() - self.first_epoch_start_time, 0)\n",
    "        \n",
    "        print(f\"Epoch: {epoch:03d} curr_lr: {self.curr_lr:.1e} - train_loss: {logs['loss']:.04f} - val_loss: {logs['val_loss']:.04f} val_score: {val_score:.03f}  best_val_score: {self.best_score:.03f}  last_epoch t={total_time:.02f}s, total_time_elapsed t={total_seconds_till_now}s\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaf8b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "\n",
    "    # MultiHead Attention\n",
    "    x = tfa.layers.MultiHeadAttention(\n",
    "        head_size=head_size,\n",
    "        num_heads=num_heads,\n",
    "        use_projection_bias=True,\n",
    "        dropout=dropout\n",
    "    )([inputs, inputs, inputs])\n",
    "\n",
    "    # Residual connection with LayerNormalization and Scaling\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + inputs) * (0.5 ** 0.5)\n",
    "    \n",
    "    # Feed Forward Part\n",
    "    ff = tf.keras.layers.Dense(ff_dim, activation='gelu')(x)\n",
    "    ff = tf.keras.layers.Dropout(dropout)(ff)\n",
    "    ff = tf.keras.layers.Dense(inputs.shape[-1])(ff)\n",
    "    ff = tf.keras.layers.Dropout(dropout)(ff)\n",
    "    \n",
    "    # Residual connection with LayerNormalization and Scaling for FFN\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + ff) * (0.5 ** 0.5)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10d34cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mce_loss(y_true, y_pred):\n",
    "    # Clip the ground truth and predictions to the range (-100, 100)\n",
    "    y_true_clipped = tf.clip_by_value(y_true, -100, 100)\n",
    "    y_pred_clipped = tf.clip_by_value(y_pred, -100, 100)\n",
    "\n",
    "    # Calculate the mean cubed error\n",
    "    loss = tf.reduce_mean(tf.abs(y_true_clipped - y_pred_clipped) ** 3)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e51837a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 09:11:23.299769: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-15 09:11:23.506315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46413 MB memory:  -> device: 0, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:23:00.0, compute capability: 8.9\n",
      "/usr/local/lib/python3.8/dist-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33813832"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conv_block(inputs, num_filters):\n",
    "    x = tf.keras.layers.Conv1D(num_filters, 8, padding=\"same\")(inputs)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.Conv1D(num_filters, 8, padding=\"same\")(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def encoder_block(inputs, num_filters):\n",
    "    x = conv_block(inputs, num_filters)\n",
    "    p = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(inputs, concat_tensors, num_filters):\n",
    "    x = tf.keras.layers.Conv1DTranspose(num_filters, 4, strides=2, padding=\"same\")(inputs)\n",
    "    i = len(concat_tensors)-1\n",
    "    for concat_tensor in concat_tensors:\n",
    "        concat_tensor_max = tf.keras.layers.MaxPool1D(pool_size=2**i)(concat_tensor)\n",
    "        x = tf.keras.layers.Concatenate()([x, concat_tensor_max])\n",
    "        i -= 1\n",
    "        \n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def get_model(input_shape, num_blocks=4):\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    orig_n_channels = x.shape[-1]\n",
    "    \n",
    "    x_t = []\n",
    "    n_split_fact = 4\n",
    "    \n",
    "    \n",
    "    for i in range(x.shape[-1]):\n",
    "        x_sp = tf.reshape(x[:, :, i], (-1, x.shape[1]//n_split_fact, n_split_fact))\n",
    "        \n",
    "        if i < orig_n_channels:\n",
    "        \n",
    "            # Calculating mean, max, and standard deviation\n",
    "            mean = tf.reduce_mean(x_sp, axis=-1, keepdims=True)\n",
    "            max_val = tf.reduce_max(x_sp, axis=-1, keepdims=True)\n",
    "            std_dev = tf.math.reduce_std(x_sp, axis=-1, keepdims=True)\n",
    "            min_val = tf.reduce_min(x_sp, axis=-1, keepdims=True)\n",
    "\n",
    "            x_sp = tf.keras.layers.Concatenate()([x_sp, mean, max_val, std_dev, min_val])\n",
    "        \n",
    "        x_sp = tf.keras.layers.Dense(n_split_fact*16, activation='relu')(x_sp)\n",
    "        \n",
    "        x_t.append(x_sp)\n",
    "        \n",
    "    \n",
    "    x_c1d = tf.keras.layers.Conv1D(64, kernel_size=12, strides=n_split_fact, padding=\"same\")(x)\n",
    "    x_c1d = tf.keras.layers.ReLU()(x_c1d)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()(x_t + [x_c1d])# tf.reshape(x, (-1, -1, 3))\n",
    "    \n",
    "    # Initial filter size\n",
    "    fsz = 64\n",
    "    \n",
    "    # Lists to hold the encoder and pooling outputs\n",
    "    encoder_outputs = []\n",
    "    pooling_outputs = []\n",
    "    \n",
    "    # Encoder\n",
    "    for i in range(num_blocks):\n",
    "        if i == 0:\n",
    "            # First block receives the model input\n",
    "            enc_out, pool_out = encoder_block(x, fsz * (2 ** i))\n",
    "        else:\n",
    "            # Subsequent blocks receive the pooling output of the previous block\n",
    "            enc_out, pool_out = encoder_block(pooling_outputs[-1], fsz * (2 ** i))\n",
    "        \n",
    "        encoder_outputs.append(enc_out)\n",
    "        pooling_outputs.append(pool_out)\n",
    "    \n",
    "    # Bottleneck\n",
    "    bottleneck = conv_block(pooling_outputs[-1], fsz * (2 ** num_blocks))\n",
    "    \n",
    "    \n",
    "    def gru_conv(x):\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(x.shape[-1]//4, return_sequences=True))(x)\n",
    "        for i in range(1):\n",
    "            x_conv = tf.keras.layers.Conv1D(x.shape[-1], kernel_size=(4,), padding='same', activation='relu')(x)\n",
    "            x = tf.keras.layers.Dense(x.shape[-1], activation='relu')(x_conv)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    for _ in range(1):\n",
    "        bottleneck = transformer_encoder(bottleneck, head_size=16, num_heads=8, ff_dim=256, dropout=0)\n",
    "\n",
    "    bottleneck = gru_conv(bottleneck)\n",
    "\n",
    "    decoder_input = bottleneck\n",
    "    for i in range(num_blocks - 1, -1, -1):\n",
    "        decoder_output = decoder_block(decoder_input, encoder_outputs[:i+1], fsz * (2 ** i))\n",
    "        decoder_input = decoder_output\n",
    "    \n",
    "    # Output Layer\n",
    "    x = tf.keras.layers.Conv1D(2*n_split_fact, 1, padding=\"same\", activation=\"linear\")(decoder_output)\n",
    "    x = tf.reshape(x, shape=(-1, x.shape[1]*n_split_fact, 2))\n",
    "    \n",
    "    outputs = x\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "    model.compile(optimizer=tfa.optimizers.AdamW(weight_decay=1e-4), loss=custom_mce_loss)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Usage:\n",
    "input_shape = (17280, 9)  # replace with your input shape\n",
    "num_blocks = 4  # specify the number of encoder/decoder blocks\n",
    "model = get_model(input_shape, num_blocks)\n",
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be712eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_cfg:\n",
    "    n_epochs = 14\n",
    "    batch_size = 8\n",
    "    start_lr = 6e-5\n",
    "    end_lr = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "507a829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d01eb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data_processed_models'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.processed_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59d255e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:04<00:00, 119.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 2-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 211 series for training and 53 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211/211 [00:12<00:00, 16.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5742, 17280, 32), y: (5742, 17280, 2), series_ids: (5742,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 68.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1462, 17280, 32), y: (1462, 17280, 2), series_ids: (1462,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model parameters: 34598408\n",
      "Epoch 1/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 09:13:57.027467: I tensorflow/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n",
      "2023-12-15 09:13:57.041566: I tensorflow/stream_executor/cuda/cuda_blas.cc:1633] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "718/718 [==============================] - ETA: 0s - loss: 4.5810Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.5810 - val_loss: 3.7517 val_score: 0.768  best_val_score: 0.768  last_epoch t=81.97s, total_time_elapsed t=82.0s\n",
      "718/718 [==============================] - 82s 104ms/step - loss: 4.5810 - val_loss: 3.7517\n",
      "Epoch 2/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.7207Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.7207 - val_loss: 4.0584 val_score: 0.733  best_val_score: 0.768  last_epoch t=67.05s, total_time_elapsed t=149.0s\n",
      "718/718 [==============================] - 67s 93ms/step - loss: 3.7207 - val_loss: 4.0584\n",
      "Epoch 3/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.6052Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.6052 - val_loss: 3.4685 val_score: 0.779  best_val_score: 0.779  last_epoch t=69.78s, total_time_elapsed t=219.0s\n",
      "718/718 [==============================] - 70s 97ms/step - loss: 3.6052 - val_loss: 3.4685\n",
      "Epoch 4/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.5499Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.5499 - val_loss: 3.4015 val_score: 0.785  best_val_score: 0.785  last_epoch t=68.97s, total_time_elapsed t=288.0s\n",
      "718/718 [==============================] - 69s 96ms/step - loss: 3.5499 - val_loss: 3.4015\n",
      "Epoch 5/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.4894Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.4894 - val_loss: 3.3430 val_score: 0.798  best_val_score: 0.798  last_epoch t=68.45s, total_time_elapsed t=356.0s\n",
      "718/718 [==============================] - 68s 95ms/step - loss: 3.4894 - val_loss: 3.3430\n",
      "Epoch 6/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.4551Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.4551 - val_loss: 3.3017 val_score: 0.809  best_val_score: 0.809  last_epoch t=68.23s, total_time_elapsed t=424.0s\n",
      "718/718 [==============================] - 68s 95ms/step - loss: 3.4551 - val_loss: 3.3017\n",
      "Epoch 7/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.4252Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.4252 - val_loss: 3.3627 val_score: 0.796  best_val_score: 0.809  last_epoch t=67.44s, total_time_elapsed t=492.0s\n",
      "718/718 [==============================] - 67s 94ms/step - loss: 3.4252 - val_loss: 3.3627\n",
      "Epoch 8/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.3963Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.3963 - val_loss: 3.2926 val_score: 0.804  best_val_score: 0.809  last_epoch t=65.83s, total_time_elapsed t=558.0s\n",
      "718/718 [==============================] - 66s 92ms/step - loss: 3.3963 - val_loss: 3.2926\n",
      "Epoch 9/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.3692Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.3692 - val_loss: 3.3679 val_score: 0.804  best_val_score: 0.809  last_epoch t=65.65s, total_time_elapsed t=623.0s\n",
      "718/718 [==============================] - 66s 91ms/step - loss: 3.3692 - val_loss: 3.3679\n",
      "Epoch 10/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.3440Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.3440 - val_loss: 3.2469 val_score: 0.809  best_val_score: 0.809  last_epoch t=65.84s, total_time_elapsed t=689.0s\n",
      "718/718 [==============================] - 66s 92ms/step - loss: 3.3440 - val_loss: 3.2469\n",
      "Epoch 11/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.3394Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.3394 - val_loss: 3.2822 val_score: 0.803  best_val_score: 0.809  last_epoch t=66.65s, total_time_elapsed t=756.0s\n",
      "718/718 [==============================] - 67s 93ms/step - loss: 3.3394 - val_loss: 3.2822\n",
      "Epoch 12/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.3148Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.3148 - val_loss: 3.2662 val_score: 0.806  best_val_score: 0.809  last_epoch t=66.90s, total_time_elapsed t=823.0s\n",
      "718/718 [==============================] - 67s 93ms/step - loss: 3.3148 - val_loss: 3.2662\n",
      "Epoch 13/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.2952"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 09:28:29.472247: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.12GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_1/conv1d_22/Conv1D' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_5043/2478982756.py\", line 48, in <module>\n      model.fit(trn_ds.X, trn_ds.y,\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1624, in fit\n      callbacks.on_epoch_end(epoch, epoch_logs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\", line 448, in on_epoch_end\n      callback.on_epoch_end(epoch, logs)\n    File \"/tmp/ipykernel_5043/2369401257.py\", line 32, in on_epoch_end\n      val_preds = self.model.predict(self.val_ds.X, batch_size=32, verbose=0)[:, :, :2]\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2253, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2041, in predict_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2027, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2015, in run_step\n      outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1983, in predict_step\n      return self(x, training=False)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/layers/convolutional/base_conv.py\", line 283, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/layers/convolutional/base_conv.py\", line 255, in convolution_op\n      return tf.nn.convolution(\nNode: 'model_1/conv1d_22/Conv1D'\nCUDNN failed to allocate the scratch space for the runner or to find a working no-scratch runner.\n\t [[{{node model_1/conv1d_22/Conv1D}}]] [Op:__inference_predict_function_33711]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m val_events_df \u001b[38;5;241m=\u001b[39m train_events\u001b[38;5;241m.\u001b[39mfilter(pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseries_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mis_in(series_ids_val))\n\u001b[1;32m     46\u001b[0m inter_eval \u001b[38;5;241m=\u001b[39m IntervalEvaluation(val_ds, val_events_df, cfg\u001b[38;5;241m.\u001b[39msamp_freq, (trn_ds\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mmodel_cfg\u001b[38;5;241m.\u001b[39mbatch_size)\u001b[38;5;241m*\u001b[39mmodel_cfg\u001b[38;5;241m.\u001b[39mn_epochs, model_cfg\u001b[38;5;241m.\u001b[39mstart_lr, model_cfg\u001b[38;5;241m.\u001b[39mend_lr)\n\u001b[0;32m---> 48\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrn_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrn_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43minter_eval\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m      \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m last_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel finished with val loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn [18], line 32\u001b[0m, in \u001b[0;36mIntervalEvaluation.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_epoch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, epoch, logs\u001b[38;5;241m=\u001b[39m{}):\n\u001b[0;32m---> 32\u001b[0m     val_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[:, :, :\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     33\u001b[0m     val_score \u001b[38;5;241m=\u001b[39m post_process_preds(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_events_df, val_preds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_ds\u001b[38;5;241m.\u001b[39mseries_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_ds\u001b[38;5;241m.\u001b[39mstarts_splits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamp_freq, get_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_score \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_score:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node 'model_1/conv1d_22/Conv1D' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_5043/2478982756.py\", line 48, in <module>\n      model.fit(trn_ds.X, trn_ds.y,\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1624, in fit\n      callbacks.on_epoch_end(epoch, epoch_logs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\", line 448, in on_epoch_end\n      callback.on_epoch_end(epoch, logs)\n    File \"/tmp/ipykernel_5043/2369401257.py\", line 32, in on_epoch_end\n      val_preds = self.model.predict(self.val_ds.X, batch_size=32, verbose=0)[:, :, :2]\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2253, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2041, in predict_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2027, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 2015, in run_step\n      outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1983, in predict_step\n      return self(x, training=False)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/layers/convolutional/base_conv.py\", line 283, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/layers/convolutional/base_conv.py\", line 255, in convolution_op\n      return tf.nn.convolution(\nNode: 'model_1/conv1d_22/Conv1D'\nCUDNN failed to allocate the scratch space for the runner or to find a working no-scratch runner.\n\t [[{{node model_1/conv1d_22/Conv1D}}]] [Op:__inference_predict_function_33711]"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "fold_run_order = [2, 1, 3, 4, 5]\n",
    "\n",
    "val_preds_lst = []\n",
    "val_series_lst = []\n",
    "val_starts_splits_lst = []\n",
    "\n",
    "val_y_lst = []\n",
    "models_lst = []\n",
    "val_scores = []\n",
    "\n",
    "model_dct = {}\n",
    "\n",
    "for fold_num in fold_run_order:\n",
    "    \n",
    "    X_s, y_s, series_ids = load_data(cfg.processed_data_path)\n",
    "    \n",
    "\n",
    "    print(f'\\n\\n\\n-----------Starting fold: {fold_num}-----------\\n\\n\\n')\n",
    "    \n",
    "    series_ids_val = splits_df.filter(pl.col(f'{n_folds}_fold') == fold_num)['series_id'].to_numpy()\n",
    "\n",
    "    val_idxs = [series_ids.index(s) for s in series_ids_val]\n",
    "    trn_idxs = np.setdiff1d(np.arange(len(series_ids)), val_idxs)\n",
    "    \n",
    "    print(f'Using {len(trn_idxs)} series for training and {len(val_idxs)} series for validation')\n",
    "\n",
    "    X_s_trn, y_s_trn, series_ids_trn = [X_s[i] for i in trn_idxs], [y_s[i] for i in trn_idxs], [series_ids[i] for i in trn_idxs]\n",
    "    X_s_val, y_s_val, series_ids_val = [X_s[i] for i in val_idxs], [y_s[i] for i in val_idxs], [series_ids[i] for i in val_idxs]\n",
    "\n",
    "    trn_ds = SleepDataset(X_s_trn, y_s_trn, series_ids_trn, cfg.samp_freq, remove_no_dets=False, is_train=True)\n",
    "    norm_params = trn_ds.norm_params\n",
    "    model_dct[fold_num] = norm_params\n",
    "    \n",
    "    val_ds = SleepDataset(X_s_val, y_s_val, series_ids_val, cfg.samp_freq, remove_no_dets=False, is_train=False, norm_params=norm_params)\n",
    "\n",
    "    del X_s_trn, y_s_trn,  X_s_val, y_s_val, series_ids_trn, X_s, y_s, series_ids\n",
    "    _ = gc.collect()\n",
    "\n",
    "\n",
    "    model = get_model(trn_ds.X.shape[1:])\n",
    "    \n",
    "    print(f'Total model parameters: {model.count_params()}')\n",
    "\n",
    "    val_events_df = train_events.filter(pl.col('series_id').is_in(series_ids_val))\n",
    "    inter_eval = IntervalEvaluation(val_ds, val_events_df, cfg.samp_freq, (trn_ds.X.shape[0]//model_cfg.batch_size)*model_cfg.n_epochs, model_cfg.start_lr, model_cfg.end_lr)\n",
    "    \n",
    "    model.fit(trn_ds.X, trn_ds.y,\n",
    "          epochs=model_cfg.n_epochs,\n",
    "          batch_size=model_cfg.batch_size,\n",
    "          callbacks=[inter_eval],\n",
    "          verbose=1,\n",
    "          validation_data=(val_ds.X, val_ds.y))\n",
    "    \n",
    "    \n",
    "    \n",
    "    last_loss = model.history.history['val_loss'][-1]\n",
    "    print(f'Model finished with val loss: {last_loss}')\n",
    "    \n",
    "    model = inter_eval.best_model\n",
    "    val_preds = model.predict(val_ds.X)\n",
    "    \n",
    "    val_score = post_process_preds(val_events_df, val_preds, val_ds.series_ids, val_ds.starts_splits, cfg.samp_freq, get_score=True)\n",
    "    print(f'Val Score: {val_score}')\n",
    "    \n",
    "    val_scores.append(val_score)\n",
    "    \n",
    "    val_y_lst.append(val_ds.y)\n",
    "    val_preds_lst.append(val_preds)\n",
    "    val_series_lst.append(val_ds.series_ids)\n",
    "    val_starts_splits_lst.append(val_ds.starts_splits)\n",
    "    \n",
    "    tf.keras.models.save_model(model, os.path.join(cfg.output_dir, cfg.ver, f'tf_model_fold_{fold_num}.h5'))\n",
    "    \n",
    "    del trn_ds, val_ds, model, inter_eval\n",
    "    _ = gc.collect()\n",
    "\n",
    "avg_val_score, std_val_score = np.mean(val_scores), np.std(val_scores)\n",
    "print(f'Avg val score: {avg_val_score} and std val score: {std_val_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "897530b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:04<00:00, 129.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 2-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 211 series for training and 53 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211/211 [00:11<00:00, 18.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5742, 17280, 32), y: (5742, 17280, 2), series_ids: (5742,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 69.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1462, 17280, 32), y: (1462, 17280, 2), series_ids: (1462,)\n",
      "Total model parameters: 34598408\n",
      "Epoch 1/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 06:00:13.090553: I tensorflow/stream_executor/cuda/cuda_blas.cc:1633] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-12-14 06:00:13.122684: I tensorflow/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "718/718 [==============================] - ETA: 0s - loss: 4.4215Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.4215 - val_loss: 3.7856 val_score: 0.752  best_val_score: 0.752  last_epoch t=77.57s, total_time_elapsed t=78.0s\n",
      "718/718 [==============================] - 78s 99ms/step - loss: 4.4215 - val_loss: 3.7856\n",
      "Epoch 2/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.7059Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.7059 - val_loss: 3.5250 val_score: 0.775  best_val_score: 0.775  last_epoch t=66.58s, total_time_elapsed t=144.0s\n",
      "718/718 [==============================] - 67s 93ms/step - loss: 3.7059 - val_loss: 3.5250\n",
      "Epoch 3/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.5783Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.5783 - val_loss: 3.4094 val_score: 0.788  best_val_score: 0.788  last_epoch t=64.10s, total_time_elapsed t=208.0s\n",
      "718/718 [==============================] - 64s 89ms/step - loss: 3.5783 - val_loss: 3.4094\n",
      "Epoch 4/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.5287Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.5287 - val_loss: 3.4144 val_score: 0.792  best_val_score: 0.792  last_epoch t=66.33s, total_time_elapsed t=275.0s\n",
      "718/718 [==============================] - 66s 92ms/step - loss: 3.5287 - val_loss: 3.4144\n",
      "Epoch 5/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.4928Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.4928 - val_loss: 3.3444 val_score: 0.787  best_val_score: 0.792  last_epoch t=64.71s, total_time_elapsed t=339.0s\n",
      "718/718 [==============================] - 65s 90ms/step - loss: 3.4928 - val_loss: 3.3444\n",
      "Epoch 6/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.4490Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.4490 - val_loss: 3.3179 val_score: 0.799  best_val_score: 0.799  last_epoch t=65.98s, total_time_elapsed t=405.0s\n",
      "718/718 [==============================] - 66s 92ms/step - loss: 3.4490 - val_loss: 3.3179\n",
      "Epoch 7/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.4126Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.4126 - val_loss: 3.6757 val_score: 0.784  best_val_score: 0.799  last_epoch t=64.88s, total_time_elapsed t=470.0s\n",
      "718/718 [==============================] - 65s 90ms/step - loss: 3.4126 - val_loss: 3.6757\n",
      "Epoch 8/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.3758Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.3758 - val_loss: 3.4671 val_score: 0.794  best_val_score: 0.799  last_epoch t=67.08s, total_time_elapsed t=537.0s\n",
      "718/718 [==============================] - 67s 93ms/step - loss: 3.3758 - val_loss: 3.4671\n",
      "Epoch 9/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.3695Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.3695 - val_loss: 3.2521 val_score: 0.802  best_val_score: 0.802  last_epoch t=69.92s, total_time_elapsed t=607.0s\n",
      "718/718 [==============================] - 70s 97ms/step - loss: 3.3695 - val_loss: 3.2521\n",
      "Epoch 10/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.3633Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.3633 - val_loss: 3.3065 val_score: 0.800  best_val_score: 0.802  last_epoch t=64.89s, total_time_elapsed t=672.0s\n",
      "718/718 [==============================] - 65s 90ms/step - loss: 3.3633 - val_loss: 3.3065\n",
      "Epoch 11/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.3248Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.3248 - val_loss: 3.4022 val_score: 0.781  best_val_score: 0.802  last_epoch t=67.83s, total_time_elapsed t=740.0s\n",
      "718/718 [==============================] - 68s 94ms/step - loss: 3.3248 - val_loss: 3.4022\n",
      "Epoch 12/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.3149Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.3149 - val_loss: 3.3622 val_score: 0.799  best_val_score: 0.802  last_epoch t=68.28s, total_time_elapsed t=808.0s\n",
      "718/718 [==============================] - 68s 95ms/step - loss: 3.3149 - val_loss: 3.3622\n",
      "Epoch 13/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.3152Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.3152 - val_loss: 3.3423 val_score: 0.798  best_val_score: 0.802  last_epoch t=65.13s, total_time_elapsed t=873.0s\n",
      "718/718 [==============================] - 65s 91ms/step - loss: 3.3152 - val_loss: 3.3423\n",
      "Epoch 14/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.2971Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.2971 - val_loss: 3.2751 val_score: 0.801  best_val_score: 0.802  last_epoch t=67.70s, total_time_elapsed t=941.0s\n",
      "718/718 [==============================] - 68s 94ms/step - loss: 3.2971 - val_loss: 3.2751\n",
      "Model finished with val loss: 3.275120735168457\n",
      "46/46 [==============================] - 5s 67ms/step\n",
      "Val Score: 0.8017610135795821\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:05<00:00, 105.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 1-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 211 series for training and 53 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211/211 [00:10<00:00, 20.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5831, 17280, 32), y: (5831, 17280, 2), series_ids: (5831,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:02<00:00, 25.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1373, 17280, 32), y: (1373, 17280, 2), series_ids: (1373,)\n",
      "Total model parameters: 34598408\n",
      "Epoch 1/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 4.4836Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.4836 - val_loss: 3.8228 val_score: 0.787  best_val_score: 0.787  last_epoch t=79.34s, total_time_elapsed t=79.0s\n",
      "729/729 [==============================] - 79s 101ms/step - loss: 4.4836 - val_loss: 3.8228\n",
      "Epoch 2/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.6446Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6446 - val_loss: 3.8220 val_score: 0.791  best_val_score: 0.791  last_epoch t=68.01s, total_time_elapsed t=147.0s\n",
      "729/729 [==============================] - 68s 93ms/step - loss: 3.6446 - val_loss: 3.8220\n",
      "Epoch 3/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.5298Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.5298 - val_loss: 3.7524 val_score: 0.812  best_val_score: 0.812  last_epoch t=67.47s, total_time_elapsed t=215.0s\n",
      "729/729 [==============================] - 67s 93ms/step - loss: 3.5298 - val_loss: 3.7524\n",
      "Epoch 4/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.4715Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4715 - val_loss: 3.7062 val_score: 0.810  best_val_score: 0.812  last_epoch t=66.32s, total_time_elapsed t=281.0s\n",
      "729/729 [==============================] - 66s 91ms/step - loss: 3.4715 - val_loss: 3.7062\n",
      "Epoch 5/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.4315Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.4315 - val_loss: 3.6169 val_score: 0.822  best_val_score: 0.822  last_epoch t=67.75s, total_time_elapsed t=349.0s\n",
      "729/729 [==============================] - 68s 93ms/step - loss: 3.4315 - val_loss: 3.6169\n",
      "Epoch 6/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.3791Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3791 - val_loss: 3.6681 val_score: 0.822  best_val_score: 0.822  last_epoch t=66.19s, total_time_elapsed t=415.0s\n",
      "729/729 [==============================] - 66s 91ms/step - loss: 3.3791 - val_loss: 3.6681\n",
      "Epoch 7/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.3447Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3447 - val_loss: 3.7017 val_score: 0.811  best_val_score: 0.822  last_epoch t=66.87s, total_time_elapsed t=482.0s\n",
      "729/729 [==============================] - 67s 92ms/step - loss: 3.3447 - val_loss: 3.7017\n",
      "Epoch 8/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.3224Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.3224 - val_loss: 3.5849 val_score: 0.824  best_val_score: 0.824  last_epoch t=69.03s, total_time_elapsed t=551.0s\n",
      "729/729 [==============================] - 69s 95ms/step - loss: 3.3224 - val_loss: 3.5849\n",
      "Epoch 9/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.3122Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.3122 - val_loss: 3.5988 val_score: 0.820  best_val_score: 0.824  last_epoch t=65.64s, total_time_elapsed t=617.0s\n",
      "729/729 [==============================] - 66s 90ms/step - loss: 3.3122 - val_loss: 3.5988\n",
      "Epoch 10/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.2788Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2788 - val_loss: 3.6343 val_score: 0.824  best_val_score: 0.824  last_epoch t=65.28s, total_time_elapsed t=682.0s\n",
      "729/729 [==============================] - 65s 90ms/step - loss: 3.2788 - val_loss: 3.6343\n",
      "Epoch 11/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.2706Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2706 - val_loss: 3.5692 val_score: 0.826  best_val_score: 0.826  last_epoch t=68.40s, total_time_elapsed t=750.0s\n",
      "729/729 [==============================] - 68s 94ms/step - loss: 3.2706 - val_loss: 3.5692\n",
      "Epoch 12/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.2477Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.2477 - val_loss: 3.6311 val_score: 0.827  best_val_score: 0.827  last_epoch t=67.88s, total_time_elapsed t=818.0s\n",
      "729/729 [==============================] - 68s 93ms/step - loss: 3.2477 - val_loss: 3.6311\n",
      "Epoch 13/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.2426Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.2426 - val_loss: 3.5776 val_score: 0.824  best_val_score: 0.827  last_epoch t=65.63s, total_time_elapsed t=884.0s\n",
      "729/729 [==============================] - 66s 90ms/step - loss: 3.2426 - val_loss: 3.5776\n",
      "Epoch 14/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.2297Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.2297 - val_loss: 3.5572 val_score: 0.829  best_val_score: 0.829  last_epoch t=68.49s, total_time_elapsed t=952.0s\n",
      "729/729 [==============================] - 68s 94ms/step - loss: 3.2297 - val_loss: 3.5572\n",
      "Model finished with val loss: 3.557244062423706\n",
      "43/43 [==============================] - 4s 65ms/step\n",
      "Val Score: 0.8288928935068927\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:04<00:00, 109.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 3-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 211 series for training and 53 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211/211 [00:08<00:00, 24.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5778, 17280, 32), y: (5778, 17280, 2), series_ids: (5778,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:02<00:00, 22.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1426, 17280, 32), y: (1426, 17280, 2), series_ids: (1426,)\n",
      "Total model parameters: 34598408\n",
      "Epoch 1/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 4.4451Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.4451 - val_loss: 3.7266 val_score: 0.773  best_val_score: 0.773  last_epoch t=81.88s, total_time_elapsed t=82.0s\n",
      "723/723 [==============================] - 82s 105ms/step - loss: 4.4451 - val_loss: 3.7266\n",
      "Epoch 2/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.6838Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6838 - val_loss: 3.5825 val_score: 0.804  best_val_score: 0.804  last_epoch t=68.47s, total_time_elapsed t=150.0s\n",
      "723/723 [==============================] - 68s 95ms/step - loss: 3.6838 - val_loss: 3.5825\n",
      "Epoch 3/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.5801Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.5801 - val_loss: 3.5486 val_score: 0.810  best_val_score: 0.810  last_epoch t=68.56s, total_time_elapsed t=219.0s\n",
      "723/723 [==============================] - 69s 95ms/step - loss: 3.5801 - val_loss: 3.5486\n",
      "Epoch 4/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.5190Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.5190 - val_loss: 3.4994 val_score: 0.814  best_val_score: 0.814  last_epoch t=69.31s, total_time_elapsed t=288.0s\n",
      "723/723 [==============================] - 69s 96ms/step - loss: 3.5190 - val_loss: 3.4994\n",
      "Epoch 5/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.5189Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.5189 - val_loss: 3.4980 val_score: 0.800  best_val_score: 0.814  last_epoch t=67.55s, total_time_elapsed t=356.0s\n",
      "723/723 [==============================] - 68s 93ms/step - loss: 3.5189 - val_loss: 3.4980\n",
      "Epoch 6/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.4333Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.4333 - val_loss: 3.4614 val_score: 0.816  best_val_score: 0.816  last_epoch t=69.58s, total_time_elapsed t=425.0s\n",
      "723/723 [==============================] - 70s 96ms/step - loss: 3.4333 - val_loss: 3.4614\n",
      "Epoch 7/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.4172Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.4172 - val_loss: 3.4183 val_score: 0.815  best_val_score: 0.816  last_epoch t=67.56s, total_time_elapsed t=493.0s\n",
      "723/723 [==============================] - 68s 93ms/step - loss: 3.4172 - val_loss: 3.4183\n",
      "Epoch 8/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.3785Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.3785 - val_loss: 3.4252 val_score: 0.813  best_val_score: 0.816  last_epoch t=67.91s, total_time_elapsed t=561.0s\n",
      "723/723 [==============================] - 68s 94ms/step - loss: 3.3785 - val_loss: 3.4252\n",
      "Epoch 9/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.3405Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.3405 - val_loss: 3.4126 val_score: 0.818  best_val_score: 0.818  last_epoch t=70.31s, total_time_elapsed t=631.0s\n",
      "723/723 [==============================] - 70s 97ms/step - loss: 3.3405 - val_loss: 3.4126\n",
      "Epoch 10/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.3392Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.3392 - val_loss: 3.4231 val_score: 0.813  best_val_score: 0.818  last_epoch t=67.82s, total_time_elapsed t=699.0s\n",
      "723/723 [==============================] - 68s 94ms/step - loss: 3.3392 - val_loss: 3.4231\n",
      "Epoch 11/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.3245Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.3245 - val_loss: 3.3613 val_score: 0.815  best_val_score: 0.818  last_epoch t=67.01s, total_time_elapsed t=766.0s\n",
      "723/723 [==============================] - 67s 93ms/step - loss: 3.3245 - val_loss: 3.3613\n",
      "Epoch 12/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.2946Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.2946 - val_loss: 3.5856 val_score: 0.792  best_val_score: 0.818  last_epoch t=67.70s, total_time_elapsed t=834.0s\n",
      "723/723 [==============================] - 68s 94ms/step - loss: 3.2946 - val_loss: 3.5856\n",
      "Epoch 13/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.2897Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.2897 - val_loss: 3.3432 val_score: 0.816  best_val_score: 0.818  last_epoch t=67.49s, total_time_elapsed t=901.0s\n",
      "723/723 [==============================] - 67s 93ms/step - loss: 3.2897 - val_loss: 3.3432\n",
      "Epoch 14/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.2683Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.2683 - val_loss: 3.3509 val_score: 0.818  best_val_score: 0.818  last_epoch t=67.48s, total_time_elapsed t=969.0s\n",
      "723/723 [==============================] - 67s 93ms/step - loss: 3.2683 - val_loss: 3.3509\n",
      "Model finished with val loss: 3.350882053375244\n",
      "45/45 [==============================] - 5s 68ms/step\n",
      "Val Score: 0.8184229626747149\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:05<00:00, 104.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 4-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 211 series for training and 53 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211/211 [00:08<00:00, 24.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5820, 17280, 32), y: (5820, 17280, 2), series_ids: (5820,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:01<00:00, 35.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1384, 17280, 32), y: (1384, 17280, 2), series_ids: (1384,)\n",
      "Total model parameters: 34598408\n",
      "Epoch 1/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 4.4297Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.4297 - val_loss: 3.8778 val_score: 0.776  best_val_score: 0.776  last_epoch t=79.25s, total_time_elapsed t=79.0s\n",
      "728/728 [==============================] - 79s 101ms/step - loss: 4.4297 - val_loss: 3.8778\n",
      "Epoch 2/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.6392Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6392 - val_loss: 3.9547 val_score: 0.779  best_val_score: 0.779  last_epoch t=67.73s, total_time_elapsed t=147.0s\n",
      "728/728 [==============================] - 68s 93ms/step - loss: 3.6392 - val_loss: 3.9547\n",
      "Epoch 3/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.5414Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.5414 - val_loss: 3.6769 val_score: 0.799  best_val_score: 0.799  last_epoch t=66.06s, total_time_elapsed t=213.0s\n",
      "728/728 [==============================] - 66s 91ms/step - loss: 3.5414 - val_loss: 3.6769\n",
      "Epoch 4/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.4828Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4828 - val_loss: 3.6396 val_score: 0.801  best_val_score: 0.801  last_epoch t=68.13s, total_time_elapsed t=281.0s\n",
      "728/728 [==============================] - 68s 94ms/step - loss: 3.4828 - val_loss: 3.6396\n",
      "Epoch 5/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.4313Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.4313 - val_loss: 3.5919 val_score: 0.803  best_val_score: 0.803  last_epoch t=68.45s, total_time_elapsed t=350.0s\n",
      "728/728 [==============================] - 68s 94ms/step - loss: 3.4313 - val_loss: 3.5919\n",
      "Epoch 6/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.3972Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3972 - val_loss: 3.5615 val_score: 0.808  best_val_score: 0.808  last_epoch t=66.93s, total_time_elapsed t=417.0s\n",
      "728/728 [==============================] - 67s 92ms/step - loss: 3.3972 - val_loss: 3.5615\n",
      "Epoch 7/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.3518Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3518 - val_loss: 3.5976 val_score: 0.801  best_val_score: 0.808  last_epoch t=65.87s, total_time_elapsed t=482.0s\n",
      "728/728 [==============================] - 66s 91ms/step - loss: 3.3518 - val_loss: 3.5976\n",
      "Epoch 8/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.3258Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.3258 - val_loss: 3.5874 val_score: 0.800  best_val_score: 0.808  last_epoch t=66.24s, total_time_elapsed t=549.0s\n",
      "728/728 [==============================] - 66s 91ms/step - loss: 3.3258 - val_loss: 3.5874\n",
      "Epoch 9/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.3090Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.3090 - val_loss: 3.5445 val_score: 0.809  best_val_score: 0.809  last_epoch t=68.03s, total_time_elapsed t=617.0s\n",
      "728/728 [==============================] - 68s 93ms/step - loss: 3.3090 - val_loss: 3.5445\n",
      "Epoch 10/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.2827Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2827 - val_loss: 3.5365 val_score: 0.810  best_val_score: 0.810  last_epoch t=67.26s, total_time_elapsed t=684.0s\n",
      "728/728 [==============================] - 67s 92ms/step - loss: 3.2827 - val_loss: 3.5365\n",
      "Epoch 11/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.2756Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2756 - val_loss: 3.5646 val_score: 0.815  best_val_score: 0.815  last_epoch t=67.01s, total_time_elapsed t=751.0s\n",
      "728/728 [==============================] - 67s 92ms/step - loss: 3.2756 - val_loss: 3.5646\n",
      "Epoch 12/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.2511Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.2511 - val_loss: 3.4805 val_score: 0.817  best_val_score: 0.817  last_epoch t=67.80s, total_time_elapsed t=819.0s\n",
      "728/728 [==============================] - 68s 93ms/step - loss: 3.2511 - val_loss: 3.4805\n",
      "Epoch 13/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.2384Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.2384 - val_loss: 3.5497 val_score: 0.815  best_val_score: 0.817  last_epoch t=64.89s, total_time_elapsed t=884.0s\n",
      "728/728 [==============================] - 65s 89ms/step - loss: 3.2384 - val_loss: 3.5497\n",
      "Epoch 14/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.2240Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.2240 - val_loss: 3.5087 val_score: 0.814  best_val_score: 0.817  last_epoch t=63.82s, total_time_elapsed t=948.0s\n",
      "728/728 [==============================] - 64s 88ms/step - loss: 3.2240 - val_loss: 3.5087\n",
      "Model finished with val loss: 3.5087311267852783\n",
      "44/44 [==============================] - 7s 68ms/step\n",
      "Val Score: 0.8172003954278348\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:04<00:00, 107.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 5-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 212 series for training and 52 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 212/212 [00:08<00:00, 25.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5645, 17280, 32), y: (5645, 17280, 2), series_ids: (5645,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:01<00:00, 29.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1559, 17280, 32), y: (1559, 17280, 2), series_ids: (1559,)\n",
      "Total model parameters: 34598408\n",
      "Epoch 1/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 4.5370Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.5370 - val_loss: 3.8569 val_score: 0.759  best_val_score: 0.759  last_epoch t=80.07s, total_time_elapsed t=80.0s\n",
      "706/706 [==============================] - 80s 105ms/step - loss: 4.5370 - val_loss: 3.8569\n",
      "Epoch 2/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.7308Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.7308 - val_loss: 3.4689 val_score: 0.785  best_val_score: 0.785  last_epoch t=68.68s, total_time_elapsed t=149.0s\n",
      "706/706 [==============================] - 69s 97ms/step - loss: 3.7308 - val_loss: 3.4689\n",
      "Epoch 3/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.6313Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.6313 - val_loss: 3.5643 val_score: 0.808  best_val_score: 0.808  last_epoch t=69.40s, total_time_elapsed t=218.0s\n",
      "706/706 [==============================] - 69s 98ms/step - loss: 3.6313 - val_loss: 3.5643\n",
      "Epoch 4/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.5606Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.5606 - val_loss: 3.6755 val_score: 0.799  best_val_score: 0.808  last_epoch t=67.13s, total_time_elapsed t=285.0s\n",
      "706/706 [==============================] - 67s 95ms/step - loss: 3.5606 - val_loss: 3.6755\n",
      "Epoch 5/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.5011Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.5011 - val_loss: 3.3932 val_score: 0.804  best_val_score: 0.808  last_epoch t=67.12s, total_time_elapsed t=352.0s\n",
      "706/706 [==============================] - 67s 95ms/step - loss: 3.5011 - val_loss: 3.3932\n",
      "Epoch 6/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.4690Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.4690 - val_loss: 3.3324 val_score: 0.804  best_val_score: 0.808  last_epoch t=67.34s, total_time_elapsed t=420.0s\n",
      "706/706 [==============================] - 67s 95ms/step - loss: 3.4690 - val_loss: 3.3324\n",
      "Epoch 7/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.4215Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.4215 - val_loss: 3.3372 val_score: 0.802  best_val_score: 0.808  last_epoch t=67.47s, total_time_elapsed t=487.0s\n",
      "706/706 [==============================] - 67s 96ms/step - loss: 3.4215 - val_loss: 3.3372\n",
      "Epoch 8/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.4038Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.4038 - val_loss: 3.2650 val_score: 0.818  best_val_score: 0.818  last_epoch t=71.84s, total_time_elapsed t=559.0s\n",
      "706/706 [==============================] - 72s 102ms/step - loss: 3.4038 - val_loss: 3.2650\n",
      "Epoch 9/14\n",
      "705/706 [============================>.] - ETA: 0s - loss: 3.3821Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.3811 - val_loss: 3.3180 val_score: 0.811  best_val_score: 0.818  last_epoch t=67.45s, total_time_elapsed t=627.0s\n",
      "706/706 [==============================] - 67s 96ms/step - loss: 3.3811 - val_loss: 3.3180\n",
      "Epoch 10/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.3539Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.3539 - val_loss: 3.3901 val_score: 0.809  best_val_score: 0.818  last_epoch t=67.38s, total_time_elapsed t=694.0s\n",
      "706/706 [==============================] - 67s 95ms/step - loss: 3.3539 - val_loss: 3.3901\n",
      "Epoch 11/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.3445Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.3445 - val_loss: 3.2581 val_score: 0.822  best_val_score: 0.822  last_epoch t=70.55s, total_time_elapsed t=764.0s\n",
      "706/706 [==============================] - 71s 100ms/step - loss: 3.3445 - val_loss: 3.2581\n",
      "Epoch 12/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.3265Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.3265 - val_loss: 3.3986 val_score: 0.814  best_val_score: 0.822  last_epoch t=68.21s, total_time_elapsed t=833.0s\n",
      "706/706 [==============================] - 68s 97ms/step - loss: 3.3265 - val_loss: 3.3986\n",
      "Epoch 13/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.3064Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.3064 - val_loss: 3.2586 val_score: 0.820  best_val_score: 0.822  last_epoch t=66.69s, total_time_elapsed t=899.0s\n",
      "706/706 [==============================] - 67s 94ms/step - loss: 3.3064 - val_loss: 3.2586\n",
      "Epoch 14/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.2902Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.2902 - val_loss: 3.2775 val_score: 0.808  best_val_score: 0.822  last_epoch t=67.43s, total_time_elapsed t=967.0s\n",
      "706/706 [==============================] - 67s 95ms/step - loss: 3.2902 - val_loss: 3.2775\n",
      "Model finished with val loss: 3.277470588684082\n",
      "49/49 [==============================] - 7s 67ms/step\n",
      "Val Score: 0.8219070302989154\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Avg val score: 0.817636859097588 and std val score: 0.008919457416151183\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "fold_run_order = [2, 1, 3, 4, 5]\n",
    "\n",
    "val_preds_lst = []\n",
    "val_series_lst = []\n",
    "val_starts_splits_lst = []\n",
    "\n",
    "val_y_lst = []\n",
    "models_lst = []\n",
    "val_scores = []\n",
    "\n",
    "model_dct = {}\n",
    "\n",
    "for fold_num in fold_run_order:\n",
    "    \n",
    "    X_s, y_s, series_ids = load_data(cfg.processed_data_path)\n",
    "    \n",
    "\n",
    "    print(f'\\n\\n\\n-----------Starting fold: {fold_num}-----------\\n\\n\\n')\n",
    "    \n",
    "    series_ids_val = splits_df.filter(pl.col(f'{n_folds}_fold') == fold_num)['series_id'].to_numpy()\n",
    "\n",
    "    val_idxs = [series_ids.index(s) for s in series_ids_val]\n",
    "    trn_idxs = np.setdiff1d(np.arange(len(series_ids)), val_idxs)\n",
    "    \n",
    "    print(f'Using {len(trn_idxs)} series for training and {len(val_idxs)} series for validation')\n",
    "\n",
    "    X_s_trn, y_s_trn, series_ids_trn = [X_s[i] for i in trn_idxs], [y_s[i] for i in trn_idxs], [series_ids[i] for i in trn_idxs]\n",
    "    X_s_val, y_s_val, series_ids_val = [X_s[i] for i in val_idxs], [y_s[i] for i in val_idxs], [series_ids[i] for i in val_idxs]\n",
    "\n",
    "    trn_ds = SleepDataset(X_s_trn, y_s_trn, series_ids_trn, cfg.samp_freq, remove_no_dets=False, is_train=True)\n",
    "    norm_params = trn_ds.norm_params\n",
    "    model_dct[fold_num] = norm_params\n",
    "    \n",
    "    val_ds = SleepDataset(X_s_val, y_s_val, series_ids_val, cfg.samp_freq, remove_no_dets=False, is_train=False, norm_params=norm_params)\n",
    "\n",
    "    del X_s_trn, y_s_trn,  X_s_val, y_s_val, series_ids_trn, X_s, y_s, series_ids\n",
    "    _ = gc.collect()\n",
    "\n",
    "\n",
    "    model = get_model(trn_ds.X.shape[1:])\n",
    "    \n",
    "    print(f'Total model parameters: {model.count_params()}')\n",
    "\n",
    "    val_events_df = train_events.filter(pl.col('series_id').is_in(series_ids_val))\n",
    "    inter_eval = IntervalEvaluation(val_ds, val_events_df, cfg.samp_freq, (trn_ds.X.shape[0]//model_cfg.batch_size)*model_cfg.n_epochs, model_cfg.start_lr, model_cfg.end_lr)\n",
    "    \n",
    "    model.fit(trn_ds.X, trn_ds.y,\n",
    "          epochs=model_cfg.n_epochs,\n",
    "          batch_size=model_cfg.batch_size,\n",
    "          callbacks=[inter_eval],\n",
    "          verbose=1,\n",
    "          validation_data=(val_ds.X, val_ds.y))\n",
    "    \n",
    "    \n",
    "    \n",
    "    last_loss = model.history.history['val_loss'][-1]\n",
    "    print(f'Model finished with val loss: {last_loss}')\n",
    "    \n",
    "    model = inter_eval.best_model\n",
    "    val_preds = model.predict(val_ds.X)\n",
    "    \n",
    "    val_score = post_process_preds(val_events_df, val_preds, val_ds.series_ids, val_ds.starts_splits, cfg.samp_freq, get_score=True)\n",
    "    print(f'Val Score: {val_score}')\n",
    "    \n",
    "    val_scores.append(val_score)\n",
    "    \n",
    "    val_y_lst.append(val_ds.y)\n",
    "    val_preds_lst.append(val_preds)\n",
    "    val_series_lst.append(val_ds.series_ids)\n",
    "    val_starts_splits_lst.append(val_ds.starts_splits)\n",
    "    \n",
    "    tf.keras.models.save_model(model, os.path.join(cfg.output_dir, cfg.ver, f'tf_model_fold_{fold_num}.h5'))\n",
    "    \n",
    "    del trn_ds, val_ds, model, inter_eval\n",
    "    _ = gc.collect()\n",
    "\n",
    "avg_val_score, std_val_score = np.mean(val_scores), np.std(val_scores)\n",
    "print(f'Avg val score: {avg_val_score} and std val score: {std_val_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ff482f",
   "metadata": {},
   "source": [
    "###### series_ids_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6d9e397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7204, 17280, 2) (7204, 17280, 2) (7204,)\n"
     ]
    }
   ],
   "source": [
    "val_starts_splits_all = np.concatenate(val_starts_splits_lst)\n",
    "val_preds_all = np.concatenate(val_preds_lst)\n",
    "val_series_all = np.concatenate(val_series_lst)\n",
    "val_y_all = np.concatenate(val_y_lst)\n",
    "\n",
    "print(val_preds_all.shape, val_y_all.shape, val_series_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "990abfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127946340, 5)\n",
      "(122539680, 5)\n",
      "(124485120, 4)\n",
      "(122539680, 4)\n"
     ]
    }
   ],
   "source": [
    "# The code is creating a DataFrame `oof_preds_df` that contains the predicted values for the \"onset\" and \"wakeup\" columns.\n",
    "\n",
    "res_steps = []\n",
    "res_preds_onsets = []\n",
    "res_preds_wakeups = []\n",
    "\n",
    "res_series_ids = []\n",
    "\n",
    "start = 0\n",
    "while start < len(val_preds_all):\n",
    "    \n",
    "    end = start+1\n",
    "    while end < len(val_preds_all) and val_series_all[end] == val_series_all[start]:\n",
    "        end += 1\n",
    "        \n",
    "    preds = val_preds_all[start:end]\n",
    "    \n",
    "    steps = np.concatenate([val_starts_splits_all[idx] + np.arange(len(val_preds_all[idx])) for idx in range(start, end)])\n",
    "    preds = preds.reshape((preds.shape[0]*preds.shape[1]), 2)\n",
    "    \n",
    "    res_preds_onsets.append(preds[:, 0])\n",
    "    res_preds_wakeups.append(preds[:, 1])\n",
    "    \n",
    "    res_steps.append(steps)\n",
    "    ser_id = val_series_all[start]\n",
    "    \n",
    "    res_series_ids.append([ser_id for _ in range(len(preds))])\n",
    "    \n",
    "    start=end\n",
    "    \n",
    "    \n",
    "oof_preds_df = pl.DataFrame().with_columns([\n",
    "    pl.Series(np.concatenate(res_series_ids)).alias('series_id'),\n",
    "    pl.Series(np.concatenate(res_steps)).alias('step'),\n",
    "    pl.Series(np.concatenate(res_preds_onsets)).alias('onset'),\n",
    "    pl.Series(np.concatenate(res_preds_wakeups)).alias('wakeup')\n",
    "\n",
    "])\n",
    "\n",
    "train_series = pl.read_parquet(cfg.train_series_path)\n",
    "print(train_series.shape)\n",
    "train_series = train_series.filter(pl.col('series_id').is_in(list(np.unique(val_series_all))))\n",
    "train_series = train_series.with_columns(pl.col('step').cast(pl.Int64))\n",
    "print(train_series.shape)\n",
    "\n",
    "\n",
    "\n",
    "print(oof_preds_df.shape)\n",
    "oof_preds_df = train_series[['series_id', 'step']].join(oof_preds_df, on=['series_id', 'step'], how='left')\n",
    "print(oof_preds_df.shape)\n",
    "\n",
    "oof_preds_df.write_parquet(os.path.join(cfg.output_dir, cfg.ver, 'oof_preds.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "800ba20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03d92c9f6f8a\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 2\n",
    "# sid = val_series_all.filter(pl.col())\n",
    "plt.plot(val_y_all[i], label=\"Actual Values\")\n",
    "print(val_series_all[i])\n",
    "\n",
    "plt.plot(val_preds_all[i], label=\"Predicted Values\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2a3f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_nikhil(preds_orig, k_orig=150, max_thresh=0.05, max_count=500):\n",
    "    \n",
    "    preds=preds_orig.copy()\n",
    "    preds = np.convolve(preds, np.array([0.2, 0.6, 0.2]), mode='same')\n",
    "\n",
    "    count = 0\n",
    "    base=6.75\n",
    "    \n",
    "    k = k_orig\n",
    "    prev_max = None\n",
    "\n",
    "    scores = []\n",
    "    indices = []\n",
    "    \n",
    "    while True:\n",
    "            \n",
    "        curr_max_idx = np.argmax(preds)\n",
    "        curr_max = preds[curr_max_idx] \n",
    "        \n",
    "        if (curr_max < max_thresh) or count > max_count:\n",
    "            break\n",
    "        \n",
    "        indices.append(curr_max_idx)\n",
    "        scores.append(curr_max)\n",
    "        \n",
    "        preds[curr_max_idx] = 0\n",
    "        \n",
    "        supress_rates = np.logspace(start=0, stop=1, num=k, base=base)/base\n",
    "        supress_rates[:20] = 0\n",
    "        # supress_rates[20:] += 0.1\n",
    "\n",
    "        preds[max(curr_max_idx-k, 0):curr_max_idx] *= supress_rates[:min(k, curr_max_idx-0)][::-1]\n",
    "\n",
    "        preds[curr_max_idx+1:min(curr_max_idx+k+1, preds.shape[0])] *= supress_rates[:min(k, preds.shape[0]-curr_max_idx-1)]\n",
    "\n",
    "        prev_max = curr_max\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    return indices, scores\n",
    "\n",
    "def get_actual_preds(val_preds, val_series_ids, val_steps, type_):\n",
    "    times = []\n",
    "    series_ids = []\n",
    "    scores = []\n",
    "    scores_y = []\n",
    "    \n",
    "    for i in np.arange(len(val_preds)):\n",
    "        \n",
    "        vp_i = val_preds[i]\n",
    "        ser_id = val_series_ids[i]\n",
    "        \n",
    "        col_index = 0 if type_ == \"onset\" else 1\n",
    "        other_col_index = 1 if type_ == \"onset\" else 0\n",
    "        \n",
    "        preds = vp_i[:, col_index]\n",
    "        preds_other = vp_i[:, other_col_index]\n",
    "\n",
    "        height_thresh = 0.05\n",
    "        \n",
    "        peaks, peak_scores = nms_nikhil(preds)\n",
    "\n",
    "        times.extend(val_steps[i][peaks])\n",
    "        scores.extend(list(peak_scores))\n",
    "        series_ids.extend([ser_id] * len(peaks))\n",
    "\n",
    "    return np.array(series_ids), np.array(times), np.array(scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def post_process_preds(val_events_df, val_preds, val_series_ids, val_starts_splits, samp_freq, get_score=False):\n",
    "    \n",
    "#     val_steps = np.concatenate([val_starts_splits[idx] + np.arange(len(val_preds[idx])) for idx in range(val_preds.shape[0])])\n",
    "\n",
    "    val_res = []\n",
    "    \n",
    "    prev_ser_id = None\n",
    "    \n",
    "    res_steps = []\n",
    "    res_preds = []\n",
    "    res_series_ids = []\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(val_preds):\n",
    "        \n",
    "        end = start+1\n",
    "        while end < len(val_preds) and val_series_ids[end] == val_series_ids[start]:\n",
    "            end += 1\n",
    "            \n",
    "        preds = val_preds[start:end]\n",
    "        \n",
    "        steps = np.concatenate([val_starts_splits[idx] + np.arange(len(val_preds[idx])) for idx in range(start, end)])\n",
    "        preds = preds.reshape((preds.shape[0]*preds.shape[1]), 2)\n",
    "        \n",
    "        res_preds.append(preds)\n",
    "        res_steps.append(steps)\n",
    "        res_series_ids.append(val_series_ids[start])\n",
    "        \n",
    "        start=end\n",
    "        \n",
    "        \n",
    "#     print(len(res_series_ids), len(res_steps), len(res_preds))\n",
    "\n",
    "    series_ids_onsets, onsets,  scores_onsets = get_actual_preds(res_preds, res_series_ids, res_steps, 'onset')\n",
    "    series_ids_wakeups, wakeups,  scores_wakeups =get_actual_preds(res_preds, res_series_ids, res_steps, 'wakeup')\n",
    "    \n",
    "    \n",
    "    onset_preds = pl.DataFrame().with_columns([pl.Series(series_ids_onsets).alias('series_id'),\n",
    "                                           pl.Series(onsets).cast(pl.Int64).alias('step'),\n",
    "                                           pl.lit('onset').alias('event'),\n",
    "                                           pl.Series(scores_onsets).alias('score')])\n",
    "\n",
    "    wakeup_preds = pl.DataFrame().with_columns([pl.Series(series_ids_wakeups).alias('series_id'),\n",
    "                                               pl.Series(wakeups).cast(pl.Int64).cast(pl.Int64).alias('step'),\n",
    "                                               pl.lit('wakeup').alias('event'),\n",
    "                                               pl.Series(scores_wakeups).alias('score')])\n",
    "    \n",
    "    val_preds_df = pl.concat([onset_preds, wakeup_preds]).sort(by=['series_id', 'step'])\n",
    "    \n",
    "    if get_score:\n",
    "        toleranaces = {'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]}\n",
    "        comp_score = metric_fast.comp_scorer(\n",
    "        val_events_df,\n",
    "        val_preds_df,\n",
    "        tolerances = toleranaces,\n",
    "        )\n",
    "        return comp_score\n",
    "    \n",
    "    else:\n",
    "        return val_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10354814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8022631312494917\n",
      "0.8294395003530312\n",
      "0.816764528932012\n",
      "0.8176620669745143\n",
      "0.8230182746728448\n",
      "CPU times: user 30 s, sys: 39.6 s, total: 1min 9s\n",
      "Wall time: 1min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8178295004363788, 0.009002105576734986)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "scores = []\n",
    "\n",
    "for fold_num in range(1, 5+1):\n",
    "    \n",
    "    test_ser_ids = list(np.unique(val_series_lst[fold_num-1]))\n",
    "\n",
    "\n",
    "    val_events_df = train_events.filter(pl.col('series_id').is_in(test_ser_ids))\n",
    "    score = post_process_preds(val_events_df,\n",
    "                               val_preds_lst[fold_num-1],\n",
    "                               val_series_lst[fold_num-1],\n",
    "                               val_starts_splits_lst[fold_num-1],\n",
    "                               cfg.samp_freq,\n",
    "                               get_score=True)\n",
    "    print(score)\n",
    "    scores.append(score)\n",
    "    \n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e911c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds_df = post_process_preds(val_events_df, val_preds_all, val_series_all, val_starts_splits_all, cfg.samp_freq, get_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b93f28c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_nearest_multiple(df, column_name='step'):\n",
    "    df[column_name] = df[column_name].apply(lambda x: x+1 if x%12==0  else x)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b63863c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds_df = round_to_nearest_multiple(val_preds_df.to_pandas())\n",
    "val_preds_df = pl.DataFrame(val_preds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02c02a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerances = {'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "               'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e72bc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9_386, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>series_id</th><th>night</th><th>event</th><th>step</th><th>timestamp</th><th>date</th></tr><tr><td>str</td><td>i64</td><td>str</td><td>i64</td><td>datetime[μs]</td><td>date</td></tr></thead><tbody><tr><td>&quot;038441c925bb&quot;</td><td>1</td><td>&quot;onset&quot;</td><td>4992</td><td>2018-08-14 22:26:00</td><td>2018-08-14</td></tr><tr><td>&quot;038441c925bb&quot;</td><td>1</td><td>&quot;wakeup&quot;</td><td>10932</td><td>2018-08-15 06:41:00</td><td>2018-08-15</td></tr><tr><td>&quot;038441c925bb&quot;</td><td>2</td><td>&quot;onset&quot;</td><td>20244</td><td>2018-08-15 19:37:00</td><td>2018-08-15</td></tr><tr><td>&quot;038441c925bb&quot;</td><td>2</td><td>&quot;wakeup&quot;</td><td>27492</td><td>2018-08-16 05:41:00</td><td>2018-08-16</td></tr><tr><td>&quot;038441c925bb&quot;</td><td>3</td><td>&quot;onset&quot;</td><td>39996</td><td>2018-08-16 23:03:00</td><td>2018-08-16</td></tr><tr><td>&quot;038441c925bb&quot;</td><td>3</td><td>&quot;wakeup&quot;</td><td>44400</td><td>2018-08-17 05:10:00</td><td>2018-08-17</td></tr><tr><td>&quot;038441c925bb&quot;</td><td>4</td><td>&quot;onset&quot;</td><td>57240</td><td>2018-08-17 23:00:00</td><td>2018-08-17</td></tr><tr><td>&quot;038441c925bb&quot;</td><td>4</td><td>&quot;wakeup&quot;</td><td>62856</td><td>2018-08-18 06:48:00</td><td>2018-08-18</td></tr><tr><td>&quot;038441c925bb&quot;</td><td>6</td><td>&quot;onset&quot;</td><td>91296</td><td>2018-08-19 22:18:00</td><td>2018-08-19</td></tr><tr><td>&quot;038441c925bb&quot;</td><td>6</td><td>&quot;wakeup&quot;</td><td>97860</td><td>2018-08-20 07:25:00</td><td>2018-08-20</td></tr><tr><td>&quot;038441c925bb&quot;</td><td>7</td><td>&quot;onset&quot;</td><td>109500</td><td>2018-08-20 23:35:00</td><td>2018-08-20</td></tr><tr><td>&quot;038441c925bb&quot;</td><td>7</td><td>&quot;wakeup&quot;</td><td>118524</td><td>2018-08-21 12:07:00</td><td>2018-08-21</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;fe90110788d2&quot;</td><td>29</td><td>&quot;onset&quot;</td><td>488424</td><td>2017-09-01 23:52:00</td><td>2017-09-01</td></tr><tr><td>&quot;fe90110788d2&quot;</td><td>29</td><td>&quot;wakeup&quot;</td><td>494952</td><td>2017-09-02 08:56:00</td><td>2017-09-02</td></tr><tr><td>&quot;fe90110788d2&quot;</td><td>30</td><td>&quot;onset&quot;</td><td>505116</td><td>2017-09-02 23:03:00</td><td>2017-09-02</td></tr><tr><td>&quot;fe90110788d2&quot;</td><td>30</td><td>&quot;wakeup&quot;</td><td>511284</td><td>2017-09-03 07:37:00</td><td>2017-09-03</td></tr><tr><td>&quot;fe90110788d2&quot;</td><td>31</td><td>&quot;onset&quot;</td><td>522852</td><td>2017-09-03 23:41:00</td><td>2017-09-03</td></tr><tr><td>&quot;fe90110788d2&quot;</td><td>31</td><td>&quot;wakeup&quot;</td><td>529104</td><td>2017-09-04 08:22:00</td><td>2017-09-04</td></tr><tr><td>&quot;fe90110788d2&quot;</td><td>32</td><td>&quot;onset&quot;</td><td>538956</td><td>2017-09-04 22:03:00</td><td>2017-09-04</td></tr><tr><td>&quot;fe90110788d2&quot;</td><td>32</td><td>&quot;wakeup&quot;</td><td>547152</td><td>2017-09-05 09:26:00</td><td>2017-09-05</td></tr><tr><td>&quot;fe90110788d2&quot;</td><td>33</td><td>&quot;onset&quot;</td><td>556560</td><td>2017-09-05 22:30:00</td><td>2017-09-05</td></tr><tr><td>&quot;fe90110788d2&quot;</td><td>33</td><td>&quot;wakeup&quot;</td><td>560604</td><td>2017-09-06 04:07:00</td><td>2017-09-06</td></tr><tr><td>&quot;fe90110788d2&quot;</td><td>34</td><td>&quot;onset&quot;</td><td>574620</td><td>2017-09-06 23:35:00</td><td>2017-09-06</td></tr><tr><td>&quot;fe90110788d2&quot;</td><td>34</td><td>&quot;wakeup&quot;</td><td>581604</td><td>2017-09-07 09:17:00</td><td>2017-09-07</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9_386, 6)\n",
       "┌──────────────┬───────┬────────┬────────┬─────────────────────┬────────────┐\n",
       "│ series_id    ┆ night ┆ event  ┆ step   ┆ timestamp           ┆ date       │\n",
       "│ ---          ┆ ---   ┆ ---    ┆ ---    ┆ ---                 ┆ ---        │\n",
       "│ str          ┆ i64   ┆ str    ┆ i64    ┆ datetime[μs]        ┆ date       │\n",
       "╞══════════════╪═══════╪════════╪════════╪═════════════════════╪════════════╡\n",
       "│ 038441c925bb ┆ 1     ┆ onset  ┆ 4992   ┆ 2018-08-14 22:26:00 ┆ 2018-08-14 │\n",
       "│ 038441c925bb ┆ 1     ┆ wakeup ┆ 10932  ┆ 2018-08-15 06:41:00 ┆ 2018-08-15 │\n",
       "│ 038441c925bb ┆ 2     ┆ onset  ┆ 20244  ┆ 2018-08-15 19:37:00 ┆ 2018-08-15 │\n",
       "│ 038441c925bb ┆ 2     ┆ wakeup ┆ 27492  ┆ 2018-08-16 05:41:00 ┆ 2018-08-16 │\n",
       "│ …            ┆ …     ┆ …      ┆ …      ┆ …                   ┆ …          │\n",
       "│ fe90110788d2 ┆ 33    ┆ onset  ┆ 556560 ┆ 2017-09-05 22:30:00 ┆ 2017-09-05 │\n",
       "│ fe90110788d2 ┆ 33    ┆ wakeup ┆ 560604 ┆ 2017-09-06 04:07:00 ┆ 2017-09-06 │\n",
       "│ fe90110788d2 ┆ 34    ┆ onset  ┆ 574620 ┆ 2017-09-06 23:35:00 ┆ 2017-09-06 │\n",
       "│ fe90110788d2 ┆ 34    ┆ wakeup ┆ 581604 ┆ 2017-09-07 09:17:00 ┆ 2017-09-07 │\n",
       "└──────────────┴───────┴────────┴────────┴─────────────────────┴────────────┘"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5dcf3c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.816398799536687"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_fast.comp_scorer(train_events, val_preds_df, tolerances=tolerances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3bbdae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:11<00:00,  4.71it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_partitions  = val_preds_df.partition_by(by=['series_id'], maintain_order=True, as_dict=True)\n",
    "events_partitions  = val_events_df.partition_by(by=['series_id'], maintain_order=True, as_dict=True)\n",
    "\n",
    "tolerances = {'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "               'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]}\n",
    "\n",
    "scores_dict = {}\n",
    "for ser_id in tqdm(events_partitions.keys()):\n",
    "    scores_dict[ser_id] = metric_fast.comp_scorer(events_partitions[ser_id], preds_partitions[ser_id],\n",
    "                                                  tolerances)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47012ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f56824b503a0    0.104892\n",
       "854206f602d0    0.341218\n",
       "703b5efa9bc1    0.476915\n",
       "0a96f4993bd7    0.567911\n",
       "6ca4f4fca6a2    0.620421\n",
       "280e08693c6d    0.660638\n",
       "cf13ed7e457a    0.725000\n",
       "1762ab70ec76    0.734673\n",
       "8b8b9e29171c    0.775790\n",
       "72bbd1ac3edf    0.776648\n",
       "ccdee561ee5d    0.791009\n",
       "91cb6c98201f    0.797848\n",
       "dfc3ccebfdc9    0.800082\n",
       "038441c925bb    0.808368\n",
       "a167532acca2    0.809247\n",
       "51b23d177971    0.816383\n",
       "694faf956ebf    0.821412\n",
       "0402a003dae9    0.821771\n",
       "6bf95a3cf91c    0.821938\n",
       "b84960841a75    0.825812\n",
       "3d53bfea61d6    0.838426\n",
       "fe90110788d2    0.838430\n",
       "d2d6b9af0553    0.844190\n",
       "e0d7b0dcf9f3    0.853044\n",
       "483d6545417f    0.853561\n",
       "ce85771a714c    0.857657\n",
       "8898e6db816d    0.868157\n",
       "d150801f3145    0.870137\n",
       "f88e18cb4100    0.876963\n",
       "ebd76e93ec7d    0.880675\n",
       "c38707ef76df    0.887521\n",
       "18b61dd5aae8    0.888008\n",
       "062cae666e2a    0.888333\n",
       "a81f4472c637    0.894801\n",
       "804594bb1f06    0.897186\n",
       "1955d568d987    0.901855\n",
       "91127c2b0e60    0.903690\n",
       "ce9164297046    0.912093\n",
       "72d2234e84e4    0.912264\n",
       "d043c0ca71cd    0.912957\n",
       "3664fe9233f9    0.913353\n",
       "25e2b3dd9c3b    0.921470\n",
       "599ca4ed791b    0.924863\n",
       "29d3469bd15d    0.928046\n",
       "78569a801a38    0.930691\n",
       "40dce6018935    0.931799\n",
       "c3072a759efb    0.932345\n",
       "08db4255286f    0.933033\n",
       "927dd0c35dfd    0.939362\n",
       "b7188813d58a    0.940270\n",
       "bb5612895813    0.950770\n",
       "76237b9406d5    0.968889\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(scores_dict).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6a12ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_attributes_dict = {k: v for k, v in cfg.__dict__.items() if not k.startswith('__') and not callable(v)}\n",
    "joblib.dump(cfg_attributes_dict, os.path.join(cfg.output_dir, cfg.ver, 'cfg.pkl'))\n",
    "joblib.dump(model_dct, os.path.join(cfg.output_dir, cfg.ver, 'model_dct.pkl'))\n",
    "\n",
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_series_all.npy'), val_series_all)\n",
    "\n",
    "\n",
    "val_preds_df.to_pandas().to_csv(os.path.join(cfg.output_dir, cfg.ver, 'val_preds_df.csv'), index=False)\n",
    "\n",
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_preds_all.npy'), val_preds_all)\n",
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_y_all.npy'), val_y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89201dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "meta_json = {\n",
    "  \"title\": f\"sleep-model-{cfg.ver}\",\n",
    "  \"id\": f\"nikhilmishradev/sleep-model-{cfg.ver}\",\n",
    "  \"licenses\": [\n",
    "    {\n",
    "      \"name\": \"CC0-1.0\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "json.dump(meta_json, open(os.path.join(cfg.output_dir, cfg.ver, 'dataset-metadata.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "482e8991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/.kaggle/kaggle.json'\n",
      "Skipping folder: .ipynb_checkpoints; use '--dir-mode' to upload folders\n",
      "Starting upload for file cfg.pkl\n",
      "100%|███████████████████████████████████████████| 119/119 [00:02<00:00, 58.7B/s]\n",
      "Upload successful: cfg.pkl (119B)\n",
      "Starting upload for file model_dct.pkl\n",
      "100%|████████████████████████████████████████| 1.99k/1.99k [00:02<00:00, 889B/s]\n",
      "Upload successful: model_dct.pkl (2KB)\n",
      "Starting upload for file oof_preds.parquet\n",
      "100%|██████████████████████████████████████| 1.02G/1.02G [00:59<00:00, 18.5MB/s]\n",
      "Upload successful: oof_preds.parquet (1GB)\n",
      "Starting upload for file tf_model_fold_1.h5\n",
      "100%|████████████████████████████████████████| 133M/133M [00:10<00:00, 12.9MB/s]\n",
      "Upload successful: tf_model_fold_1.h5 (133MB)\n",
      "Starting upload for file tf_model_fold_2.h5\n",
      "100%|████████████████████████████████████████| 133M/133M [00:11<00:00, 12.3MB/s]\n",
      "Upload successful: tf_model_fold_2.h5 (133MB)\n",
      "Starting upload for file tf_model_fold_3.h5\n",
      "100%|████████████████████████████████████████| 133M/133M [00:11<00:00, 12.3MB/s]\n",
      "Upload successful: tf_model_fold_3.h5 (133MB)\n",
      "Starting upload for file tf_model_fold_4.h5\n",
      "100%|████████████████████████████████████████| 133M/133M [00:12<00:00, 11.3MB/s]\n",
      "Upload successful: tf_model_fold_4.h5 (133MB)\n",
      "Starting upload for file tf_model_fold_5.h5\n",
      "100%|████████████████████████████████████████| 133M/133M [00:10<00:00, 13.4MB/s]\n",
      "Upload successful: tf_model_fold_5.h5 (133MB)\n",
      "Starting upload for file val_preds_df.csv\n",
      "100%|██████████████████████████████████████| 11.4M/11.4M [00:04<00:00, 2.97MB/s]\n",
      "Upload successful: val_preds_df.csv (11MB)\n",
      "Starting upload for file val_series_all.npy\n",
      "100%|█████████████████████████████████████████| 338k/338k [00:02<00:00, 143kB/s]\n",
      "Upload successful: val_series_all.npy (338KB)\n",
      "Your private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/nikhilmishradev/sleep-model-fm-v13-final\n"
     ]
    }
   ],
   "source": [
    "# !rm -r ../outputs/vx/*\n",
    "# !cp -r {os.path.join(cfg.output_dir, cfg.ver)}/* ../outputs/vx\n",
    "# !rm ../outputs/vx/val_preds_all.npy ../outputs/vx/val_y_all.npy\n",
    "# !pip install -q kaggle\n",
    "# !kaggle datasets create -p ../outputs/vx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4ed9dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_preds_all.npy'), val_preds_all)\n",
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_y_all.npy'), val_y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61dd1def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fm-v13-final'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf4d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fa6aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
