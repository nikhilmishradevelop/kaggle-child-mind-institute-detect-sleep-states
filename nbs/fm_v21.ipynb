{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b4ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The line `# !pip install --upgrade -q numpy numba polars lightgbm tensorflow-addons` is a command that installs or upgrades several Python packages.\n",
    "# !pip install --upgrade -q numpy numba polars lightgbm tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b94e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 16:49:30.818246: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'src.metric_fast' from '/home/sleep-kaggle/kaggle_final_solution/../src/metric_fast.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src import metric_fast\n",
    "import joblib\n",
    "import time\n",
    "import importlib\n",
    "importlib.reload(metric_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    samp_freq=1\n",
    "    gaussian_overlay_len = 60\n",
    "    std_dev_num = 2400\n",
    "    ver='fm-v21-final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6d7e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_json = json.load(open('../settings.json', 'r'))\n",
    "print(settings_json)\n",
    "\n",
    "for k,v in settings_json.items():\n",
    "    setattr(cfg, k, v)\n",
    "    \n",
    "print(cfg.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2384741",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_events = pl.read_ipc(os.path.join(cfg.processed_data_path, 'train_events.ipc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_df = train_events[['series_id']].unique(maintain_order=True)\n",
    "splits_df = splits_df.to_pandas()\n",
    "\n",
    "for n_splits in [5, 7, 10]:\n",
    "    folds = KFold(n_splits, shuffle=True, random_state=258)\n",
    "\n",
    "    splits_df[f'{n_splits}_fold'] = 0\n",
    "    for i, (trn_idx, val_idx) in enumerate(folds.split(splits_df['series_id'], splits_df['series_id'])):\n",
    "        \n",
    "        splits_df.loc[val_idx, f'{n_splits}_fold'] = i+1\n",
    "                                       \n",
    "    \n",
    "splits_df = pl.DataFrame(splits_df)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85190e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory):\n",
    "    X_s = []\n",
    "    y_s = []\n",
    "    series_ids = []\n",
    "\n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        if filename.endswith('_X.npy'):\n",
    "            series_id = filename.split('_X.npy')[0]\n",
    "            X = np.load(os.path.join(directory, filename))\n",
    "            y = np.load(os.path.join(directory, f'{series_id}_y.npy'))\n",
    "\n",
    "            X_s.append(X)\n",
    "            y_s.append(y)\n",
    "            series_ids.append(series_id)\n",
    "\n",
    "    return X_s, y_s, series_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ad2da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_y(y):\n",
    "    \n",
    "    for i in range(y.shape[1]):\n",
    "        \n",
    "        mean = y[:,i].mean()\n",
    "        std = y[:,i].std()\n",
    "        y[:,i] = (y[:,i]-mean)/(std+1e-16)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d507fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c515709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_pad(X, y=None, split_length=1440, stride=1440):\n",
    "    \"\"\"\n",
    "    Splits and pads the arrays X and y using a sliding window.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.array): Array with shape (timesteps, features)\n",
    "    - y (np.array): Array with shape (timesteps, 2)\n",
    "    - split_length (int): Desired timestep length for the resulting arrays.\n",
    "    - stride (int): Step size for sliding window.\n",
    "\n",
    "    Returns:\n",
    "    - List of arrays for X and y, each with shape (split_length, features)\n",
    "    \"\"\"\n",
    "    \n",
    "    if y is not None and len(X) != len(y):\n",
    "        raise ValueError(\"X and y should have the same number of timesteps.\")\n",
    "    \n",
    "    timesteps, features = X.shape\n",
    "    \n",
    "    # Create empty lists to store split segments\n",
    "    X_splits = []\n",
    "    y_splits = []\n",
    "    starts = []\n",
    "    \n",
    "    # Use sliding window to extract segments\n",
    "    \n",
    "    for start in range(0, timesteps, stride):\n",
    "        end = start + split_length\n",
    "        if end <= timesteps:\n",
    "            starts.append(start)\n",
    "            X_splits.append(X[start:end].copy())\n",
    "            if y is not None:\n",
    "                y_splits.append(y[start:end].copy())\n",
    "        else:\n",
    "            # If the segment is shorter than split_length, pad it\n",
    "            starts.append(start)\n",
    "            padding_length = end - timesteps\n",
    "            X_segment_padded = np.pad(X[start:], ((0, padding_length), (0, 0)), mode='constant', constant_values=-9)\n",
    "            X_splits.append(X_segment_padded)\n",
    "            \n",
    "            if y is not None:\n",
    "                y_segment_padded = np.pad(y[start:], ((0, padding_length), (0, 0)), mode='constant', constant_values=0)\n",
    "                y_splits.append(y_segment_padded)\n",
    "                \n",
    "            break\n",
    "            \n",
    "            \n",
    "    if y is not None:\n",
    "        return X_splits, y_splits, starts\n",
    "    \n",
    "    return X_splits, starts\n",
    "\n",
    "\n",
    "\n",
    "class SleepDataset:\n",
    "    \n",
    "    def __init__(self, X_s, y_s=None, series_ids=None, samp_freq=None, remove_no_dets=True, is_train=False, split_factor=1, norm_params=None):\n",
    "        \n",
    "        self.split_len = (24*60*12) // cfg.samp_freq\n",
    "        self.split_strides = self.split_len\n",
    "        self.remove_no_dets = remove_no_dets\n",
    "        self.is_train = is_train\n",
    "\n",
    "        print(f'Using a split len of {self.split_len}')\n",
    "        self.create_dataset(X_s, y_s, series_ids)\n",
    "\n",
    "        if self.is_train:\n",
    "            self.norm_params = self.calculate_norm_params()\n",
    "        else:\n",
    "            if norm_params is None:\n",
    "                raise ValueError(\"Normalization parameters must be provided for non-training data.\")\n",
    "            self.norm_params = norm_params\n",
    "\n",
    "        self.normalize_data()\n",
    "\n",
    "    def calculate_norm_params(self):\n",
    "        mean = np.mean(self.X, axis=(0, 1))\n",
    "        std = np.std(self.X, axis=(0, 1))\n",
    "        return {'mean': mean, 'std': std}\n",
    "\n",
    "    def normalize_data(self):\n",
    "        self.X = (self.X - self.norm_params['mean']) / (1e-6 + self.norm_params['std'])\n",
    "        \n",
    "        \n",
    "    def create_dataset(self, X_s, y_s=None, series_ids=None):\n",
    "        X_s_splits, y_s_splits, series_splits, starts_splits = [], [], [], []\n",
    "\n",
    "        for i in tqdm(range(len(X_s))):\n",
    "            x_splits, starts = split_and_pad(X_s[i].copy(), split_length = self.split_len, stride=self.split_strides)\n",
    "            X_s_splits.extend(x_splits)\n",
    "            starts_splits.extend(starts)\n",
    "\n",
    "            if y_s is not None:\n",
    "                _, y_splits, _ = split_and_pad(X_s[i].copy(), y_s[i].copy(), split_length=self.split_len, stride=self.split_strides)\n",
    "                y_s_splits.extend(y_splits)\n",
    "\n",
    "            if series_ids is not None:\n",
    "                series_splits.extend([series_ids[i] for _ in range(len(x_splits))])\n",
    "            \n",
    "        self.X = np.array(X_s_splits)\n",
    "        self.starts_splits = np.array(starts_splits)\n",
    "        \n",
    "        if y_s is not None:\n",
    "            self.y = np.array(y_s_splits)\n",
    "\n",
    "            if self.remove_no_dets:\n",
    "                fltr = (self.y[:, :, 1].sum(axis=1) + self.y[:, :, 0].sum(axis=1)) != 0\n",
    "                self.X = self.X[fltr]\n",
    "                self.y = self.y[fltr]\n",
    "                if series_ids is not None:\n",
    "                    self.series_ids = np.array(series_splits)[fltr]\n",
    "                else:\n",
    "                    self.series_ids = None\n",
    "            \n",
    "            self.y = np.array([normalize_y(yts) for yts in self.y])\n",
    "\n",
    "        else:\n",
    "            self.y = None\n",
    "\n",
    "        if series_ids is not None:\n",
    "            if not hasattr(self, 'series_ids'):\n",
    "                self.series_ids = np.array(series_splits)\n",
    "            print(f'X: {self.X.shape}, y: {self.y.shape if self.y is not None else \"Not provided\"}, series_ids: {self.series_ids.shape}')\n",
    "        else:\n",
    "            print(f'X: {self.X.shape}, y: {self.y.shape if self.y is not None else \"Not provided\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20edbd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_nikhil(preds_orig, k_orig=125, max_thresh=0.05, max_count=1000):\n",
    "    \n",
    "    preds=preds_orig.copy()\n",
    "\n",
    "    count = 0\n",
    "    base=6.75\n",
    "    \n",
    "    k = k_orig\n",
    "    prev_max = None\n",
    "\n",
    "    scores = []\n",
    "    indices = []\n",
    "    \n",
    "    while True:\n",
    "            \n",
    "        curr_max_idx = np.argmax(preds)\n",
    "        curr_max = preds[curr_max_idx] \n",
    "        \n",
    "        if (curr_max < max_thresh) or count > max_count:\n",
    "            break\n",
    "        \n",
    "        indices.append(curr_max_idx)\n",
    "        scores.append(curr_max)\n",
    "        \n",
    "        preds[curr_max_idx] = 0\n",
    "        \n",
    "        supress_rates = np.logspace(start=0, stop=1, num=k, base=base)/base\n",
    "\n",
    "        preds[max(curr_max_idx-k, 0):curr_max_idx] *= supress_rates[:min(k, curr_max_idx-0)][::-1]\n",
    "        preds[curr_max_idx+1:min(curr_max_idx+k+1, preds.shape[0])] *= supress_rates[:min(k, preds.shape[0]-curr_max_idx-1)]\n",
    "\n",
    "        prev_max = curr_max\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    return indices, scores\n",
    "\n",
    "def get_actual_preds(val_preds, val_series_ids, val_steps, type_):\n",
    "    times = []\n",
    "    series_ids = []\n",
    "    scores = []\n",
    "    scores_y = []\n",
    "    \n",
    "    for i in np.arange(len(val_preds)):\n",
    "        \n",
    "        vp_i = val_preds[i]\n",
    "        ser_id = val_series_ids[i]\n",
    "        \n",
    "        col_index = 0 if type_ == \"onset\" else 1\n",
    "        other_col_index = 1 if type_ == \"onset\" else 0\n",
    "        \n",
    "        preds = vp_i[:, col_index]\n",
    "        preds_other = vp_i[:, other_col_index]\n",
    "\n",
    "        height_thresh = 0.05\n",
    "        \n",
    "        peaks, peak_scores = nms_nikhil(preds)\n",
    "\n",
    "        times.extend(val_steps[i][peaks])\n",
    "        scores.extend(list(peak_scores))\n",
    "        series_ids.extend([ser_id] * len(peaks))\n",
    "\n",
    "    return np.array(series_ids), np.array(times), np.array(scores)\n",
    "\n",
    "\n",
    "def post_process_preds(val_events_df, val_preds, val_series_ids, val_starts_splits, samp_freq, get_score=False):\n",
    "    \n",
    "#     val_steps = np.concatenate([val_starts_splits[idx] + np.arange(len(val_preds[idx])) for idx in range(val_preds.shape[0])])\n",
    "\n",
    "    val_res = []\n",
    "    \n",
    "    prev_ser_id = None\n",
    "    \n",
    "    res_steps = []\n",
    "    res_preds = []\n",
    "    res_series_ids = []\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(val_preds):\n",
    "        \n",
    "        end = start+1\n",
    "        while end < len(val_preds) and val_series_ids[end] == val_series_ids[start]:\n",
    "            end += 1\n",
    "            \n",
    "        preds = val_preds[start:end]\n",
    "        \n",
    "        steps = np.concatenate([val_starts_splits[idx] + np.arange(len(val_preds[idx])) for idx in range(start, end)])\n",
    "        preds = preds.reshape((preds.shape[0]*preds.shape[1]), 2)\n",
    "        \n",
    "        res_preds.append(preds)\n",
    "        res_steps.append(steps)\n",
    "        res_series_ids.append(val_series_ids[start])\n",
    "        \n",
    "        start=end\n",
    "        \n",
    "        \n",
    "#     print(len(res_series_ids), len(res_steps), len(res_preds))\n",
    "\n",
    "    series_ids_onsets, onsets,  scores_onsets = get_actual_preds(res_preds, res_series_ids, res_steps, 'onset')\n",
    "    series_ids_wakeups, wakeups,  scores_wakeups =get_actual_preds(res_preds, res_series_ids, res_steps, 'wakeup')\n",
    "    \n",
    "    \n",
    "    onset_preds = pl.DataFrame().with_columns([pl.Series(series_ids_onsets).alias('series_id'),\n",
    "                                           pl.Series(onsets).cast(pl.Int64).alias('step'),\n",
    "                                           pl.lit('onset').alias('event'),\n",
    "                                           pl.Series(scores_onsets).alias('score')])\n",
    "\n",
    "    wakeup_preds = pl.DataFrame().with_columns([pl.Series(series_ids_wakeups).alias('series_id'),\n",
    "                                               pl.Series(wakeups).cast(pl.Int64).cast(pl.Int64).alias('step'),\n",
    "                                               pl.lit('wakeup').alias('event'),\n",
    "                                               pl.Series(scores_wakeups).alias('score')])\n",
    "    \n",
    "    val_preds_df = pl.concat([onset_preds, wakeup_preds]).sort(by=['series_id', 'step'])\n",
    "    \n",
    "    if get_score:\n",
    "        toleranaces = {'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]}\n",
    "        comp_score = metric_fast.comp_scorer(\n",
    "        val_events_df,\n",
    "        val_preds_df,\n",
    "        tolerances = toleranaces,\n",
    "        )\n",
    "        return comp_score\n",
    "    \n",
    "    else:\n",
    "        return val_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a630548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_exponential_lr(start_lr, end_lr, num_steps, decay_rate=None):\n",
    "    \"\"\"\n",
    "    Calculate the exponentially decreasing learning rates.\n",
    "\n",
    "    Parameters:\n",
    "    start_lr (float): Initial learning rate.\n",
    "    end_lr (float): Final learning rate.\n",
    "    num_steps (int): Total number of steps over which the learning rate should decay.\n",
    "    decay_rate (float): Decay rate per step. If None, it will be computed based on start_lr, end_lr, and num_steps.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing the learning rate for each step.\n",
    "    \"\"\"\n",
    "    if decay_rate is None:\n",
    "        # Calculate decay rate based on the start_lr, end_lr, and num_steps\n",
    "        decay_rate = (end_lr / start_lr) ** (1 / (num_steps - 1))\n",
    "\n",
    "    learning_rates = [start_lr * (decay_rate ** step) for step in range(num_steps)]\n",
    "    return learning_rates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cf0fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntervalEvaluation(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_ds, val_events_df, samp_freq, n_steps, start_lr, end_lr):\n",
    "\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.val_ds = val_ds\n",
    "        self.val_events_df = val_events_df\n",
    "        self.samp_freq = samp_freq\n",
    "        \n",
    "        warmup_pct_steps = 0.25\n",
    "        warmup_steps = int(n_steps * warmup_pct_steps)\n",
    "        self.learning_rates = [start_lr] * (warmup_steps) + calculate_exponential_lr(start_lr, end_lr, n_steps-warmup_steps)\n",
    "        self.best_score = -np.inf\n",
    "        self.best_model = None\n",
    "        self.step_count=0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        if epoch == 0:\n",
    "            self.first_epoch_start_time = self.start_time\n",
    "        \n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        if self.step_count < len(self.learning_rates):\n",
    "\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, self.learning_rates[self.step_count])\n",
    "            self.curr_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        self.step_count += 1\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        val_preds = self.model.predict(self.val_ds.X, batch_size=8, verbose=0)[:, :, :2]\n",
    "        val_score = post_process_preds(self.val_events_df, val_preds, self.val_ds.series_ids, self.val_ds.starts_splits, self.samp_freq, get_score=True)\n",
    "        \n",
    "        if val_score > self.best_score:\n",
    "            self.best_score = val_score\n",
    "            self.best_model = tf.keras.models.clone_model(self.model)\n",
    "            self.best_model.set_weights(self.model.get_weights()) \n",
    "        \n",
    "        total_time = round(time.time() - self.start_time, 2)\n",
    "        total_seconds_till_now = round(time.time() - self.first_epoch_start_time, 0)\n",
    "        \n",
    "        print(f\"Epoch: {epoch:03d} curr_lr: {self.curr_lr:.1e} - train_loss: {logs['loss']:.04f} - val_loss: {logs['val_loss']:.04f} val_score: {val_score:.03f}  best_val_score: {self.best_score:.03f}  last_epoch t={total_time:.02f}s, total_time_elapsed t={total_seconds_till_now}s\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaf8b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "\n",
    "    # MultiHead Attention\n",
    "    x = tfa.layers.MultiHeadAttention(\n",
    "        head_size=head_size,\n",
    "        num_heads=num_heads,\n",
    "        use_projection_bias=True,\n",
    "        dropout=dropout\n",
    "    )([inputs, inputs, inputs])\n",
    "\n",
    "    # Residual connection with LayerNormalization and Scaling\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + inputs) * (0.5 ** 0.5)\n",
    "    \n",
    "    # Feed Forward Part\n",
    "    ff = tf.keras.layers.Dense(ff_dim, activation='relu')(x)\n",
    "    # ff = tf.keras.layers.Dropout(dropout)(ff)\n",
    "    ff = tf.keras.layers.Dense(inputs.shape[-1])(ff)\n",
    "    # ff = tf.keras.layers.Dropout(dropout)(ff)\n",
    "    \n",
    "    # Residual connection with LayerNormalization and Scaling for FFN\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + ff) * (0.5 ** 0.5)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10d34cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mce_loss(y_true, y_pred):\n",
    "    # Clip the ground truth and predictions to the range (-100, 100)\n",
    "    y_true_clipped = tf.clip_by_value(y_true, -100, 100)\n",
    "    y_pred_clipped = tf.clip_by_value(y_pred, -100, 100)\n",
    "\n",
    "    # Calculate the mean cubed error\n",
    "    loss = tf.reduce_mean(tf.abs(y_true_clipped - y_pred_clipped) ** 3)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 16:49:33.451738: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-14 16:49:33.575273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46413 MB memory:  -> device: 0, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:23:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37824268"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conv_block(inputs, num_filters):\n",
    "    x = tf.keras.layers.Conv1D(num_filters, 8, padding=\"same\")(inputs)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.Conv1D(num_filters, 8, padding=\"same\")(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def encoder_block(inputs, num_filters):\n",
    "    x = conv_block(inputs, num_filters)\n",
    "    p = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(inputs, concat_tensors, num_filters):\n",
    "    x = tf.keras.layers.Conv1DTranspose(num_filters, 4, strides=2, padding=\"same\")(inputs)\n",
    "    i = len(concat_tensors)-1\n",
    "    for concat_tensor in concat_tensors:\n",
    "        concat_tensor_max = tf.keras.layers.MaxPool1D(pool_size=2**i)(concat_tensor)\n",
    "        x = tf.keras.layers.Concatenate()([x, concat_tensor_max])\n",
    "        i -= 1\n",
    "        \n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def get_model(input_shape, num_blocks=4):\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    orig_n_channels = x.shape[-1]\n",
    "    \n",
    "    x_t = []\n",
    "    n_split_fact = 6\n",
    "    \n",
    "    \n",
    "    for i in range(x.shape[-1]):\n",
    "        x_sp = tf.reshape(x[:, :, i], (-1, x.shape[1]//n_split_fact, n_split_fact))\n",
    "        \n",
    "        if i < orig_n_channels:\n",
    "        \n",
    "            # Calculating mean, max, and standard deviation\n",
    "            mean = tf.reduce_mean(x_sp, axis=-1, keepdims=True)\n",
    "            max_val = tf.reduce_max(x_sp, axis=-1, keepdims=True)\n",
    "            std_dev = tf.math.reduce_std(x_sp, axis=-1, keepdims=True)\n",
    "            min_val = tf.reduce_min(x_sp, axis=-1, keepdims=True)\n",
    "\n",
    "            x_sp = tf.keras.layers.Concatenate()([x_sp, mean, max_val, std_dev, min_val])\n",
    "        \n",
    "        x_sp = tf.keras.layers.Dense(n_split_fact*16, activation='relu')(x_sp)\n",
    "        \n",
    "        x_t.append(x_sp)\n",
    "        \n",
    "    \n",
    "    x_c1d = tf.keras.layers.Conv1D(64, kernel_size=12, strides=n_split_fact, padding=\"same\")(x)\n",
    "    x_c1d = tf.keras.layers.ReLU()(x_c1d)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()(x_t + [x_c1d])# tf.reshape(x, (-1, -1, 3))\n",
    "    \n",
    "    # Initial filter size\n",
    "    fsz = 64\n",
    "    \n",
    "    # Lists to hold the encoder and pooling outputs\n",
    "    encoder_outputs = []\n",
    "    pooling_outputs = []\n",
    "    \n",
    "    # Encoder\n",
    "    for i in range(num_blocks):\n",
    "        if i == 0:\n",
    "            # First block receives the model input\n",
    "            enc_out, pool_out = encoder_block(x, fsz * (2 ** i))\n",
    "        else:\n",
    "            # Subsequent blocks receive the pooling output of the previous block\n",
    "            enc_out, pool_out = encoder_block(pooling_outputs[-1], fsz * (2 ** i))\n",
    "        \n",
    "        if i == 0:\n",
    "            pool_out_copy = pool_out\n",
    "            encoder_out_copy = enc_out\n",
    "            \n",
    "        else:\n",
    "            pool_out_mp = tf.keras.layers.MaxPooling1D(pool_size=2**i)(pool_out_copy)\n",
    "            encoder_out_mp = tf.keras.layers.MaxPooling1D(pool_size=2**i)(encoder_out_copy)\n",
    "            \n",
    "            pool_out = tf.keras.layers.Concatenate()([pool_out, pool_out_mp])\n",
    "            enc_out = tf.keras.layers.Concatenate()([enc_out, encoder_out_mp])\n",
    "        \n",
    "        \n",
    "        encoder_outputs.append(enc_out)\n",
    "        pooling_outputs.append(pool_out)\n",
    "    \n",
    "    # Bottleneck\n",
    "    bottleneck = conv_block(pooling_outputs[-1], fsz * (2 ** num_blocks))\n",
    "    \n",
    "    \n",
    "    def gru_conv(x):\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(x.shape[-1]//4, return_sequences=True))(x)\n",
    "        for i in range(1):\n",
    "            x_conv = tf.keras.layers.Conv1D(x.shape[-1], kernel_size=(4,), padding='same', activation='relu')(x)\n",
    "            x = tf.keras.layers.Dense(x.shape[-1], activation='relu')(x_conv)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    for _ in range(1):\n",
    "        bottleneck = transformer_encoder(bottleneck, head_size=16, num_heads=8, ff_dim=256, dropout=0)\n",
    "\n",
    "    bottleneck = gru_conv(bottleneck)\n",
    "\n",
    "    decoder_input = bottleneck\n",
    "    for i in range(num_blocks - 1, -1, -1):\n",
    "        decoder_output = decoder_block(decoder_input, encoder_outputs[:i+1], fsz * (2 ** i))\n",
    "        decoder_input = decoder_output\n",
    "    \n",
    "    # Output Layer\n",
    "    x = tf.keras.layers.Conv1D(2*n_split_fact, 1, padding=\"same\", activation=\"linear\")(decoder_output)\n",
    "    x = tf.reshape(x, shape=(-1, x.shape[1]*n_split_fact, 2))\n",
    "    \n",
    "    outputs = x\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "    model.compile(optimizer=tfa.optimizers.AdamW(weight_decay=1e-4), loss=custom_mce_loss)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Usage:\n",
    "input_shape = (17280, 32)  # replace with your input shape\n",
    "num_blocks = 4  # specify the number of encoder/decoder blocks\n",
    "model = get_model(input_shape, num_blocks)\n",
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e61eaf0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 17280, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be712eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_cfg:\n",
    "    n_epochs = 14\n",
    "    batch_size = 8\n",
    "    start_lr = 6e-5\n",
    "    end_lr = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:05<00:00, 91.05it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 1-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 237 series for training and 27 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 237/237 [00:14<00:00, 16.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6401, 17280, 32), y: (6401, 17280, 2), series_ids: (6401,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 58.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (803, 17280, 32), y: (803, 17280, 2), series_ids: (803,)\n",
      "Total model parameters: 37824268\n",
      "Epoch 1/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 16:50:50.240925: I tensorflow/stream_executor/cuda/cuda_blas.cc:1633] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-12-14 16:50:50.259969: I tensorflow/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801/801 [==============================] - ETA: 0s - loss: 4.3512Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.3512 - val_loss: 3.6821 val_score: 0.764  best_val_score: 0.764  last_epoch t=75.64s, total_time_elapsed t=76.0s\n",
      "801/801 [==============================] - 76s 85ms/step - loss: 4.3512 - val_loss: 3.6821\n",
      "Epoch 2/14\n",
      "801/801 [==============================] - ETA: 0s - loss: 3.6384Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6384 - val_loss: 3.4252 val_score: 0.795  best_val_score: 0.795  last_epoch t=63.23s, total_time_elapsed t=139.0s\n",
      "801/801 [==============================] - 63s 79ms/step - loss: 3.6384 - val_loss: 3.4252\n",
      "Epoch 3/14\n",
      "800/801 [============================>.] - ETA: 0s - loss: 3.5197Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.5197 - val_loss: 3.5321 val_score: 0.802  best_val_score: 0.802  last_epoch t=62.70s, total_time_elapsed t=202.0s\n",
      "801/801 [==============================] - 63s 78ms/step - loss: 3.5197 - val_loss: 3.5321\n",
      "Epoch 4/14\n",
      "800/801 [============================>.] - ETA: 0s - loss: 3.4705Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4700 - val_loss: 3.4882 val_score: 0.795  best_val_score: 0.802  last_epoch t=61.16s, total_time_elapsed t=263.0s\n",
      "801/801 [==============================] - 61s 76ms/step - loss: 3.4700 - val_loss: 3.4882\n",
      "Epoch 5/14\n",
      "800/801 [============================>.] - ETA: 0s - loss: 3.4024Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.4023 - val_loss: 3.5790 val_score: 0.800  best_val_score: 0.802  last_epoch t=60.70s, total_time_elapsed t=323.0s\n",
      "801/801 [==============================] - 61s 76ms/step - loss: 3.4023 - val_loss: 3.5790\n",
      "Epoch 6/14\n",
      "800/801 [============================>.] - ETA: 0s - loss: 3.3574Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3580 - val_loss: 3.3685 val_score: 0.814  best_val_score: 0.814  last_epoch t=62.45s, total_time_elapsed t=386.0s\n",
      "801/801 [==============================] - 62s 78ms/step - loss: 3.3580 - val_loss: 3.3685\n",
      "Epoch 7/14\n",
      "800/801 [============================>.] - ETA: 0s - loss: 3.3217Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3216 - val_loss: 3.3193 val_score: 0.804  best_val_score: 0.814  last_epoch t=61.58s, total_time_elapsed t=447.0s\n",
      "801/801 [==============================] - 62s 77ms/step - loss: 3.3216 - val_loss: 3.3193\n",
      "Epoch 8/14\n",
      "800/801 [============================>.] - ETA: 0s - loss: 3.2932Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.2927 - val_loss: 3.3188 val_score: 0.803  best_val_score: 0.814  last_epoch t=61.08s, total_time_elapsed t=509.0s\n",
      "801/801 [==============================] - 61s 76ms/step - loss: 3.2927 - val_loss: 3.3188\n",
      "Epoch 9/14\n",
      "801/801 [==============================] - ETA: 0s - loss: 3.2465Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.2465 - val_loss: 3.3591 val_score: 0.812  best_val_score: 0.814  last_epoch t=59.58s, total_time_elapsed t=568.0s\n",
      "801/801 [==============================] - 60s 74ms/step - loss: 3.2465 - val_loss: 3.3591\n",
      "Epoch 10/14\n",
      "800/801 [============================>.] - ETA: 0s - loss: 3.2421Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2426 - val_loss: 3.3666 val_score: 0.801  best_val_score: 0.814  last_epoch t=61.78s, total_time_elapsed t=630.0s\n",
      "801/801 [==============================] - 62s 77ms/step - loss: 3.2426 - val_loss: 3.3666\n",
      "Epoch 11/14\n",
      "800/801 [============================>.] - ETA: 0s - loss: 3.2087Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2082 - val_loss: 3.4440 val_score: 0.797  best_val_score: 0.814  last_epoch t=60.31s, total_time_elapsed t=690.0s\n",
      "801/801 [==============================] - 60s 75ms/step - loss: 3.2082 - val_loss: 3.4440\n",
      "Epoch 12/14\n",
      "800/801 [============================>.] - ETA: 0s - loss: 3.2039Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.2037 - val_loss: 3.3596 val_score: 0.812  best_val_score: 0.814  last_epoch t=61.20s, total_time_elapsed t=751.0s\n",
      "801/801 [==============================] - 61s 76ms/step - loss: 3.2037 - val_loss: 3.3596\n",
      "Epoch 13/14\n",
      "800/801 [============================>.] - ETA: 0s - loss: 3.1711Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.1715 - val_loss: 3.3496 val_score: 0.810  best_val_score: 0.814  last_epoch t=61.33s, total_time_elapsed t=813.0s\n",
      "801/801 [==============================] - 61s 77ms/step - loss: 3.1715 - val_loss: 3.3496\n",
      "Epoch 14/14\n",
      "800/801 [============================>.] - ETA: 0s - loss: 3.1676Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.1671 - val_loss: 3.3574 val_score: 0.801  best_val_score: 0.814  last_epoch t=62.72s, total_time_elapsed t=876.0s\n",
      "801/801 [==============================] - 63s 78ms/step - loss: 3.1671 - val_loss: 3.3574\n",
      "Model finished with val loss: 3.35744047164917\n",
      "26/26 [==============================] - 4s 61ms/step\n",
      "Val Score: 0.8139644832948383\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:05<00:00, 97.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 2-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 237 series for training and 27 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 237/237 [00:13<00:00, 18.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6499, 17280, 32), y: (6499, 17280, 2), series_ids: (6499,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 27.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (705, 17280, 32), y: (705, 17280, 2), series_ids: (705,)\n",
      "Total model parameters: 37824268\n",
      "Epoch 1/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 4.3184Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.3184 - val_loss: 3.5623 val_score: 0.847  best_val_score: 0.847  last_epoch t=73.61s, total_time_elapsed t=74.0s\n",
      "813/813 [==============================] - 74s 82ms/step - loss: 4.3184 - val_loss: 3.5623\n",
      "Epoch 2/14\n",
      "812/813 [============================>.] - ETA: 0s - loss: 3.6516Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6526 - val_loss: 3.2892 val_score: 0.866  best_val_score: 0.866  last_epoch t=60.86s, total_time_elapsed t=134.0s\n",
      "813/813 [==============================] - 61s 75ms/step - loss: 3.6526 - val_loss: 3.2892\n",
      "Epoch 3/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.5231Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.5231 - val_loss: 3.1946 val_score: 0.879  best_val_score: 0.879  last_epoch t=61.12s, total_time_elapsed t=196.0s\n",
      "813/813 [==============================] - 61s 75ms/step - loss: 3.5231 - val_loss: 3.1946\n",
      "Epoch 4/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.4722Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4722 - val_loss: 3.1808 val_score: 0.886  best_val_score: 0.886  last_epoch t=60.15s, total_time_elapsed t=256.0s\n",
      "813/813 [==============================] - 60s 74ms/step - loss: 3.4722 - val_loss: 3.1808\n",
      "Epoch 5/14\n",
      "812/813 [============================>.] - ETA: 0s - loss: 3.4004Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.4008 - val_loss: 3.1732 val_score: 0.879  best_val_score: 0.886  last_epoch t=59.65s, total_time_elapsed t=315.0s\n",
      "813/813 [==============================] - 60s 73ms/step - loss: 3.4008 - val_loss: 3.1732\n",
      "Epoch 6/14\n",
      "812/813 [============================>.] - ETA: 0s - loss: 3.3708Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3710 - val_loss: 3.1631 val_score: 0.878  best_val_score: 0.886  last_epoch t=60.15s, total_time_elapsed t=376.0s\n",
      "813/813 [==============================] - 60s 74ms/step - loss: 3.3710 - val_loss: 3.1631\n",
      "Epoch 7/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.3430Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3430 - val_loss: 3.0807 val_score: 0.885  best_val_score: 0.886  last_epoch t=60.28s, total_time_elapsed t=436.0s\n",
      "813/813 [==============================] - 60s 74ms/step - loss: 3.3430 - val_loss: 3.0807\n",
      "Epoch 8/14\n",
      "812/813 [============================>.] - ETA: 0s - loss: 3.3088Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.3080 - val_loss: 3.1860 val_score: 0.888  best_val_score: 0.888  last_epoch t=63.36s, total_time_elapsed t=499.0s\n",
      "813/813 [==============================] - 63s 78ms/step - loss: 3.3080 - val_loss: 3.1860\n",
      "Epoch 9/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.2904Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.2904 - val_loss: 3.1797 val_score: 0.887  best_val_score: 0.888  last_epoch t=58.75s, total_time_elapsed t=558.0s\n",
      "813/813 [==============================] - 59s 72ms/step - loss: 3.2904 - val_loss: 3.1797\n",
      "Epoch 10/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.2636Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2636 - val_loss: 3.1292 val_score: 0.883  best_val_score: 0.888  last_epoch t=60.72s, total_time_elapsed t=619.0s\n",
      "813/813 [==============================] - 61s 75ms/step - loss: 3.2636 - val_loss: 3.1292\n",
      "Epoch 11/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.2340Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2340 - val_loss: 3.0320 val_score: 0.890  best_val_score: 0.890  last_epoch t=63.15s, total_time_elapsed t=682.0s\n",
      "813/813 [==============================] - 63s 78ms/step - loss: 3.2340 - val_loss: 3.0320\n",
      "Epoch 12/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.2200Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.2200 - val_loss: 3.0892 val_score: 0.889  best_val_score: 0.890  last_epoch t=59.37s, total_time_elapsed t=741.0s\n",
      "813/813 [==============================] - 59s 73ms/step - loss: 3.2200 - val_loss: 3.0892\n",
      "Epoch 13/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.2138Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.2138 - val_loss: 3.0645 val_score: 0.890  best_val_score: 0.890  last_epoch t=60.65s, total_time_elapsed t=802.0s\n",
      "813/813 [==============================] - 61s 75ms/step - loss: 3.2138 - val_loss: 3.0645\n",
      "Epoch 14/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.1753Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.1753 - val_loss: 3.0731 val_score: 0.889  best_val_score: 0.890  last_epoch t=60.35s, total_time_elapsed t=862.0s\n",
      "813/813 [==============================] - 60s 74ms/step - loss: 3.1753 - val_loss: 3.0731\n",
      "Model finished with val loss: 3.073077440261841\n",
      "23/23 [==============================] - 3s 55ms/step\n",
      "Val Score: 0.8898965718517323\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:05<00:00, 92.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 3-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 237 series for training and 27 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 237/237 [00:10<00:00, 22.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6391, 17280, 32), y: (6391, 17280, 2), series_ids: (6391,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:01<00:00, 20.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (813, 17280, 32), y: (813, 17280, 2), series_ids: (813,)\n",
      "Total model parameters: 37824268\n",
      "Epoch 1/14\n",
      "799/799 [==============================] - ETA: 0s - loss: 4.2937Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.2937 - val_loss: 3.5135 val_score: 0.803  best_val_score: 0.803  last_epoch t=77.38s, total_time_elapsed t=77.0s\n",
      "799/799 [==============================] - 77s 88ms/step - loss: 4.2937 - val_loss: 3.5135\n",
      "Epoch 2/14\n",
      "799/799 [==============================] - ETA: 0s - loss: 3.6620Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6620 - val_loss: 3.4589 val_score: 0.812  best_val_score: 0.812  last_epoch t=62.58s, total_time_elapsed t=140.0s\n",
      "799/799 [==============================] - 63s 78ms/step - loss: 3.6620 - val_loss: 3.4589\n",
      "Epoch 3/14\n",
      "799/799 [==============================] - ETA: 0s - loss: 3.5450Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.5450 - val_loss: 3.2086 val_score: 0.832  best_val_score: 0.832  last_epoch t=62.44s, total_time_elapsed t=202.0s\n",
      "799/799 [==============================] - 62s 78ms/step - loss: 3.5450 - val_loss: 3.2086\n",
      "Epoch 4/14\n",
      "799/799 [==============================] - ETA: 0s - loss: 3.4564Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4564 - val_loss: 3.3314 val_score: 0.831  best_val_score: 0.832  last_epoch t=62.42s, total_time_elapsed t=265.0s\n",
      "799/799 [==============================] - 62s 78ms/step - loss: 3.4564 - val_loss: 3.3314\n",
      "Epoch 5/14\n",
      "799/799 [==============================] - ETA: 0s - loss: 3.4366Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.4366 - val_loss: 3.2106 val_score: 0.828  best_val_score: 0.832  last_epoch t=61.47s, total_time_elapsed t=326.0s\n",
      "799/799 [==============================] - 61s 77ms/step - loss: 3.4366 - val_loss: 3.2106\n",
      "Epoch 6/14\n",
      "799/799 [==============================] - ETA: 0s - loss: 3.3702Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3702 - val_loss: 3.3599 val_score: 0.823  best_val_score: 0.832  last_epoch t=62.72s, total_time_elapsed t=389.0s\n",
      "799/799 [==============================] - 63s 79ms/step - loss: 3.3702 - val_loss: 3.3599\n",
      "Epoch 7/14\n",
      "799/799 [==============================] - ETA: 0s - loss: 3.3457Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3457 - val_loss: 3.2927 val_score: 0.824  best_val_score: 0.832  last_epoch t=62.69s, total_time_elapsed t=452.0s\n",
      "799/799 [==============================] - 63s 78ms/step - loss: 3.3457 - val_loss: 3.2927\n",
      "Epoch 8/14\n",
      "799/799 [==============================] - ETA: 0s - loss: 3.3099Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.3099 - val_loss: 3.2995 val_score: 0.821  best_val_score: 0.832  last_epoch t=61.64s, total_time_elapsed t=513.0s\n",
      "799/799 [==============================] - 62s 77ms/step - loss: 3.3099 - val_loss: 3.2995\n",
      "Epoch 9/14\n",
      "799/799 [==============================] - ETA: 0s - loss: 3.3021Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.3021 - val_loss: 3.1683 val_score: 0.837  best_val_score: 0.837  last_epoch t=64.05s, total_time_elapsed t=577.0s\n",
      "799/799 [==============================] - 64s 80ms/step - loss: 3.3021 - val_loss: 3.1683\n",
      "Epoch 10/14\n",
      "799/799 [==============================] - ETA: 0s - loss: 3.2830Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2830 - val_loss: 3.1608 val_score: 0.842  best_val_score: 0.842  last_epoch t=64.37s, total_time_elapsed t=642.0s\n",
      "799/799 [==============================] - 64s 81ms/step - loss: 3.2830 - val_loss: 3.1608\n",
      "Epoch 11/14\n",
      "799/799 [==============================] - ETA: 0s - loss: 3.2544Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2544 - val_loss: 3.1614 val_score: 0.838  best_val_score: 0.842  last_epoch t=61.20s, total_time_elapsed t=703.0s\n",
      "799/799 [==============================] - 61s 77ms/step - loss: 3.2544 - val_loss: 3.1614\n",
      "Epoch 12/14\n",
      "799/799 [==============================] - ETA: 0s - loss: 3.2475Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.2475 - val_loss: 3.1488 val_score: 0.839  best_val_score: 0.842  last_epoch t=60.81s, total_time_elapsed t=764.0s\n",
      "799/799 [==============================] - 61s 76ms/step - loss: 3.2475 - val_loss: 3.1488\n",
      "Epoch 13/14\n",
      "799/799 [==============================] - ETA: 0s - loss: 3.2222Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.2222 - val_loss: 3.1907 val_score: 0.840  best_val_score: 0.842  last_epoch t=61.41s, total_time_elapsed t=825.0s\n",
      "799/799 [==============================] - 61s 77ms/step - loss: 3.2222 - val_loss: 3.1907\n",
      "Epoch 14/14\n",
      "799/799 [==============================] - ETA: 0s - loss: 3.2079Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.2079 - val_loss: 3.1636 val_score: 0.837  best_val_score: 0.842  last_epoch t=60.69s, total_time_elapsed t=886.0s\n",
      "799/799 [==============================] - 61s 76ms/step - loss: 3.2079 - val_loss: 3.1636\n",
      "Model finished with val loss: 3.1635587215423584\n",
      "26/26 [==============================] - 4s 66ms/step\n",
      "Val Score: 0.8415920553862649\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:05<00:00, 91.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 4-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 237 series for training and 27 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 237/237 [00:10<00:00, 21.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6501, 17280, 32), y: (6501, 17280, 2), series_ids: (6501,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 29.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (703, 17280, 32), y: (703, 17280, 2), series_ids: (703,)\n",
      "Total model parameters: 37824268\n",
      "Epoch 1/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 4.2696Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.2696 - val_loss: 3.9784 val_score: 0.809  best_val_score: 0.809  last_epoch t=69.76s, total_time_elapsed t=70.0s\n",
      "813/813 [==============================] - 70s 77ms/step - loss: 4.2696 - val_loss: 3.9784\n",
      "Epoch 2/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.6031Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6031 - val_loss: 3.7848 val_score: 0.822  best_val_score: 0.822  last_epoch t=60.41s, total_time_elapsed t=130.0s\n",
      "813/813 [==============================] - 60s 74ms/step - loss: 3.6031 - val_loss: 3.7848\n",
      "Epoch 3/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.4848Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.4848 - val_loss: 3.7469 val_score: 0.820  best_val_score: 0.822  last_epoch t=57.86s, total_time_elapsed t=188.0s\n",
      "813/813 [==============================] - 58s 71ms/step - loss: 3.4848 - val_loss: 3.7469\n",
      "Epoch 4/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.4131Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4131 - val_loss: 3.6069 val_score: 0.827  best_val_score: 0.827  last_epoch t=61.67s, total_time_elapsed t=250.0s\n",
      "813/813 [==============================] - 62s 76ms/step - loss: 3.4131 - val_loss: 3.6069\n",
      "Epoch 5/14\n",
      "812/813 [============================>.] - ETA: 0s - loss: 3.3570Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.3549 - val_loss: 3.7302 val_score: 0.828  best_val_score: 0.828  last_epoch t=59.87s, total_time_elapsed t=310.0s\n",
      "813/813 [==============================] - 60s 74ms/step - loss: 3.3549 - val_loss: 3.7302\n",
      "Epoch 6/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.3166Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3166 - val_loss: 3.6074 val_score: 0.834  best_val_score: 0.834  last_epoch t=59.94s, total_time_elapsed t=370.0s\n",
      "813/813 [==============================] - 60s 74ms/step - loss: 3.3166 - val_loss: 3.6074\n",
      "Epoch 7/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.2856Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.2856 - val_loss: 3.6352 val_score: 0.836  best_val_score: 0.836  last_epoch t=57.55s, total_time_elapsed t=427.0s\n",
      "813/813 [==============================] - 58s 71ms/step - loss: 3.2856 - val_loss: 3.6352\n",
      "Epoch 8/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.2511Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.2511 - val_loss: 3.6082 val_score: 0.836  best_val_score: 0.836  last_epoch t=62.20s, total_time_elapsed t=489.0s\n",
      "813/813 [==============================] - 62s 77ms/step - loss: 3.2511 - val_loss: 3.6082\n",
      "Epoch 9/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.2368Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.2368 - val_loss: 3.6559 val_score: 0.837  best_val_score: 0.837  last_epoch t=61.49s, total_time_elapsed t=551.0s\n",
      "813/813 [==============================] - 61s 76ms/step - loss: 3.2368 - val_loss: 3.6559\n",
      "Epoch 10/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.1957Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.1957 - val_loss: 3.6859 val_score: 0.832  best_val_score: 0.837  last_epoch t=56.60s, total_time_elapsed t=607.0s\n",
      "813/813 [==============================] - 57s 70ms/step - loss: 3.1957 - val_loss: 3.6859\n",
      "Epoch 11/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.1954Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.1954 - val_loss: 3.5012 val_score: 0.847  best_val_score: 0.847  last_epoch t=62.10s, total_time_elapsed t=669.0s\n",
      "813/813 [==============================] - 62s 76ms/step - loss: 3.1954 - val_loss: 3.5012\n",
      "Epoch 12/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.1633Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.1633 - val_loss: 3.5270 val_score: 0.844  best_val_score: 0.847  last_epoch t=60.43s, total_time_elapsed t=730.0s\n",
      "813/813 [==============================] - 60s 74ms/step - loss: 3.1633 - val_loss: 3.5270\n",
      "Epoch 13/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.1544Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.1544 - val_loss: 3.5342 val_score: 0.846  best_val_score: 0.847  last_epoch t=59.45s, total_time_elapsed t=789.0s\n",
      "813/813 [==============================] - 59s 73ms/step - loss: 3.1544 - val_loss: 3.5342\n",
      "Epoch 14/14\n",
      "813/813 [==============================] - ETA: 0s - loss: 3.1332Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.1332 - val_loss: 3.5737 val_score: 0.840  best_val_score: 0.847  last_epoch t=60.17s, total_time_elapsed t=850.0s\n",
      "813/813 [==============================] - 60s 74ms/step - loss: 3.1332 - val_loss: 3.5737\n",
      "Model finished with val loss: 3.5736544132232666\n",
      "22/22 [==============================] - 6s 71ms/step\n",
      "Val Score: 0.8467969883275508\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:05<00:00, 92.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 5-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 238 series for training and 26 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:10<00:00, 22.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6570, 17280, 32), y: (6570, 17280, 2), series_ids: (6570,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 27.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (634, 17280, 32), y: (634, 17280, 2), series_ids: (634,)\n",
      "Total model parameters: 37824268\n",
      "Epoch 1/14\n",
      "822/822 [==============================] - ETA: 0s - loss: 4.3386Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.3386 - val_loss: 3.5607 val_score: 0.764  best_val_score: 0.764  last_epoch t=72.85s, total_time_elapsed t=73.0s\n",
      "822/822 [==============================] - 73s 80ms/step - loss: 4.3386 - val_loss: 3.5607\n",
      "Epoch 2/14\n",
      "821/822 [============================>.] - ETA: 0s - loss: 3.6337Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6334 - val_loss: 3.4539 val_score: 0.787  best_val_score: 0.787  last_epoch t=59.75s, total_time_elapsed t=133.0s\n",
      "822/822 [==============================] - 60s 73ms/step - loss: 3.6334 - val_loss: 3.4539\n",
      "Epoch 3/14\n",
      "821/822 [============================>.] - ETA: 0s - loss: 3.5210Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.5210 - val_loss: 3.3886 val_score: 0.787  best_val_score: 0.787  last_epoch t=57.58s, total_time_elapsed t=190.0s\n",
      "822/822 [==============================] - 58s 70ms/step - loss: 3.5210 - val_loss: 3.3886\n",
      "Epoch 4/14\n",
      "821/822 [============================>.] - ETA: 0s - loss: 3.4660Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4663 - val_loss: 3.3673 val_score: 0.783  best_val_score: 0.787  last_epoch t=57.85s, total_time_elapsed t=248.0s\n",
      "822/822 [==============================] - 58s 70ms/step - loss: 3.4663 - val_loss: 3.3673\n",
      "Epoch 5/14\n",
      "821/822 [============================>.] - ETA: 0s - loss: 3.3914Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.3918 - val_loss: 3.3757 val_score: 0.791  best_val_score: 0.791  last_epoch t=58.73s, total_time_elapsed t=307.0s\n",
      "822/822 [==============================] - 59s 71ms/step - loss: 3.3918 - val_loss: 3.3757\n",
      "Epoch 6/14\n",
      "822/822 [==============================] - ETA: 0s - loss: 3.3454Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3454 - val_loss: 3.4547 val_score: 0.778  best_val_score: 0.791  last_epoch t=56.73s, total_time_elapsed t=364.0s\n",
      "822/822 [==============================] - 57s 69ms/step - loss: 3.3454 - val_loss: 3.4547\n",
      "Epoch 7/14\n",
      "822/822 [==============================] - ETA: 0s - loss: 3.3123Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3123 - val_loss: 3.2843 val_score: 0.800  best_val_score: 0.800  last_epoch t=59.55s, total_time_elapsed t=423.0s\n",
      "822/822 [==============================] - 60s 72ms/step - loss: 3.3123 - val_loss: 3.2843\n",
      "Epoch 8/14\n",
      "821/822 [============================>.] - ETA: 0s - loss: 3.2922Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.2919 - val_loss: 3.3263 val_score: 0.802  best_val_score: 0.802  last_epoch t=59.73s, total_time_elapsed t=483.0s\n",
      "822/822 [==============================] - 60s 73ms/step - loss: 3.2919 - val_loss: 3.3263\n",
      "Epoch 9/14\n",
      "821/822 [============================>.] - ETA: 0s - loss: 3.2680Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.2689 - val_loss: 3.2703 val_score: 0.804  best_val_score: 0.804  last_epoch t=59.14s, total_time_elapsed t=542.0s\n",
      "822/822 [==============================] - 59s 72ms/step - loss: 3.2689 - val_loss: 3.2703\n",
      "Epoch 10/14\n",
      "821/822 [============================>.] - ETA: 0s - loss: 3.2485Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2492 - val_loss: 3.2632 val_score: 0.812  best_val_score: 0.812  last_epoch t=58.92s, total_time_elapsed t=601.0s\n",
      "822/822 [==============================] - 59s 72ms/step - loss: 3.2492 - val_loss: 3.2632\n",
      "Epoch 11/14\n",
      "821/822 [============================>.] - ETA: 0s - loss: 3.2237Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2233 - val_loss: 3.2871 val_score: 0.805  best_val_score: 0.812  last_epoch t=56.70s, total_time_elapsed t=658.0s\n",
      "822/822 [==============================] - 57s 69ms/step - loss: 3.2233 - val_loss: 3.2871\n",
      "Epoch 12/14\n",
      "822/822 [==============================] - ETA: 0s - loss: 3.1995Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.1995 - val_loss: 3.2435 val_score: 0.804  best_val_score: 0.812  last_epoch t=57.28s, total_time_elapsed t=715.0s\n",
      "822/822 [==============================] - 57s 70ms/step - loss: 3.1995 - val_loss: 3.2435\n",
      "Epoch 13/14\n",
      "822/822 [==============================] - ETA: 0s - loss: 3.1819Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.1819 - val_loss: 3.2431 val_score: 0.809  best_val_score: 0.812  last_epoch t=57.57s, total_time_elapsed t=772.0s\n",
      "822/822 [==============================] - 58s 70ms/step - loss: 3.1819 - val_loss: 3.2431\n",
      "Epoch 14/14\n",
      "821/822 [============================>.] - ETA: 0s - loss: 3.1655Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.1659 - val_loss: 3.2836 val_score: 0.808  best_val_score: 0.812  last_epoch t=57.70s, total_time_elapsed t=830.0s\n",
      "822/822 [==============================] - 58s 70ms/step - loss: 3.1659 - val_loss: 3.2836\n",
      "Model finished with val loss: 3.283602476119995\n",
      "20/20 [==============================] - 3s 75ms/step\n",
      "Val Score: 0.8121230817456917\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:04<00:00, 118.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 6-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 238 series for training and 26 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:09<00:00, 25.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6515, 17280, 32), y: (6515, 17280, 2), series_ids: (6515,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 31.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (689, 17280, 32), y: (689, 17280, 2), series_ids: (689,)\n",
      "Total model parameters: 37824268\n",
      "Epoch 1/14\n",
      "814/815 [============================>.] - ETA: 0s - loss: 4.1622Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.1616 - val_loss: 4.5121 val_score: 0.754  best_val_score: 0.754  last_epoch t=70.19s, total_time_elapsed t=70.0s\n",
      "815/815 [==============================] - 70s 78ms/step - loss: 4.1616 - val_loss: 4.5121\n",
      "Epoch 2/14\n",
      "814/815 [============================>.] - ETA: 0s - loss: 3.5268Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.5267 - val_loss: 4.3936 val_score: 0.776  best_val_score: 0.776  last_epoch t=61.14s, total_time_elapsed t=131.0s\n",
      "815/815 [==============================] - 61s 75ms/step - loss: 3.5267 - val_loss: 4.3936\n",
      "Epoch 3/14\n",
      "814/815 [============================>.] - ETA: 0s - loss: 3.4187Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.4209 - val_loss: 4.2706 val_score: 0.781  best_val_score: 0.781  last_epoch t=61.07s, total_time_elapsed t=192.0s\n",
      "815/815 [==============================] - 61s 75ms/step - loss: 3.4209 - val_loss: 4.2706\n",
      "Epoch 4/14\n",
      "814/815 [============================>.] - ETA: 0s - loss: 3.3655Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.3661 - val_loss: 4.3155 val_score: 0.780  best_val_score: 0.781  last_epoch t=58.30s, total_time_elapsed t=251.0s\n",
      "815/815 [==============================] - 58s 72ms/step - loss: 3.3661 - val_loss: 4.3155\n",
      "Epoch 5/14\n",
      "815/815 [==============================] - ETA: 0s - loss: 3.3289Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.3289 - val_loss: 4.2491 val_score: 0.790  best_val_score: 0.790  last_epoch t=61.05s, total_time_elapsed t=312.0s\n",
      "815/815 [==============================] - 61s 75ms/step - loss: 3.3289 - val_loss: 4.2491\n",
      "Epoch 6/14\n",
      "815/815 [==============================] - ETA: 0s - loss: 3.2613Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.2613 - val_loss: 4.3148 val_score: 0.776  best_val_score: 0.790  last_epoch t=58.75s, total_time_elapsed t=371.0s\n",
      "815/815 [==============================] - 59s 72ms/step - loss: 3.2613 - val_loss: 4.3148\n",
      "Epoch 7/14\n",
      "814/815 [============================>.] - ETA: 0s - loss: 3.2301Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.2294 - val_loss: 4.3328 val_score: 0.771  best_val_score: 0.790  last_epoch t=58.89s, total_time_elapsed t=429.0s\n",
      "815/815 [==============================] - 59s 72ms/step - loss: 3.2294 - val_loss: 4.3328\n",
      "Epoch 8/14\n",
      "815/815 [==============================] - ETA: 0s - loss: 3.2004Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.2004 - val_loss: 4.2236 val_score: 0.788  best_val_score: 0.790  last_epoch t=60.31s, total_time_elapsed t=490.0s\n",
      "815/815 [==============================] - 60s 74ms/step - loss: 3.2004 - val_loss: 4.2236\n",
      "Epoch 9/14\n",
      "815/815 [==============================] - ETA: 0s - loss: 3.1754Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.1754 - val_loss: 4.2542 val_score: 0.792  best_val_score: 0.792  last_epoch t=63.04s, total_time_elapsed t=553.0s\n",
      "815/815 [==============================] - 63s 77ms/step - loss: 3.1754 - val_loss: 4.2542\n",
      "Epoch 10/14\n",
      "815/815 [==============================] - ETA: 0s - loss: 3.1498Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.1498 - val_loss: 4.3801 val_score: 0.779  best_val_score: 0.792  last_epoch t=58.53s, total_time_elapsed t=611.0s\n",
      "815/815 [==============================] - 59s 72ms/step - loss: 3.1498 - val_loss: 4.3801\n",
      "Epoch 11/14\n",
      "815/815 [==============================] - ETA: 0s - loss: 3.1389Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.1389 - val_loss: 4.1686 val_score: 0.787  best_val_score: 0.792  last_epoch t=58.89s, total_time_elapsed t=670.0s\n",
      "815/815 [==============================] - 59s 72ms/step - loss: 3.1389 - val_loss: 4.1686\n",
      "Epoch 12/14\n",
      "815/815 [==============================] - ETA: 0s - loss: 3.1218Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.1218 - val_loss: 4.2458 val_score: 0.784  best_val_score: 0.792  last_epoch t=59.98s, total_time_elapsed t=730.0s\n",
      "815/815 [==============================] - 60s 74ms/step - loss: 3.1218 - val_loss: 4.2458\n",
      "Epoch 13/14\n",
      "815/815 [==============================] - ETA: 0s - loss: 3.1007Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.1007 - val_loss: 4.2992 val_score: 0.795  best_val_score: 0.795  last_epoch t=61.04s, total_time_elapsed t=791.0s\n",
      "815/815 [==============================] - 61s 75ms/step - loss: 3.1007 - val_loss: 4.2992\n",
      "Epoch 14/14\n",
      "814/815 [============================>.] - ETA: 0s - loss: 3.0862Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.0854 - val_loss: 4.2684 val_score: 0.782  best_val_score: 0.795  last_epoch t=58.61s, total_time_elapsed t=850.0s\n",
      "815/815 [==============================] - 59s 72ms/step - loss: 3.0854 - val_loss: 4.2684\n",
      "Model finished with val loss: 4.268378257751465\n",
      "22/22 [==============================] - 3s 62ms/step\n",
      "Val Score: 0.7951476258079793\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:04<00:00, 129.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 7-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 238 series for training and 26 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:11<00:00, 20.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6522, 17280, 32), y: (6522, 17280, 2), series_ids: (6522,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 30.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (682, 17280, 32), y: (682, 17280, 2), series_ids: (682,)\n",
      "Total model parameters: 37824268\n",
      "Epoch 1/14\n",
      "816/816 [==============================] - ETA: 0s - loss: 4.2397Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.2397 - val_loss: 4.2960 val_score: 0.670  best_val_score: 0.670  last_epoch t=73.37s, total_time_elapsed t=73.0s\n",
      "816/816 [==============================] - 73s 81ms/step - loss: 4.2397 - val_loss: 4.2960\n",
      "Epoch 2/14\n",
      "815/816 [============================>.] - ETA: 0s - loss: 3.5559Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.5570 - val_loss: 4.2260 val_score: 0.676  best_val_score: 0.676  last_epoch t=61.14s, total_time_elapsed t=135.0s\n",
      "816/816 [==============================] - 61s 75ms/step - loss: 3.5570 - val_loss: 4.2260\n",
      "Epoch 3/14\n",
      "815/816 [============================>.] - ETA: 0s - loss: 3.4618Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.4619 - val_loss: 4.1452 val_score: 0.693  best_val_score: 0.693  last_epoch t=60.51s, total_time_elapsed t=195.0s\n",
      "816/816 [==============================] - 61s 74ms/step - loss: 3.4619 - val_loss: 4.1452\n",
      "Epoch 4/14\n",
      "816/816 [==============================] - ETA: 0s - loss: 3.3923Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.3923 - val_loss: 4.1057 val_score: 0.704  best_val_score: 0.704  last_epoch t=60.40s, total_time_elapsed t=255.0s\n",
      "816/816 [==============================] - 60s 74ms/step - loss: 3.3923 - val_loss: 4.1057\n",
      "Epoch 5/14\n",
      "815/816 [============================>.] - ETA: 0s - loss: 3.3504Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.3498 - val_loss: 4.1031 val_score: 0.697  best_val_score: 0.704  last_epoch t=58.32s, total_time_elapsed t=314.0s\n",
      "816/816 [==============================] - 58s 71ms/step - loss: 3.3498 - val_loss: 4.1031\n",
      "Epoch 6/14\n",
      "816/816 [==============================] - ETA: 0s - loss: 3.2919Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.2919 - val_loss: 3.9926 val_score: 0.714  best_val_score: 0.714  last_epoch t=62.07s, total_time_elapsed t=376.0s\n",
      "816/816 [==============================] - 62s 76ms/step - loss: 3.2919 - val_loss: 3.9926\n",
      "Epoch 7/14\n",
      "816/816 [==============================] - ETA: 0s - loss: 3.2630Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.2630 - val_loss: 4.0238 val_score: 0.697  best_val_score: 0.714  last_epoch t=58.97s, total_time_elapsed t=435.0s\n",
      "816/816 [==============================] - 59s 72ms/step - loss: 3.2630 - val_loss: 4.0238\n",
      "Epoch 8/14\n",
      "816/816 [==============================] - ETA: 0s - loss: 3.2391Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.2391 - val_loss: 4.0813 val_score: 0.692  best_val_score: 0.714  last_epoch t=59.15s, total_time_elapsed t=494.0s\n",
      "816/816 [==============================] - 59s 73ms/step - loss: 3.2391 - val_loss: 4.0813\n",
      "Epoch 9/14\n",
      "816/816 [==============================] - ETA: 0s - loss: 3.2222Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.2222 - val_loss: 4.2549 val_score: 0.676  best_val_score: 0.714  last_epoch t=59.40s, total_time_elapsed t=553.0s\n",
      "816/816 [==============================] - 59s 73ms/step - loss: 3.2222 - val_loss: 4.2549\n",
      "Epoch 10/14\n",
      "815/816 [============================>.] - ETA: 0s - loss: 3.2301Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2300 - val_loss: 4.0654 val_score: 0.711  best_val_score: 0.714  last_epoch t=60.75s, total_time_elapsed t=614.0s\n",
      "816/816 [==============================] - 61s 74ms/step - loss: 3.2300 - val_loss: 4.0654\n",
      "Epoch 11/14\n",
      "816/816 [==============================] - ETA: 0s - loss: 3.1693Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.1693 - val_loss: 3.9830 val_score: 0.708  best_val_score: 0.714  last_epoch t=60.14s, total_time_elapsed t=674.0s\n",
      "816/816 [==============================] - 60s 74ms/step - loss: 3.1693 - val_loss: 3.9830\n",
      "Epoch 12/14\n",
      "816/816 [==============================] - ETA: 0s - loss: 3.1531Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.1531 - val_loss: 3.9132 val_score: 0.708  best_val_score: 0.714  last_epoch t=60.86s, total_time_elapsed t=735.0s\n",
      "816/816 [==============================] - 61s 75ms/step - loss: 3.1531 - val_loss: 3.9132\n",
      "Epoch 13/14\n",
      "815/816 [============================>.] - ETA: 0s - loss: 3.1470Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.1469 - val_loss: 3.9249 val_score: 0.719  best_val_score: 0.719  last_epoch t=62.94s, total_time_elapsed t=798.0s\n",
      "816/816 [==============================] - 63s 77ms/step - loss: 3.1469 - val_loss: 3.9249\n",
      "Epoch 14/14\n",
      "815/816 [============================>.] - ETA: 0s - loss: 3.1473Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.1477 - val_loss: 3.9813 val_score: 0.708  best_val_score: 0.719  last_epoch t=57.95s, total_time_elapsed t=856.0s\n",
      "816/816 [==============================] - 58s 71ms/step - loss: 3.1477 - val_loss: 3.9813\n",
      "Model finished with val loss: 3.981318950653076\n",
      "22/22 [==============================] - 3s 69ms/step\n",
      "Val Score: 0.718425312522752\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:05<00:00, 95.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 8-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 238 series for training and 26 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:10<00:00, 22.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6538, 17280, 32), y: (6538, 17280, 2), series_ids: (6538,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 36.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (666, 17280, 32), y: (666, 17280, 2), series_ids: (666,)\n",
      "Total model parameters: 37824268\n",
      "Epoch 1/14\n",
      "817/818 [============================>.] - ETA: 0s - loss: 4.3560Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.3551 - val_loss: 3.8585 val_score: 0.785  best_val_score: 0.785  last_epoch t=73.56s, total_time_elapsed t=74.0s\n",
      "818/818 [==============================] - 74s 81ms/step - loss: 4.3551 - val_loss: 3.8585\n",
      "Epoch 2/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.6325Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6325 - val_loss: 3.6181 val_score: 0.817  best_val_score: 0.817  last_epoch t=59.66s, total_time_elapsed t=133.0s\n",
      "818/818 [==============================] - 60s 73ms/step - loss: 3.6325 - val_loss: 3.6181\n",
      "Epoch 3/14\n",
      "817/818 [============================>.] - ETA: 0s - loss: 3.5171Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.5164 - val_loss: 3.3033 val_score: 0.822  best_val_score: 0.822  last_epoch t=59.91s, total_time_elapsed t=193.0s\n",
      "818/818 [==============================] - 60s 73ms/step - loss: 3.5164 - val_loss: 3.3033\n",
      "Epoch 4/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.4699Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4699 - val_loss: 3.3231 val_score: 0.831  best_val_score: 0.831  last_epoch t=61.03s, total_time_elapsed t=254.0s\n",
      "818/818 [==============================] - 61s 75ms/step - loss: 3.4699 - val_loss: 3.3231\n",
      "Epoch 5/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.3954Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.3954 - val_loss: 3.3190 val_score: 0.817  best_val_score: 0.831  last_epoch t=57.94s, total_time_elapsed t=312.0s\n",
      "818/818 [==============================] - 58s 71ms/step - loss: 3.3954 - val_loss: 3.3190\n",
      "Epoch 6/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.3635Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3635 - val_loss: 3.2523 val_score: 0.842  best_val_score: 0.842  last_epoch t=64.24s, total_time_elapsed t=376.0s\n",
      "818/818 [==============================] - 64s 79ms/step - loss: 3.3635 - val_loss: 3.2523\n",
      "Epoch 7/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.3272Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3272 - val_loss: 3.2181 val_score: 0.833  best_val_score: 0.842  last_epoch t=60.02s, total_time_elapsed t=436.0s\n",
      "818/818 [==============================] - 60s 73ms/step - loss: 3.3272 - val_loss: 3.2181\n",
      "Epoch 8/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.2883Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.2883 - val_loss: 3.1822 val_score: 0.839  best_val_score: 0.842  last_epoch t=60.02s, total_time_elapsed t=496.0s\n",
      "818/818 [==============================] - 60s 73ms/step - loss: 3.2883 - val_loss: 3.1822\n",
      "Epoch 9/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.2691Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.2691 - val_loss: 3.1594 val_score: 0.850  best_val_score: 0.850  last_epoch t=61.64s, total_time_elapsed t=558.0s\n",
      "818/818 [==============================] - 62s 75ms/step - loss: 3.2691 - val_loss: 3.1594\n",
      "Epoch 10/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.2453Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2453 - val_loss: 3.1526 val_score: 0.847  best_val_score: 0.850  last_epoch t=58.99s, total_time_elapsed t=617.0s\n",
      "818/818 [==============================] - 59s 72ms/step - loss: 3.2453 - val_loss: 3.1526\n",
      "Epoch 11/14\n",
      "817/818 [============================>.] - ETA: 0s - loss: 3.2287Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2288 - val_loss: 3.2208 val_score: 0.847  best_val_score: 0.850  last_epoch t=58.53s, total_time_elapsed t=676.0s\n",
      "818/818 [==============================] - 59s 72ms/step - loss: 3.2288 - val_loss: 3.2208\n",
      "Epoch 12/14\n",
      "817/818 [============================>.] - ETA: 0s - loss: 3.2022Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.2017 - val_loss: 3.1890 val_score: 0.848  best_val_score: 0.850  last_epoch t=59.15s, total_time_elapsed t=735.0s\n",
      "818/818 [==============================] - 59s 72ms/step - loss: 3.2017 - val_loss: 3.1890\n",
      "Epoch 13/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.1912Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.1912 - val_loss: 3.1356 val_score: 0.856  best_val_score: 0.856  last_epoch t=62.40s, total_time_elapsed t=797.0s\n",
      "818/818 [==============================] - 62s 76ms/step - loss: 3.1912 - val_loss: 3.1356\n",
      "Epoch 14/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.1779Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.1779 - val_loss: 3.1376 val_score: 0.856  best_val_score: 0.856  last_epoch t=60.92s, total_time_elapsed t=858.0s\n",
      "818/818 [==============================] - 61s 74ms/step - loss: 3.1779 - val_loss: 3.1376\n",
      "Model finished with val loss: 3.137620210647583\n",
      "21/21 [==============================] - 3s 61ms/step\n",
      "Val Score: 0.8558363409815695\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:05<00:00, 104.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 9-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 238 series for training and 26 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:11<00:00, 21.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6415, 17280, 32), y: (6415, 17280, 2), series_ids: (6415,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 26.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (789, 17280, 32), y: (789, 17280, 2), series_ids: (789,)\n",
      "Total model parameters: 37824268\n",
      "Epoch 1/14\n",
      "802/802 [==============================] - ETA: 0s - loss: 4.3408Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.3408 - val_loss: 2.6698 val_score: 0.806  best_val_score: 0.806  last_epoch t=79.05s, total_time_elapsed t=79.0s\n",
      "802/802 [==============================] - 79s 89ms/step - loss: 4.3408 - val_loss: 2.6698\n",
      "Epoch 2/14\n",
      "802/802 [==============================] - ETA: 0s - loss: 3.7636Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.7636 - val_loss: 2.6627 val_score: 0.806  best_val_score: 0.806  last_epoch t=63.96s, total_time_elapsed t=143.0s\n",
      "802/802 [==============================] - 64s 80ms/step - loss: 3.7636 - val_loss: 2.6627\n",
      "Epoch 3/14\n",
      "802/802 [==============================] - ETA: 0s - loss: 3.6388Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.6388 - val_loss: 2.5118 val_score: 0.820  best_val_score: 0.820  last_epoch t=62.29s, total_time_elapsed t=205.0s\n",
      "802/802 [==============================] - 62s 78ms/step - loss: 3.6388 - val_loss: 2.5118\n",
      "Epoch 4/14\n",
      "802/802 [==============================] - ETA: 0s - loss: 3.5719Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.5719 - val_loss: 2.5123 val_score: 0.819  best_val_score: 0.820  last_epoch t=60.60s, total_time_elapsed t=266.0s\n",
      "802/802 [==============================] - 61s 76ms/step - loss: 3.5719 - val_loss: 2.5123\n",
      "Epoch 5/14\n",
      "802/802 [==============================] - ETA: 0s - loss: 3.5416Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.5416 - val_loss: 2.4969 val_score: 0.817  best_val_score: 0.820  last_epoch t=59.99s, total_time_elapsed t=326.0s\n",
      "802/802 [==============================] - 60s 75ms/step - loss: 3.5416 - val_loss: 2.4969\n",
      "Epoch 6/14\n",
      "802/802 [==============================] - ETA: 0s - loss: 3.4754Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.4754 - val_loss: 2.4766 val_score: 0.827  best_val_score: 0.827  last_epoch t=62.95s, total_time_elapsed t=389.0s\n",
      "802/802 [==============================] - 63s 79ms/step - loss: 3.4754 - val_loss: 2.4766\n",
      "Epoch 7/14\n",
      "802/802 [==============================] - ETA: 0s - loss: 3.4381Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.4381 - val_loss: 2.5346 val_score: 0.819  best_val_score: 0.827  last_epoch t=62.45s, total_time_elapsed t=451.0s\n",
      "802/802 [==============================] - 62s 78ms/step - loss: 3.4381 - val_loss: 2.5346\n",
      "Epoch 8/14\n",
      "802/802 [==============================] - ETA: 0s - loss: 3.4099Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.4099 - val_loss: 2.4446 val_score: 0.837  best_val_score: 0.837  last_epoch t=64.21s, total_time_elapsed t=516.0s\n",
      "802/802 [==============================] - 64s 80ms/step - loss: 3.4099 - val_loss: 2.4446\n",
      "Epoch 9/14\n",
      "802/802 [==============================] - ETA: 0s - loss: 3.3822Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.3822 - val_loss: 2.5026 val_score: 0.820  best_val_score: 0.837  last_epoch t=60.83s, total_time_elapsed t=576.0s\n",
      "802/802 [==============================] - 61s 76ms/step - loss: 3.3822 - val_loss: 2.5026\n",
      "Epoch 10/14\n",
      "802/802 [==============================] - ETA: 0s - loss: 3.3663Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.3663 - val_loss: 2.5906 val_score: 0.815  best_val_score: 0.837  last_epoch t=61.14s, total_time_elapsed t=637.0s\n",
      "802/802 [==============================] - 61s 76ms/step - loss: 3.3663 - val_loss: 2.5906\n",
      "Epoch 11/14\n",
      "802/802 [==============================] - ETA: 0s - loss: 3.3402Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.3402 - val_loss: 2.4092 val_score: 0.845  best_val_score: 0.845  last_epoch t=62.60s, total_time_elapsed t=700.0s\n",
      "802/802 [==============================] - 63s 78ms/step - loss: 3.3402 - val_loss: 2.4092\n",
      "Epoch 12/14\n",
      "802/802 [==============================] - ETA: 0s - loss: 3.3253Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.3253 - val_loss: 2.5141 val_score: 0.828  best_val_score: 0.845  last_epoch t=60.97s, total_time_elapsed t=761.0s\n",
      "802/802 [==============================] - 61s 76ms/step - loss: 3.3253 - val_loss: 2.5141\n",
      "Epoch 13/14\n",
      "802/802 [==============================] - ETA: 0s - loss: 3.3121Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.3121 - val_loss: 2.5386 val_score: 0.829  best_val_score: 0.845  last_epoch t=59.90s, total_time_elapsed t=821.0s\n",
      "802/802 [==============================] - 60s 75ms/step - loss: 3.3121 - val_loss: 2.5386\n",
      "Epoch 14/14\n",
      "802/802 [==============================] - ETA: 0s - loss: 3.3118Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.3118 - val_loss: 2.5817 val_score: 0.832  best_val_score: 0.845  last_epoch t=59.41s, total_time_elapsed t=880.0s\n",
      "802/802 [==============================] - 59s 74ms/step - loss: 3.3118 - val_loss: 2.5817\n",
      "Model finished with val loss: 2.5816714763641357\n",
      "25/25 [==============================] - 4s 66ms/step\n",
      "Val Score: 0.8450064758790179\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:05<00:00, 98.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 10-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 238 series for training and 26 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:10<00:00, 23.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6484, 17280, 32), y: (6484, 17280, 2), series_ids: (6484,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 32.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (720, 17280, 32), y: (720, 17280, 2), series_ids: (720,)\n",
      "Total model parameters: 37824268\n",
      "Epoch 1/14\n",
      "811/811 [==============================] - ETA: 0s - loss: 4.2096Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.2096 - val_loss: 3.7806 val_score: 0.756  best_val_score: 0.756  last_epoch t=73.56s, total_time_elapsed t=74.0s\n",
      "811/811 [==============================] - 74s 81ms/step - loss: 4.2096 - val_loss: 3.7806\n",
      "Epoch 2/14\n",
      "810/811 [============================>.] - ETA: 0s - loss: 3.5933Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.5930 - val_loss: 3.6341 val_score: 0.770  best_val_score: 0.770  last_epoch t=61.55s, total_time_elapsed t=135.0s\n",
      "811/811 [==============================] - 62s 76ms/step - loss: 3.5930 - val_loss: 3.6341\n",
      "Epoch 3/14\n",
      "811/811 [==============================] - ETA: 0s - loss: 3.4918Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.4918 - val_loss: 3.7586 val_score: 0.771  best_val_score: 0.771  last_epoch t=61.19s, total_time_elapsed t=196.0s\n",
      "811/811 [==============================] - 61s 75ms/step - loss: 3.4918 - val_loss: 3.7586\n",
      "Epoch 4/14\n",
      "811/811 [==============================] - ETA: 0s - loss: 3.4120Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4120 - val_loss: 3.6237 val_score: 0.770  best_val_score: 0.771  last_epoch t=57.71s, total_time_elapsed t=254.0s\n",
      "811/811 [==============================] - 58s 71ms/step - loss: 3.4120 - val_loss: 3.6237\n",
      "Epoch 5/14\n",
      "811/811 [==============================] - ETA: 0s - loss: 3.3647Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.3647 - val_loss: 3.5826 val_score: 0.769  best_val_score: 0.771  last_epoch t=59.63s, total_time_elapsed t=314.0s\n",
      "811/811 [==============================] - 60s 74ms/step - loss: 3.3647 - val_loss: 3.5826\n",
      "Epoch 6/14\n",
      "811/811 [==============================] - ETA: 0s - loss: 3.3076Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3076 - val_loss: 3.6120 val_score: 0.774  best_val_score: 0.774  last_epoch t=61.06s, total_time_elapsed t=375.0s\n",
      "811/811 [==============================] - 61s 75ms/step - loss: 3.3076 - val_loss: 3.6120\n",
      "Epoch 7/14\n",
      "811/811 [==============================] - ETA: 0s - loss: 3.2859Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.2859 - val_loss: 3.6144 val_score: 0.778  best_val_score: 0.778  last_epoch t=61.24s, total_time_elapsed t=436.0s\n",
      "811/811 [==============================] - 61s 76ms/step - loss: 3.2859 - val_loss: 3.6144\n",
      "Epoch 8/14\n",
      "811/811 [==============================] - ETA: 0s - loss: 3.2510Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.2510 - val_loss: 3.6080 val_score: 0.789  best_val_score: 0.789  last_epoch t=60.95s, total_time_elapsed t=497.0s\n",
      "811/811 [==============================] - 61s 75ms/step - loss: 3.2510 - val_loss: 3.6080\n",
      "Epoch 9/14\n",
      "811/811 [==============================] - ETA: 0s - loss: 3.2394Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.2394 - val_loss: 3.8058 val_score: 0.775  best_val_score: 0.789  last_epoch t=58.66s, total_time_elapsed t=556.0s\n",
      "811/811 [==============================] - 59s 72ms/step - loss: 3.2394 - val_loss: 3.8058\n",
      "Epoch 10/14\n",
      "811/811 [==============================] - ETA: 0s - loss: 3.1934Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.1934 - val_loss: 3.6713 val_score: 0.781  best_val_score: 0.789  last_epoch t=58.21s, total_time_elapsed t=614.0s\n",
      "811/811 [==============================] - 58s 72ms/step - loss: 3.1934 - val_loss: 3.6713\n",
      "Epoch 11/14\n",
      "811/811 [==============================] - ETA: 0s - loss: 3.1804Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.1804 - val_loss: 3.5228 val_score: 0.793  best_val_score: 0.793  last_epoch t=61.50s, total_time_elapsed t=675.0s\n",
      "811/811 [==============================] - 62s 76ms/step - loss: 3.1804 - val_loss: 3.5228\n",
      "Epoch 12/14\n",
      "811/811 [==============================] - ETA: 0s - loss: 3.1638Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.1638 - val_loss: 3.6336 val_score: 0.790  best_val_score: 0.793  last_epoch t=58.78s, total_time_elapsed t=734.0s\n",
      "811/811 [==============================] - 59s 72ms/step - loss: 3.1638 - val_loss: 3.6336\n",
      "Epoch 13/14\n",
      "811/811 [==============================] - ETA: 0s - loss: 3.1408Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.1408 - val_loss: 3.6164 val_score: 0.788  best_val_score: 0.793  last_epoch t=58.61s, total_time_elapsed t=793.0s\n",
      "811/811 [==============================] - 59s 72ms/step - loss: 3.1408 - val_loss: 3.6164\n",
      "Epoch 14/14\n",
      "811/811 [==============================] - ETA: 0s - loss: 3.1262Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.1262 - val_loss: 3.5772 val_score: 0.789  best_val_score: 0.793  last_epoch t=59.12s, total_time_elapsed t=852.0s\n",
      "811/811 [==============================] - 59s 73ms/step - loss: 3.1262 - val_loss: 3.5772\n",
      "Model finished with val loss: 3.57720947265625\n",
      "23/23 [==============================] - 4s 67ms/step\n",
      "Val Score: 0.7928747953298481\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Avg val score: 0.8211663731127243 and std val score: 0.04442420148395574\n"
     ]
    }
   ],
   "source": [
    "n_folds = 10\n",
    "fold_run_order = list(np.arange(1, n_folds+1))\n",
    "\n",
    "val_preds_lst = []\n",
    "val_series_lst = []\n",
    "val_starts_splits_lst = []\n",
    "\n",
    "val_y_lst = []\n",
    "models_lst = []\n",
    "val_scores = []\n",
    "\n",
    "model_dct = {}\n",
    "\n",
    "for fold_num in fold_run_order:\n",
    "    \n",
    "    X_s, y_s, series_ids = load_data(cfg.processed_data_path)\n",
    "    \n",
    "\n",
    "    print(f'\\n\\n\\n-----------Starting fold: {fold_num}-----------\\n\\n\\n')\n",
    "    \n",
    "    series_ids_val = splits_df.filter(pl.col(f'{n_folds}_fold') == fold_num)['series_id'].to_numpy()\n",
    "\n",
    "    val_idxs = [series_ids.index(s) for s in series_ids_val]\n",
    "    trn_idxs = np.setdiff1d(np.arange(len(series_ids)), val_idxs)\n",
    "    \n",
    "    print(f'Using {len(trn_idxs)} series for training and {len(val_idxs)} series for validation')\n",
    "\n",
    "    X_s_trn, y_s_trn, series_ids_trn = [X_s[i] for i in trn_idxs], [y_s[i] for i in trn_idxs], [series_ids[i] for i in trn_idxs]\n",
    "    X_s_val, y_s_val, series_ids_val = [X_s[i] for i in val_idxs], [y_s[i] for i in val_idxs], [series_ids[i] for i in val_idxs]\n",
    "\n",
    "    trn_ds = SleepDataset(X_s_trn, y_s_trn, series_ids_trn, cfg.samp_freq, remove_no_dets=False, is_train=True)\n",
    "    norm_params = trn_ds.norm_params\n",
    "    model_dct[fold_num] = norm_params\n",
    "    \n",
    "    val_ds = SleepDataset(X_s_val, y_s_val, series_ids_val, cfg.samp_freq, remove_no_dets=False, is_train=False, norm_params=norm_params)\n",
    "\n",
    "    del X_s_trn, y_s_trn,  X_s_val, y_s_val, series_ids_trn, X_s, y_s, series_ids\n",
    "    _ = gc.collect()\n",
    "\n",
    "\n",
    "    model = get_model(trn_ds.X.shape[1:])\n",
    "    \n",
    "    print(f'Total model parameters: {model.count_params()}')\n",
    "\n",
    "    val_events_df = train_events.filter(pl.col('series_id').is_in(series_ids_val))\n",
    "    inter_eval = IntervalEvaluation(val_ds, val_events_df, cfg.samp_freq, (trn_ds.X.shape[0]//model_cfg.batch_size)*model_cfg.n_epochs, model_cfg.start_lr, model_cfg.end_lr)\n",
    "    \n",
    "    model.fit(trn_ds.X, trn_ds.y,\n",
    "          epochs=model_cfg.n_epochs,\n",
    "          batch_size=model_cfg.batch_size,\n",
    "          callbacks=[inter_eval],\n",
    "          verbose=1,\n",
    "          validation_data=(val_ds.X, val_ds.y))\n",
    "    \n",
    "    \n",
    "    \n",
    "    last_loss = model.history.history['val_loss'][-1]\n",
    "    print(f'Model finished with val loss: {last_loss}')\n",
    "    \n",
    "    model = inter_eval.best_model\n",
    "    val_preds = model.predict(val_ds.X)\n",
    "    \n",
    "    val_score = post_process_preds(val_events_df, val_preds, val_ds.series_ids, val_ds.starts_splits, cfg.samp_freq, get_score=True)\n",
    "    print(f'Val Score: {val_score}')\n",
    "    \n",
    "    val_scores.append(val_score)\n",
    "    \n",
    "    val_y_lst.append(val_ds.y)\n",
    "    val_preds_lst.append(val_preds)\n",
    "    val_series_lst.append(val_ds.series_ids)\n",
    "    val_starts_splits_lst.append(val_ds.starts_splits)\n",
    "    \n",
    "    tf.keras.models.save_model(model, os.path.join(cfg.output_dir, cfg.ver, f'tf_model_fold_{fold_num}.h5'))\n",
    "    \n",
    "    del trn_ds, val_ds, model, inter_eval\n",
    "    _ = gc.collect()\n",
    "\n",
    "avg_val_score, std_val_score = np.mean(val_scores), np.std(val_scores)\n",
    "print(f'Avg val score: {avg_val_score} and std val score: {std_val_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ff482f",
   "metadata": {},
   "source": [
    "###### series_ids_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6d9e397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7204, 17280, 2) (7204, 17280, 2) (7204,)\n"
     ]
    }
   ],
   "source": [
    "val_starts_splits_all = np.concatenate(val_starts_splits_lst)\n",
    "val_preds_all = np.concatenate(val_preds_lst)\n",
    "val_series_all = np.concatenate(val_series_lst)\n",
    "val_y_all = np.concatenate(val_y_lst)\n",
    "\n",
    "print(val_preds_all.shape, val_y_all.shape, val_series_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "990abfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127946340, 5)\n",
      "(122539680, 5)\n",
      "(124485120, 4)\n",
      "(122539680, 4)\n"
     ]
    }
   ],
   "source": [
    "# The code is creating a DataFrame `oof_preds_df` that contains the predicted values for the \"onset\" and \"wakeup\" columns.\n",
    "\n",
    "res_steps = []\n",
    "res_preds_onsets = []\n",
    "res_preds_wakeups = []\n",
    "\n",
    "res_series_ids = []\n",
    "\n",
    "start = 0\n",
    "while start < len(val_preds_all):\n",
    "    \n",
    "    end = start+1\n",
    "    while end < len(val_preds_all) and val_series_all[end] == val_series_all[start]:\n",
    "        end += 1\n",
    "        \n",
    "    preds = val_preds_all[start:end]\n",
    "    \n",
    "    steps = np.concatenate([val_starts_splits_all[idx] + np.arange(len(val_preds_all[idx])) for idx in range(start, end)])\n",
    "    preds = preds.reshape((preds.shape[0]*preds.shape[1]), 2)\n",
    "    \n",
    "    res_preds_onsets.append(preds[:, 0])\n",
    "    res_preds_wakeups.append(preds[:, 1])\n",
    "    \n",
    "    res_steps.append(steps)\n",
    "    ser_id = val_series_all[start]\n",
    "    \n",
    "    res_series_ids.append([ser_id for _ in range(len(preds))])\n",
    "    \n",
    "    start=end\n",
    "    \n",
    "    \n",
    "oof_preds_df = pl.DataFrame().with_columns([\n",
    "    pl.Series(np.concatenate(res_series_ids)).alias('series_id'),\n",
    "    pl.Series(np.concatenate(res_steps)).alias('step'),\n",
    "    pl.Series(np.concatenate(res_preds_onsets)).alias('onset'),\n",
    "    pl.Series(np.concatenate(res_preds_wakeups)).alias('wakeup')\n",
    "\n",
    "])\n",
    "\n",
    "train_series = pl.read_parquet(cfg.train_series_path)\n",
    "print(train_series.shape)\n",
    "train_series = train_series.filter(pl.col('series_id').is_in(list(np.unique(val_series_all))))\n",
    "train_series = train_series.with_columns(pl.col('step').cast(pl.Int64))\n",
    "print(train_series.shape)\n",
    "\n",
    "\n",
    "\n",
    "print(oof_preds_df.shape)\n",
    "oof_preds_df = train_series[['series_id', 'step']].join(oof_preds_df, on=['series_id', 'step'], how='left')\n",
    "print(oof_preds_df.shape)\n",
    "\n",
    "oof_preds_df.write_parquet(os.path.join(cfg.output_dir, cfg.ver, 'oof_preds.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc0cef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r ../data_processed/{cfg.ver}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "800ba20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05e1944c3818\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 2\n",
    "# sid = val_series_all.filter(pl.col())\n",
    "plt.plot(val_y_all[i], label=\"Actual Values\")\n",
    "print(val_series_all[i])\n",
    "\n",
    "plt.plot(val_preds_all[i], label=\"Predicted Values\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2a3f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_nikhil(preds_orig, k_orig=150, max_thresh=0.05, max_count=500):\n",
    "    \n",
    "    preds=preds_orig.copy()\n",
    "    preds = np.convolve(preds, np.array([0.2, 0.6, 0.2]), mode='same')\n",
    "\n",
    "    count = 0\n",
    "    base=6.75\n",
    "    \n",
    "    k = k_orig\n",
    "    prev_max = None\n",
    "\n",
    "    scores = []\n",
    "    indices = []\n",
    "    \n",
    "    while True:\n",
    "            \n",
    "        curr_max_idx = np.argmax(preds)\n",
    "        curr_max = preds[curr_max_idx] \n",
    "        \n",
    "        if (curr_max < max_thresh) or count > max_count:\n",
    "            break\n",
    "        \n",
    "        indices.append(curr_max_idx)\n",
    "        scores.append(curr_max)\n",
    "        \n",
    "        preds[curr_max_idx] = 0\n",
    "        \n",
    "        supress_rates = np.logspace(start=0, stop=1, num=k, base=base)/base\n",
    "        supress_rates[:20] = 0\n",
    "        # supress_rates[20:] += 0.1\n",
    "\n",
    "        preds[max(curr_max_idx-k, 0):curr_max_idx] *= supress_rates[:min(k, curr_max_idx-0)][::-1]\n",
    "\n",
    "        preds[curr_max_idx+1:min(curr_max_idx+k+1, preds.shape[0])] *= supress_rates[:min(k, preds.shape[0]-curr_max_idx-1)]\n",
    "\n",
    "        prev_max = curr_max\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    return indices, scores\n",
    "\n",
    "def get_actual_preds(val_preds, val_series_ids, val_steps, type_):\n",
    "    times = []\n",
    "    series_ids = []\n",
    "    scores = []\n",
    "    scores_y = []\n",
    "    \n",
    "    for i in np.arange(len(val_preds)):\n",
    "        \n",
    "        vp_i = val_preds[i]\n",
    "        ser_id = val_series_ids[i]\n",
    "        \n",
    "        col_index = 0 if type_ == \"onset\" else 1\n",
    "        other_col_index = 1 if type_ == \"onset\" else 0\n",
    "        \n",
    "        preds = vp_i[:, col_index]\n",
    "        preds_other = vp_i[:, other_col_index]\n",
    "\n",
    "        height_thresh = 0.05\n",
    "        \n",
    "        peaks, peak_scores = nms_nikhil(preds)\n",
    "\n",
    "        times.extend(val_steps[i][peaks])\n",
    "        scores.extend(list(peak_scores))\n",
    "        series_ids.extend([ser_id] * len(peaks))\n",
    "\n",
    "    return np.array(series_ids), np.array(times), np.array(scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def post_process_preds(val_events_df, val_preds, val_series_ids, val_starts_splits, samp_freq, get_score=False):\n",
    "    \n",
    "#     val_steps = np.concatenate([val_starts_splits[idx] + np.arange(len(val_preds[idx])) for idx in range(val_preds.shape[0])])\n",
    "\n",
    "    val_res = []\n",
    "    \n",
    "    prev_ser_id = None\n",
    "    \n",
    "    res_steps = []\n",
    "    res_preds = []\n",
    "    res_series_ids = []\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(val_preds):\n",
    "        \n",
    "        end = start+1\n",
    "        while end < len(val_preds) and val_series_ids[end] == val_series_ids[start]:\n",
    "            end += 1\n",
    "            \n",
    "        preds = val_preds[start:end]\n",
    "        \n",
    "        steps = np.concatenate([val_starts_splits[idx] + np.arange(len(val_preds[idx])) for idx in range(start, end)])\n",
    "        preds = preds.reshape((preds.shape[0]*preds.shape[1]), 2)\n",
    "        \n",
    "        res_preds.append(preds)\n",
    "        res_steps.append(steps)\n",
    "        res_series_ids.append(val_series_ids[start])\n",
    "        \n",
    "        start=end\n",
    "        \n",
    "        \n",
    "#     print(len(res_series_ids), len(res_steps), len(res_preds))\n",
    "\n",
    "    series_ids_onsets, onsets,  scores_onsets = get_actual_preds(res_preds, res_series_ids, res_steps, 'onset')\n",
    "    series_ids_wakeups, wakeups,  scores_wakeups =get_actual_preds(res_preds, res_series_ids, res_steps, 'wakeup')\n",
    "    \n",
    "    \n",
    "    onset_preds = pl.DataFrame().with_columns([pl.Series(series_ids_onsets).alias('series_id'),\n",
    "                                           pl.Series(onsets).cast(pl.Int64).alias('step'),\n",
    "                                           pl.lit('onset').alias('event'),\n",
    "                                           pl.Series(scores_onsets).alias('score')])\n",
    "\n",
    "    wakeup_preds = pl.DataFrame().with_columns([pl.Series(series_ids_wakeups).alias('series_id'),\n",
    "                                               pl.Series(wakeups).cast(pl.Int64).cast(pl.Int64).alias('step'),\n",
    "                                               pl.lit('wakeup').alias('event'),\n",
    "                                               pl.Series(scores_wakeups).alias('score')])\n",
    "    \n",
    "    val_preds_df = pl.concat([onset_preds, wakeup_preds]).sort(by=['series_id', 'step'])\n",
    "    \n",
    "    if get_score:\n",
    "        toleranaces = {'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]}\n",
    "        comp_score = metric_fast.comp_scorer(\n",
    "        val_events_df,\n",
    "        val_preds_df,\n",
    "        tolerances = toleranaces,\n",
    "        )\n",
    "        return comp_score\n",
    "    \n",
    "    else:\n",
    "        return val_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10354814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8104805500602077\n",
      "0.8905906691126992\n",
      "0.8447164398555154\n",
      "0.8444944727192153\n",
      "0.8127955505922131\n",
      "0.7953424761182206\n",
      "0.7191334872219555\n",
      "0.8555174238499521\n",
      "0.8498881563788429\n",
      "0.7909417592262871\n",
      "CPU times: user 35.4 s, sys: 43.5 s, total: 1min 18s\n",
      "Wall time: 1min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8213900985135109, 0.04483117438553839)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "scores = []\n",
    "\n",
    "for fold_num in range(1, n_folds+1):\n",
    "    \n",
    "    test_ser_ids = list(np.unique(val_series_lst[fold_num-1]))\n",
    "\n",
    "\n",
    "    val_events_df = train_events.filter(pl.col('series_id').is_in(test_ser_ids))\n",
    "    score = post_process_preds(val_events_df,\n",
    "                               val_preds_lst[fold_num-1],\n",
    "                               val_series_lst[fold_num-1],\n",
    "                               val_starts_splits_lst[fold_num-1],\n",
    "                               cfg.samp_freq,\n",
    "                               get_score=True)\n",
    "    print(score)\n",
    "    scores.append(score)\n",
    "    \n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e911c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds_df = post_process_preds(val_events_df, val_preds_all, val_series_all, val_starts_splits_all, cfg.samp_freq, get_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b93f28c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_nearest_multiple(df, column_name='step'):\n",
    "    df[column_name] = df[column_name].apply(lambda x: x+1 if x%12==0  else x)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b63863c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds_df = round_to_nearest_multiple(val_preds_df.to_pandas())\n",
    "val_preds_df = pl.DataFrame(val_preds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02c02a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerances = {'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "               'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5dcf3c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.811546857079024"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_fast.comp_scorer(train_events, val_preds_df, tolerances=tolerances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa0b2952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261376, 4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bbdae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:06<00:00,  4.31it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_partitions  = val_preds_df.partition_by(by=['series_id'], maintain_order=True, as_dict=True)\n",
    "events_partitions  = val_events_df.partition_by(by=['series_id'], maintain_order=True, as_dict=True)\n",
    "\n",
    "tolerances = {'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "               'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]}\n",
    "\n",
    "scores_dict = {}\n",
    "for ser_id in tqdm(events_partitions.keys()):\n",
    "    scores_dict[ser_id] = metric_fast.comp_scorer(events_partitions[ser_id], preds_partitions[ser_id],\n",
    "                                                  tolerances)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47012ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a596ad0b82aa    0.354350\n",
       "0a96f4993bd7    0.576204\n",
       "062dbd4c95e6    0.627559\n",
       "fa149c3c4bde    0.729653\n",
       "aa81faa78747    0.731018\n",
       "de6fedfb6139    0.742531\n",
       "ad425f3ee76d    0.781065\n",
       "c68260cc9e8f    0.793531\n",
       "51c49c540b4e    0.797994\n",
       "c908a0ad3e31    0.803042\n",
       "83fa182bec3a    0.806568\n",
       "bfe41e96d12f    0.810714\n",
       "0cd1e3d0ed95    0.849057\n",
       "1c7c0bad1263    0.855242\n",
       "e2b60820c325    0.869627\n",
       "c8053490cec2    0.882568\n",
       "e586cbfa7762    0.891209\n",
       "eec197a4bdca    0.891943\n",
       "ebb6fae8ed43    0.907523\n",
       "9ddd40f2cb36    0.920076\n",
       "87a6cbb7c4ed    0.927780\n",
       "78569a801a38    0.929197\n",
       "2b0a1fa8eba8    0.933291\n",
       "c7d693f24684    0.956661\n",
       "1b92be89db4c    0.956774\n",
       "939932f1822d    0.975336\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(scores_dict).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6a12ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_attributes_dict = {k: v for k, v in cfg.__dict__.items() if not k.startswith('__') and not callable(v)}\n",
    "joblib.dump(cfg_attributes_dict, os.path.join(cfg.output_dir, cfg.ver, 'cfg.pkl'))\n",
    "joblib.dump(model_dct, os.path.join(cfg.output_dir, cfg.ver, 'model_dct.pkl'))\n",
    "\n",
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_series_all.npy'), val_series_all)\n",
    "\n",
    "\n",
    "val_preds_df.to_pandas().to_csv(os.path.join(cfg.output_dir, cfg.ver, 'val_preds_df.csv'), index=False)\n",
    "\n",
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_preds_all.npy'), val_preds_all)\n",
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_y_all.npy'), val_y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89201dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "meta_json = {\n",
    "  \"title\": f\"sleep-model-{cfg.ver}\",\n",
    "  \"id\": f\"nikhilmishradev/sleep-model-{cfg.ver}\",\n",
    "  \"licenses\": [\n",
    "    {\n",
    "      \"name\": \"CC0-1.0\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "json.dump(meta_json, open(os.path.join(cfg.output_dir, cfg.ver, 'dataset-metadata.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "482e8991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/.kaggle/kaggle.json'\n",
      "Skipping folder: .ipynb_checkpoints; use '--dir-mode' to upload folders\n",
      "Starting upload for file cfg.pkl\n",
      "100%|███████████████████████████████████████████| 119/119 [00:03<00:00, 39.3B/s]\n",
      "Upload successful: cfg.pkl (119B)\n",
      "Starting upload for file model_dct.pkl\n",
      "100%|██████████████████████████████████████| 4.00k/4.00k [00:02<00:00, 1.77kB/s]\n",
      "Upload successful: model_dct.pkl (4KB)\n",
      "Starting upload for file oof_preds.parquet\n",
      "100%|██████████████████████████████████████| 1.02G/1.02G [01:00<00:00, 18.3MB/s]\n",
      "Upload successful: oof_preds.parquet (1GB)\n",
      "Starting upload for file tf_model_fold_1.h5\n",
      "100%|████████████████████████████████████████| 145M/145M [00:11<00:00, 13.6MB/s]\n",
      "Upload successful: tf_model_fold_1.h5 (145MB)\n",
      "Starting upload for file tf_model_fold_10.h5\n",
      "100%|████████████████████████████████████████| 145M/145M [00:11<00:00, 13.1MB/s]\n",
      "Upload successful: tf_model_fold_10.h5 (145MB)\n",
      "Starting upload for file tf_model_fold_2.h5\n",
      "100%|████████████████████████████████████████| 145M/145M [00:13<00:00, 11.3MB/s]\n",
      "Upload successful: tf_model_fold_2.h5 (145MB)\n",
      "Starting upload for file tf_model_fold_3.h5\n",
      "100%|████████████████████████████████████████| 145M/145M [00:10<00:00, 14.1MB/s]\n",
      "Upload successful: tf_model_fold_3.h5 (145MB)\n",
      "Starting upload for file tf_model_fold_4.h5\n",
      "100%|████████████████████████████████████████| 145M/145M [00:11<00:00, 12.7MB/s]\n",
      "Upload successful: tf_model_fold_4.h5 (145MB)\n",
      "Starting upload for file tf_model_fold_5.h5\n",
      "100%|████████████████████████████████████████| 145M/145M [00:11<00:00, 13.3MB/s]\n",
      "Upload successful: tf_model_fold_5.h5 (145MB)\n",
      "Starting upload for file tf_model_fold_6.h5\n",
      "100%|████████████████████████████████████████| 145M/145M [00:10<00:00, 14.1MB/s]\n",
      "Upload successful: tf_model_fold_6.h5 (145MB)\n",
      "Starting upload for file tf_model_fold_7.h5\n",
      "100%|████████████████████████████████████████| 145M/145M [00:11<00:00, 13.8MB/s]\n",
      "Upload successful: tf_model_fold_7.h5 (145MB)\n",
      "Starting upload for file tf_model_fold_8.h5\n",
      "100%|████████████████████████████████████████| 145M/145M [00:10<00:00, 13.9MB/s]\n",
      "Upload successful: tf_model_fold_8.h5 (145MB)\n",
      "Starting upload for file tf_model_fold_9.h5\n",
      "100%|████████████████████████████████████████| 145M/145M [00:11<00:00, 13.5MB/s]\n",
      "Upload successful: tf_model_fold_9.h5 (145MB)\n",
      "Starting upload for file val_preds_df.csv\n",
      "100%|██████████████████████████████████████| 11.3M/11.3M [00:03<00:00, 3.04MB/s]\n",
      "Upload successful: val_preds_df.csv (11MB)\n",
      "Starting upload for file val_series_all.npy\n",
      "100%|█████████████████████████████████████████| 338k/338k [00:02<00:00, 143kB/s]\n",
      "Upload successful: val_series_all.npy (338KB)\n",
      "Your private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/nikhilmishradev/sleep-model-fm-v21-final\n"
     ]
    }
   ],
   "source": [
    "# !rm -r ../outputs/vx/*\n",
    "# !cp -r {os.path.join(cfg.output_dir, cfg.ver)}/* ../outputs/vx\n",
    "# !rm ../outputs/vx/val_preds_all.npy ../outputs/vx/val_y_all.npy\n",
    "# !pip install -q kaggle\n",
    "# !kaggle datasets create -p ../outputs/vx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4ed9dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_preds_all.npy'), val_preds_all)\n",
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_y_all.npy'), val_y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61dd1def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fm-v21-final'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf4d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
