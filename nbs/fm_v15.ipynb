{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b58577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade -q numpy numba polars lightgbm tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b305924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 07:56:28.940194: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'src.metric_fast' from '/home/sleep-kaggle/kaggle_final_solution/../src/metric_fast.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src import metric_fast\n",
    "import joblib\n",
    "import time\n",
    "import importlib\n",
    "importlib.reload(metric_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b91c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    samp_freq=1\n",
    "    gaussian_overlay_len = 60\n",
    "    std_dev_num = 2400\n",
    "    ver='fm-v15-final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5642b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_json = json.load(open('../settings.json', 'r'))\n",
    "print(settings_json)\n",
    "\n",
    "for k,v in settings_json.items():\n",
    "    setattr(cfg, k, v)\n",
    "    \n",
    "print(cfg.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a8ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_events = pl.read_ipc(os.path.join(cfg.processed_data_path, 'train_events.ipc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_df = train_events[['series_id']].unique(maintain_order=True)\n",
    "splits_df = splits_df.to_pandas()\n",
    "\n",
    "for n_splits in [5, 7, 10]:\n",
    "    folds = KFold(n_splits, shuffle=True, random_state=5)\n",
    "\n",
    "    splits_df[f'{n_splits}_fold'] = 0\n",
    "    for i, (trn_idx, val_idx) in enumerate(folds.split(splits_df['series_id'], splits_df['series_id'])):\n",
    "        \n",
    "        splits_df.loc[val_idx, f'{n_splits}_fold'] = i+1\n",
    "        \n",
    "        \n",
    "splits_df = pl.DataFrame(splits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86c90a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory):\n",
    "    X_s = []\n",
    "    y_s = []\n",
    "    series_ids = []\n",
    "\n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        if filename.endswith('_X.npy'):\n",
    "            series_id = filename.split('_X.npy')[0]\n",
    "            X = np.load(os.path.join(directory, filename))\n",
    "            y = np.load(os.path.join(directory, f'{series_id}_y.npy'))\n",
    "\n",
    "            X_s.append(X)\n",
    "            y_s.append(y)\n",
    "            series_ids.append(series_id)\n",
    "\n",
    "    return X_s, y_s, series_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_y(y):\n",
    "    \n",
    "    for i in range(y.shape[1]):\n",
    "        \n",
    "        mean = y[:,i].mean()\n",
    "        std = y[:,i].std()\n",
    "        y[:,i] = (y[:,i]-mean)/(std+1e-16)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e715f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_pad(X, y=None, split_length=1440, stride=1440):\n",
    "    \"\"\"\n",
    "    Splits and pads the arrays X and y using a sliding window.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.array): Array with shape (timesteps, features)\n",
    "    - y (np.array): Array with shape (timesteps, 2)\n",
    "    - split_length (int): Desired timestep length for the resulting arrays.\n",
    "    - stride (int): Step size for sliding window.\n",
    "\n",
    "    Returns:\n",
    "    - List of arrays for X and y, each with shape (split_length, features)\n",
    "    \"\"\"\n",
    "    \n",
    "    if y is not None and len(X) != len(y):\n",
    "        raise ValueError(\"X and y should have the same number of timesteps.\")\n",
    "    \n",
    "    timesteps, features = X.shape\n",
    "    \n",
    "    # Create empty lists to store split segments\n",
    "    X_splits = []\n",
    "    y_splits = []\n",
    "    starts = []\n",
    "    \n",
    "    # Use sliding window to extract segments\n",
    "    \n",
    "    for start in range(0, timesteps, stride):\n",
    "        end = start + split_length\n",
    "        if end <= timesteps:\n",
    "            starts.append(start)\n",
    "            X_splits.append(X[start:end].copy())\n",
    "            if y is not None:\n",
    "                y_splits.append(y[start:end].copy())\n",
    "        else:\n",
    "            # If the segment is shorter than split_length, pad it\n",
    "            starts.append(start)\n",
    "            padding_length = end - timesteps\n",
    "            X_segment_padded = np.pad(X[start:], ((0, padding_length), (0, 0)), mode='constant', constant_values=-9)\n",
    "            X_splits.append(X_segment_padded)\n",
    "            \n",
    "            if y is not None:\n",
    "                y_segment_padded = np.pad(y[start:], ((0, padding_length), (0, 0)), mode='constant', constant_values=0)\n",
    "                y_splits.append(y_segment_padded)\n",
    "                \n",
    "            break\n",
    "            \n",
    "            \n",
    "    if y is not None:\n",
    "        return X_splits, y_splits, starts\n",
    "    \n",
    "    return X_splits, starts\n",
    "\n",
    "\n",
    "\n",
    "class SleepDataset:\n",
    "    \n",
    "    def __init__(self, X_s, y_s=None, series_ids=None, samp_freq=None, remove_no_dets=True, is_train=False, split_factor=1, norm_params=None):\n",
    "        \n",
    "        self.split_len = (24*60*12) // cfg.samp_freq\n",
    "        self.split_strides = self.split_len\n",
    "        self.remove_no_dets = remove_no_dets\n",
    "        self.is_train = is_train\n",
    "\n",
    "        print(f'Using a split len of {self.split_len}')\n",
    "        self.create_dataset(X_s, y_s, series_ids)\n",
    "\n",
    "        if self.is_train:\n",
    "            self.norm_params = self.calculate_norm_params()\n",
    "        else:\n",
    "            if norm_params is None:\n",
    "                raise ValueError(\"Normalization parameters must be provided for non-training data.\")\n",
    "            self.norm_params = norm_params\n",
    "\n",
    "        self.normalize_data()\n",
    "\n",
    "    def calculate_norm_params(self):\n",
    "        mean = np.mean(self.X, axis=(0, 1))\n",
    "        std = np.std(self.X, axis=(0, 1))\n",
    "        return {'mean': mean, 'std': std}\n",
    "\n",
    "    def normalize_data(self):\n",
    "        self.X = (self.X - self.norm_params['mean']) / (1e-6 + self.norm_params['std'])\n",
    "        \n",
    "        \n",
    "    def create_dataset(self, X_s, y_s=None, series_ids=None):\n",
    "        X_s_splits, y_s_splits, series_splits, starts_splits = [], [], [], []\n",
    "\n",
    "        for i in tqdm(range(len(X_s))):\n",
    "            x_splits, starts = split_and_pad(X_s[i].copy(), split_length = self.split_len, stride=self.split_strides)\n",
    "            X_s_splits.extend(x_splits)\n",
    "            starts_splits.extend(starts)\n",
    "\n",
    "            if y_s is not None:\n",
    "                _, y_splits, _ = split_and_pad(X_s[i].copy(), y_s[i].copy(), split_length=self.split_len, stride=self.split_strides)\n",
    "                y_s_splits.extend(y_splits)\n",
    "\n",
    "            if series_ids is not None:\n",
    "                series_splits.extend([series_ids[i] for _ in range(len(x_splits))])\n",
    "            \n",
    "        self.X = np.array(X_s_splits)\n",
    "        self.starts_splits = np.array(starts_splits)\n",
    "        \n",
    "        if y_s is not None:\n",
    "            self.y = np.array(y_s_splits)\n",
    "\n",
    "            if self.remove_no_dets:\n",
    "                fltr = (self.y[:, :, 1].sum(axis=1) + self.y[:, :, 0].sum(axis=1)) != 0\n",
    "                self.X = self.X[fltr]\n",
    "                self.y = self.y[fltr]\n",
    "                if series_ids is not None:\n",
    "                    self.series_ids = np.array(series_splits)[fltr]\n",
    "                else:\n",
    "                    self.series_ids = None\n",
    "            \n",
    "            self.y = np.array([normalize_y(yts) for yts in self.y])\n",
    "\n",
    "        else:\n",
    "            self.y = None\n",
    "\n",
    "        if series_ids is not None:\n",
    "            if not hasattr(self, 'series_ids'):\n",
    "                self.series_ids = np.array(series_splits)\n",
    "            print(f'X: {self.X.shape}, y: {self.y.shape if self.y is not None else \"Not provided\"}, series_ids: {self.series_ids.shape}')\n",
    "        else:\n",
    "            print(f'X: {self.X.shape}, y: {self.y.shape if self.y is not None else \"Not provided\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d11033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_nikhil(preds_orig, k_orig=125, max_thresh=0.05, max_count=1000):\n",
    "    \n",
    "    preds=preds_orig.copy()\n",
    "\n",
    "    count = 0\n",
    "    base=6.75\n",
    "    \n",
    "    k = k_orig\n",
    "    prev_max = None\n",
    "\n",
    "    scores = []\n",
    "    indices = []\n",
    "    \n",
    "    while True:\n",
    "            \n",
    "        curr_max_idx = np.argmax(preds)\n",
    "        curr_max = preds[curr_max_idx] \n",
    "        \n",
    "        if (curr_max < max_thresh) or count > max_count:\n",
    "            break\n",
    "        \n",
    "        indices.append(curr_max_idx)\n",
    "        scores.append(curr_max)\n",
    "        \n",
    "        preds[curr_max_idx] = 0\n",
    "        \n",
    "        supress_rates = np.logspace(start=0, stop=1, num=k, base=base)/base\n",
    "\n",
    "        preds[max(curr_max_idx-k, 0):curr_max_idx] *= supress_rates[:min(k, curr_max_idx-0)][::-1]\n",
    "        preds[curr_max_idx+1:min(curr_max_idx+k+1, preds.shape[0])] *= supress_rates[:min(k, preds.shape[0]-curr_max_idx-1)]\n",
    "\n",
    "        prev_max = curr_max\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    return indices, scores\n",
    "\n",
    "def get_actual_preds(val_preds, val_series_ids, val_steps, type_):\n",
    "    times = []\n",
    "    series_ids = []\n",
    "    scores = []\n",
    "    scores_y = []\n",
    "    \n",
    "    for i in np.arange(len(val_preds)):\n",
    "        \n",
    "        vp_i = val_preds[i]\n",
    "        ser_id = val_series_ids[i]\n",
    "        \n",
    "        col_index = 0 if type_ == \"onset\" else 1\n",
    "        other_col_index = 1 if type_ == \"onset\" else 0\n",
    "        \n",
    "        preds = vp_i[:, col_index]\n",
    "        preds_other = vp_i[:, other_col_index]\n",
    "\n",
    "        height_thresh = 0.05\n",
    "        \n",
    "        peaks, peak_scores = nms_nikhil(preds)\n",
    "\n",
    "        times.extend(val_steps[i][peaks])\n",
    "        scores.extend(list(peak_scores))\n",
    "        series_ids.extend([ser_id] * len(peaks))\n",
    "\n",
    "    return np.array(series_ids), np.array(times), np.array(scores)\n",
    "\n",
    "\n",
    "def post_process_preds(val_events_df, val_preds, val_series_ids, val_starts_splits, samp_freq, get_score=False):\n",
    "    \n",
    "#     val_steps = np.concatenate([val_starts_splits[idx] + np.arange(len(val_preds[idx])) for idx in range(val_preds.shape[0])])\n",
    "\n",
    "    val_res = []\n",
    "    \n",
    "    prev_ser_id = None\n",
    "    \n",
    "    res_steps = []\n",
    "    res_preds = []\n",
    "    res_series_ids = []\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(val_preds):\n",
    "        \n",
    "        end = start+1\n",
    "        while end < len(val_preds) and val_series_ids[end] == val_series_ids[start]:\n",
    "            end += 1\n",
    "            \n",
    "        preds = val_preds[start:end]\n",
    "        \n",
    "        steps = np.concatenate([val_starts_splits[idx] + np.arange(len(val_preds[idx])) for idx in range(start, end)])\n",
    "        preds = preds.reshape((preds.shape[0]*preds.shape[1]), 2)\n",
    "        \n",
    "        res_preds.append(preds)\n",
    "        res_steps.append(steps)\n",
    "        res_series_ids.append(val_series_ids[start])\n",
    "        \n",
    "        start=end\n",
    "        \n",
    "        \n",
    "#     print(len(res_series_ids), len(res_steps), len(res_preds))\n",
    "\n",
    "    series_ids_onsets, onsets,  scores_onsets = get_actual_preds(res_preds, res_series_ids, res_steps, 'onset')\n",
    "    series_ids_wakeups, wakeups,  scores_wakeups =get_actual_preds(res_preds, res_series_ids, res_steps, 'wakeup')\n",
    "    \n",
    "    \n",
    "    onset_preds = pl.DataFrame().with_columns([pl.Series(series_ids_onsets).alias('series_id'),\n",
    "                                           pl.Series(onsets).cast(pl.Int64).alias('step'),\n",
    "                                           pl.lit('onset').alias('event'),\n",
    "                                           pl.Series(scores_onsets).alias('score')])\n",
    "\n",
    "    wakeup_preds = pl.DataFrame().with_columns([pl.Series(series_ids_wakeups).alias('series_id'),\n",
    "                                               pl.Series(wakeups).cast(pl.Int64).cast(pl.Int64).alias('step'),\n",
    "                                               pl.lit('wakeup').alias('event'),\n",
    "                                               pl.Series(scores_wakeups).alias('score')])\n",
    "    \n",
    "    val_preds_df = pl.concat([onset_preds, wakeup_preds]).sort(by=['series_id', 'step'])\n",
    "    \n",
    "    if get_score:\n",
    "        toleranaces = {'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]}\n",
    "        comp_score = metric_fast.comp_scorer(\n",
    "        val_events_df,\n",
    "        val_preds_df,\n",
    "        tolerances = toleranaces,\n",
    "        )\n",
    "        return comp_score\n",
    "    \n",
    "    else:\n",
    "        return val_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f558db15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_exponential_lr(start_lr, end_lr, num_steps, decay_rate=None):\n",
    "    \"\"\"\n",
    "    Calculate the exponentially decreasing learning rates.\n",
    "\n",
    "    Parameters:\n",
    "    start_lr (float): Initial learning rate.\n",
    "    end_lr (float): Final learning rate.\n",
    "    num_steps (int): Total number of steps over which the learning rate should decay.\n",
    "    decay_rate (float): Decay rate per step. If None, it will be computed based on start_lr, end_lr, and num_steps.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing the learning rate for each step.\n",
    "    \"\"\"\n",
    "    if decay_rate is None:\n",
    "        # Calculate decay rate based on the start_lr, end_lr, and num_steps\n",
    "        decay_rate = (end_lr / start_lr) ** (1 / (num_steps - 1))\n",
    "\n",
    "    learning_rates = [start_lr * (decay_rate ** step) for step in range(num_steps)]\n",
    "    return learning_rates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b288692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntervalEvaluation(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_ds, val_events_df, samp_freq, n_steps, start_lr, end_lr):\n",
    "\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.val_ds = val_ds\n",
    "        self.val_events_df = val_events_df\n",
    "        self.samp_freq = samp_freq\n",
    "        \n",
    "        warmup_pct_steps = 0.25\n",
    "        warmup_steps = int(n_steps * warmup_pct_steps)\n",
    "        self.learning_rates = [start_lr] * (warmup_steps) + calculate_exponential_lr(start_lr, end_lr, n_steps-warmup_steps)\n",
    "        self.best_score = -np.inf\n",
    "        self.best_model = None\n",
    "        self.step_count=0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        if epoch == 0:\n",
    "            self.first_epoch_start_time = self.start_time\n",
    "        \n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        if self.step_count < len(self.learning_rates):\n",
    "\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, self.learning_rates[self.step_count])\n",
    "            self.curr_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        self.step_count += 1\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        val_preds = self.model.predict(self.val_ds.X, batch_size=32, verbose=0)[:, :, :2]\n",
    "        val_score = post_process_preds(self.val_events_df, val_preds, self.val_ds.series_ids, self.val_ds.starts_splits, self.samp_freq, get_score=True)\n",
    "        \n",
    "        if val_score > self.best_score:\n",
    "            self.best_score = val_score\n",
    "            self.best_model = tf.keras.models.clone_model(self.model)\n",
    "            self.best_model.set_weights(self.model.get_weights()) \n",
    "        \n",
    "        total_time = round(time.time() - self.start_time, 2)\n",
    "        total_seconds_till_now = round(time.time() - self.first_epoch_start_time, 0)\n",
    "        \n",
    "        print(f\"Epoch: {epoch:03d} curr_lr: {self.curr_lr:.1e} - train_loss: {logs['loss']:.04f} - val_loss: {logs['val_loss']:.04f} val_score: {val_score:.03f}  best_val_score: {self.best_score:.03f}  last_epoch t={total_time:.02f}s, total_time_elapsed t={total_seconds_till_now}s\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8abb817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "\n",
    "    # MultiHead Attention\n",
    "    x = tfa.layers.MultiHeadAttention(\n",
    "        head_size=head_size,\n",
    "        num_heads=num_heads,\n",
    "        use_projection_bias=True,\n",
    "        dropout=dropout\n",
    "    )([inputs, inputs, inputs])\n",
    "\n",
    "    # Residual connection with LayerNormalization and Scaling\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + inputs) * (0.5 ** 0.5)\n",
    "    \n",
    "    # Feed Forward Part\n",
    "    ff = tf.keras.layers.Dense(ff_dim, activation='gelu')(x)\n",
    "    ff = tf.keras.layers.Dropout(dropout)(ff)\n",
    "    ff = tf.keras.layers.Dense(inputs.shape[-1])(ff)\n",
    "    ff = tf.keras.layers.Dropout(dropout)(ff)\n",
    "    \n",
    "    # Residual connection with LayerNormalization and Scaling for FFN\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + ff) * (0.5 ** 0.5)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e2e3cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mce_loss(y_true, y_pred):\n",
    "    # Clip the ground truth and predictions to the range (-100, 100)\n",
    "    y_true_clipped = tf.clip_by_value(y_true, -100, 100)\n",
    "    y_pred_clipped = tf.clip_by_value(y_pred, -100, 100)\n",
    "\n",
    "    # Calculate the mean cubed error\n",
    "    loss = tf.reduce_mean(tf.abs(y_true_clipped - y_pred_clipped) ** 3)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "366821c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 07:56:31.187796: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 07:56:31.315684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46413 MB memory:  -> device: 0, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:23:00.0, compute capability: 8.9\n",
      "/usr/local/lib/python3.8/dist-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34620204"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conv_block(inputs, num_filters):\n",
    "    x = tf.keras.layers.Conv1D(num_filters, 8, padding=\"same\")(inputs)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.Conv1D(num_filters, 8, padding=\"same\")(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def encoder_block(inputs, num_filters):\n",
    "    x = conv_block(inputs, num_filters)\n",
    "    p = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(inputs, concat_tensors, num_filters):\n",
    "    x = tf.keras.layers.Conv1DTranspose(num_filters, 4, strides=2, padding=\"same\")(inputs)\n",
    "    i = len(concat_tensors)-1\n",
    "    for concat_tensor in concat_tensors:\n",
    "        concat_tensor_max = tf.keras.layers.MaxPool1D(pool_size=2**i)(concat_tensor)\n",
    "        x = tf.keras.layers.Concatenate()([x, concat_tensor_max])\n",
    "        i -= 1\n",
    "        \n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def get_model(input_shape, num_blocks=4):\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    orig_n_channels = x.shape[-1]\n",
    "    \n",
    "    x_t = []\n",
    "    n_split_fact = 6\n",
    "\n",
    "    for i in range(x.shape[-1]):\n",
    "        x_sp = tf.reshape(x[:, :, i], (-1, x.shape[1]//n_split_fact, n_split_fact))\n",
    "        \n",
    "        if i < orig_n_channels:\n",
    "        \n",
    "            # Calculating mean, max, and standard deviation\n",
    "            mean = tf.reduce_mean(x_sp, axis=-1, keepdims=True)\n",
    "            max_val = tf.reduce_max(x_sp, axis=-1, keepdims=True)\n",
    "            std_dev = tf.math.reduce_std(x_sp, axis=-1, keepdims=True)\n",
    "            min_val = tf.reduce_min(x_sp, axis=-1, keepdims=True)\n",
    "\n",
    "            x_sp = tf.keras.layers.Concatenate()([x_sp, mean, max_val, std_dev, min_val])\n",
    "        \n",
    "        x_sp = tf.keras.layers.Dense(n_split_fact*16, activation='relu')(x_sp)\n",
    "        \n",
    "        x_t.append(x_sp)\n",
    "        \n",
    "    \n",
    "    x_c1d = tf.keras.layers.Conv1D(64, kernel_size=12, strides=n_split_fact, padding=\"same\")(x)\n",
    "    x_c1d = tf.keras.layers.ReLU()(x_c1d)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()(x_t + [x_c1d])# tf.reshape(x, (-1, -1, 3))\n",
    "    \n",
    "    # Initial filter size\n",
    "    fsz = 64\n",
    "    \n",
    "    # Lists to hold the encoder and pooling outputs\n",
    "    encoder_outputs = []\n",
    "    pooling_outputs = []\n",
    "    \n",
    "    # Encoder\n",
    "    for i in range(num_blocks):\n",
    "        if i == 0:\n",
    "            # First block receives the model input\n",
    "            enc_out, pool_out = encoder_block(x, fsz * (2 ** i))\n",
    "        else:\n",
    "            # Subsequent blocks receive the pooling output of the previous block\n",
    "            enc_out, pool_out = encoder_block(pooling_outputs[-1], fsz * (2 ** i))\n",
    "        \n",
    "        encoder_outputs.append(enc_out)\n",
    "        pooling_outputs.append(pool_out)\n",
    "    \n",
    "    # Bottleneck\n",
    "    bottleneck = conv_block(pooling_outputs[-1], fsz * (2 ** num_blocks))\n",
    "    \n",
    "    \n",
    "    def gru_conv(x):\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(x.shape[-1]//4, return_sequences=True))(x)\n",
    "        for i in range(1):\n",
    "            x_conv = tf.keras.layers.Conv1D(x.shape[-1], kernel_size=(4,), padding='same', activation='relu')(x)\n",
    "            x = tf.keras.layers.Dense(x.shape[-1], activation='relu')(x_conv)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    for _ in range(1):\n",
    "        bottleneck = transformer_encoder(bottleneck, head_size=16, num_heads=8, ff_dim=256, dropout=0)\n",
    "\n",
    "    bottleneck = gru_conv(bottleneck)\n",
    "\n",
    "    decoder_input = bottleneck\n",
    "    for i in range(num_blocks - 1, -1, -1):\n",
    "        decoder_output = decoder_block(decoder_input, encoder_outputs[:i+1], fsz * (2 ** i))\n",
    "        decoder_input = decoder_output\n",
    "    \n",
    "    # Output Layer\n",
    "    x = tf.keras.layers.Conv1D(2*n_split_fact, 1, padding=\"same\", activation=\"linear\")(decoder_output)\n",
    "    x = tf.reshape(x, shape=(-1, x.shape[1]*n_split_fact, 2))\n",
    "    \n",
    "    outputs = x\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "    model.compile(optimizer=tfa.optimizers.AdamW(weight_decay=1e-4), loss=custom_mce_loss)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Usage:\n",
    "input_shape = (17280, 9)  # replace with your input shape\n",
    "num_blocks = 4  # specify the number of encoder/decoder blocks\n",
    "model = get_model(input_shape, num_blocks)\n",
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "055e0501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 17280, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f1b3d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_cfg:\n",
    "    n_epochs = 14\n",
    "    batch_size = 8\n",
    "    start_lr = 6e-5\n",
    "    end_lr = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:04<00:00, 129.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 2-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 211 series for training and 53 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211/211 [00:11<00:00, 18.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5742, 17280, 32), y: (5742, 17280, 2), series_ids: (5742,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 73.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1462, 17280, 32), y: (1462, 17280, 2), series_ids: (1462,)\n",
      "Total model parameters: 35792652\n",
      "Epoch 1/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 07:57:30.447323: I tensorflow/stream_executor/cuda/cuda_blas.cc:1633] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-12-14 07:57:30.467279: I tensorflow/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "718/718 [==============================] - ETA: 0s - loss: 4.4141Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.4141 - val_loss: 3.5563 val_score: 0.769  best_val_score: 0.769  last_epoch t=73.32s, total_time_elapsed t=73.0s\n",
      "718/718 [==============================] - 73s 93ms/step - loss: 4.4141 - val_loss: 3.5563\n",
      "Epoch 2/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.6932Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6932 - val_loss: 3.6727 val_score: 0.755  best_val_score: 0.769  last_epoch t=59.55s, total_time_elapsed t=133.0s\n",
      "718/718 [==============================] - 60s 83ms/step - loss: 3.6932 - val_loss: 3.6727\n",
      "Epoch 3/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.5766Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.5766 - val_loss: 3.4079 val_score: 0.785  best_val_score: 0.785  last_epoch t=64.02s, total_time_elapsed t=197.0s\n",
      "718/718 [==============================] - 64s 89ms/step - loss: 3.5766 - val_loss: 3.4079\n",
      "Epoch 4/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.5133Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.5133 - val_loss: 3.4243 val_score: 0.780  best_val_score: 0.785  last_epoch t=59.59s, total_time_elapsed t=256.0s\n",
      "718/718 [==============================] - 60s 83ms/step - loss: 3.5133 - val_loss: 3.4243\n",
      "Epoch 5/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.4497Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.4497 - val_loss: 3.3157 val_score: 0.788  best_val_score: 0.788  last_epoch t=64.51s, total_time_elapsed t=321.0s\n",
      "718/718 [==============================] - 65s 90ms/step - loss: 3.4497 - val_loss: 3.3157\n",
      "Epoch 6/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.4012Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.4012 - val_loss: 3.2930 val_score: 0.797  best_val_score: 0.797  last_epoch t=61.27s, total_time_elapsed t=382.0s\n",
      "718/718 [==============================] - 61s 85ms/step - loss: 3.4012 - val_loss: 3.2930\n",
      "Epoch 7/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.3799Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3799 - val_loss: 3.4307 val_score: 0.794  best_val_score: 0.797  last_epoch t=58.88s, total_time_elapsed t=441.0s\n",
      "718/718 [==============================] - 59s 82ms/step - loss: 3.3799 - val_loss: 3.4307\n",
      "Epoch 8/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.3518Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.3518 - val_loss: 3.3386 val_score: 0.790  best_val_score: 0.797  last_epoch t=62.71s, total_time_elapsed t=504.0s\n",
      "718/718 [==============================] - 63s 87ms/step - loss: 3.3518 - val_loss: 3.3386\n",
      "Epoch 9/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.3189Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.3189 - val_loss: 3.2885 val_score: 0.800  best_val_score: 0.800  last_epoch t=65.33s, total_time_elapsed t=569.0s\n",
      "718/718 [==============================] - 65s 91ms/step - loss: 3.3189 - val_loss: 3.2885\n",
      "Epoch 10/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.3008Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.3008 - val_loss: 3.2264 val_score: 0.808  best_val_score: 0.808  last_epoch t=62.05s, total_time_elapsed t=631.0s\n",
      "718/718 [==============================] - 62s 86ms/step - loss: 3.3008 - val_loss: 3.2264\n",
      "Epoch 11/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.2845Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2845 - val_loss: 3.2403 val_score: 0.806  best_val_score: 0.808  last_epoch t=59.61s, total_time_elapsed t=691.0s\n",
      "718/718 [==============================] - 60s 83ms/step - loss: 3.2845 - val_loss: 3.2403\n",
      "Epoch 12/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.2794Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.2794 - val_loss: 3.3644 val_score: 0.799  best_val_score: 0.808  last_epoch t=62.39s, total_time_elapsed t=753.0s\n",
      "718/718 [==============================] - 62s 87ms/step - loss: 3.2794 - val_loss: 3.3644\n",
      "Epoch 13/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.2570Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.2570 - val_loss: 3.2576 val_score: 0.798  best_val_score: 0.808  last_epoch t=61.63s, total_time_elapsed t=815.0s\n",
      "718/718 [==============================] - 62s 86ms/step - loss: 3.2570 - val_loss: 3.2576\n",
      "Epoch 14/14\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.2345Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.2345 - val_loss: 3.3544 val_score: 0.791  best_val_score: 0.808  last_epoch t=61.60s, total_time_elapsed t=877.0s\n",
      "718/718 [==============================] - 62s 86ms/step - loss: 3.2345 - val_loss: 3.3544\n",
      "Model finished with val loss: 3.354443311691284\n",
      "46/46 [==============================] - 5s 54ms/step\n",
      "Val Score: 0.8081756201817432\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:05<00:00, 100.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 1-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 211 series for training and 53 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211/211 [00:10<00:00, 20.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5831, 17280, 32), y: (5831, 17280, 2), series_ids: (5831,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:01<00:00, 40.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1373, 17280, 32), y: (1373, 17280, 2), series_ids: (1373,)\n",
      "Total model parameters: 35792652\n",
      "Epoch 1/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 4.2703Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.2703 - val_loss: 3.9469 val_score: 0.786  best_val_score: 0.786  last_epoch t=71.97s, total_time_elapsed t=72.0s\n",
      "729/729 [==============================] - 72s 90ms/step - loss: 4.2703 - val_loss: 3.9469\n",
      "Epoch 2/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.5914Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.5914 - val_loss: 3.7166 val_score: 0.810  best_val_score: 0.810  last_epoch t=61.01s, total_time_elapsed t=133.0s\n",
      "729/729 [==============================] - 61s 84ms/step - loss: 3.5914 - val_loss: 3.7166\n",
      "Epoch 3/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.4931Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.4931 - val_loss: 3.9475 val_score: 0.793  best_val_score: 0.810  last_epoch t=59.32s, total_time_elapsed t=192.0s\n",
      "729/729 [==============================] - 59s 81ms/step - loss: 3.4931 - val_loss: 3.9475\n",
      "Epoch 4/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.4304Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4304 - val_loss: 3.6782 val_score: 0.793  best_val_score: 0.810  last_epoch t=58.84s, total_time_elapsed t=251.0s\n",
      "729/729 [==============================] - 59s 81ms/step - loss: 3.4304 - val_loss: 3.6782\n",
      "Epoch 5/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.3555Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.3555 - val_loss: 3.6650 val_score: 0.812  best_val_score: 0.812  last_epoch t=63.69s, total_time_elapsed t=315.0s\n",
      "729/729 [==============================] - 64s 87ms/step - loss: 3.3555 - val_loss: 3.6650\n",
      "Epoch 6/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.3535Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3535 - val_loss: 3.5904 val_score: 0.807  best_val_score: 0.812  last_epoch t=58.27s, total_time_elapsed t=373.0s\n",
      "729/729 [==============================] - 58s 80ms/step - loss: 3.3535 - val_loss: 3.5904\n",
      "Epoch 7/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.2913Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.2913 - val_loss: 3.5483 val_score: 0.815  best_val_score: 0.815  last_epoch t=63.00s, total_time_elapsed t=436.0s\n",
      "729/729 [==============================] - 63s 86ms/step - loss: 3.2913 - val_loss: 3.5483\n",
      "Epoch 8/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.2647Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.2647 - val_loss: 3.6709 val_score: 0.813  best_val_score: 0.815  last_epoch t=58.95s, total_time_elapsed t=495.0s\n",
      "729/729 [==============================] - 59s 81ms/step - loss: 3.2647 - val_loss: 3.6709\n",
      "Epoch 9/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.2482Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.2482 - val_loss: 3.5627 val_score: 0.821  best_val_score: 0.821  last_epoch t=63.10s, total_time_elapsed t=558.0s\n",
      "729/729 [==============================] - 63s 87ms/step - loss: 3.2482 - val_loss: 3.5627\n",
      "Epoch 10/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.2278Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2278 - val_loss: 3.5433 val_score: 0.822  best_val_score: 0.822  last_epoch t=60.81s, total_time_elapsed t=619.0s\n",
      "729/729 [==============================] - 61s 83ms/step - loss: 3.2278 - val_loss: 3.5433\n",
      "Epoch 11/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.2079Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2079 - val_loss: 3.5919 val_score: 0.819  best_val_score: 0.822  last_epoch t=58.41s, total_time_elapsed t=677.0s\n",
      "729/729 [==============================] - 58s 80ms/step - loss: 3.2079 - val_loss: 3.5919\n",
      "Epoch 12/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.1969Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.1969 - val_loss: 3.5839 val_score: 0.817  best_val_score: 0.822  last_epoch t=60.69s, total_time_elapsed t=738.0s\n",
      "729/729 [==============================] - 61s 83ms/step - loss: 3.1969 - val_loss: 3.5839\n",
      "Epoch 13/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.1740Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.1740 - val_loss: 3.6240 val_score: 0.824  best_val_score: 0.824  last_epoch t=63.93s, total_time_elapsed t=802.0s\n",
      "729/729 [==============================] - 64s 88ms/step - loss: 3.1740 - val_loss: 3.6240\n",
      "Epoch 14/14\n",
      "729/729 [==============================] - ETA: 0s - loss: 3.1629Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.1629 - val_loss: 3.5610 val_score: 0.820  best_val_score: 0.824  last_epoch t=58.66s, total_time_elapsed t=861.0s\n",
      "729/729 [==============================] - 59s 80ms/step - loss: 3.1629 - val_loss: 3.5610\n",
      "Model finished with val loss: 3.5610404014587402\n",
      "43/43 [==============================] - 4s 60ms/step\n",
      "Val Score: 0.823502501880517\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:03<00:00, 154.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 3-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 211 series for training and 53 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211/211 [00:08<00:00, 24.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5778, 17280, 32), y: (5778, 17280, 2), series_ids: (5778,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:01<00:00, 32.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1426, 17280, 32), y: (1426, 17280, 2), series_ids: (1426,)\n",
      "Total model parameters: 35792652\n",
      "Epoch 1/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 4.4906Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.4906 - val_loss: 3.8097 val_score: 0.748  best_val_score: 0.748  last_epoch t=76.74s, total_time_elapsed t=77.0s\n",
      "723/723 [==============================] - 77s 98ms/step - loss: 4.4906 - val_loss: 3.8097\n",
      "Epoch 2/14\n",
      "722/723 [============================>.] - ETA: 0s - loss: 3.6765Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6758 - val_loss: 3.5728 val_score: 0.784  best_val_score: 0.784  last_epoch t=63.71s, total_time_elapsed t=140.0s\n",
      "723/723 [==============================] - 64s 88ms/step - loss: 3.6758 - val_loss: 3.5728\n",
      "Epoch 3/14\n",
      "722/723 [============================>.] - ETA: 0s - loss: 3.5761Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.5765 - val_loss: 3.7460 val_score: 0.786  best_val_score: 0.786  last_epoch t=64.11s, total_time_elapsed t=205.0s\n",
      "723/723 [==============================] - 64s 89ms/step - loss: 3.5765 - val_loss: 3.7460\n",
      "Epoch 4/14\n",
      "722/723 [============================>.] - ETA: 0s - loss: 3.5035Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.5041 - val_loss: 3.5233 val_score: 0.798  best_val_score: 0.798  last_epoch t=62.75s, total_time_elapsed t=267.0s\n",
      "723/723 [==============================] - 63s 87ms/step - loss: 3.5041 - val_loss: 3.5233\n",
      "Epoch 5/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.4549Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.4549 - val_loss: 3.5436 val_score: 0.788  best_val_score: 0.798  last_epoch t=61.32s, total_time_elapsed t=329.0s\n",
      "723/723 [==============================] - 61s 85ms/step - loss: 3.4549 - val_loss: 3.5436\n",
      "Epoch 6/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.4076Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.4076 - val_loss: 3.4394 val_score: 0.815  best_val_score: 0.815  last_epoch t=62.90s, total_time_elapsed t=392.0s\n",
      "723/723 [==============================] - 63s 87ms/step - loss: 3.4076 - val_loss: 3.4394\n",
      "Epoch 7/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.3734Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3734 - val_loss: 3.3721 val_score: 0.818  best_val_score: 0.818  last_epoch t=63.23s, total_time_elapsed t=455.0s\n",
      "723/723 [==============================] - 63s 88ms/step - loss: 3.3734 - val_loss: 3.3721\n",
      "Epoch 8/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.3490Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.3490 - val_loss: 3.4317 val_score: 0.808  best_val_score: 0.818  last_epoch t=62.19s, total_time_elapsed t=517.0s\n",
      "723/723 [==============================] - 62s 86ms/step - loss: 3.3490 - val_loss: 3.4317\n",
      "Epoch 9/14\n",
      "722/723 [============================>.] - ETA: 0s - loss: 3.3097Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.3098 - val_loss: 3.3575 val_score: 0.821  best_val_score: 0.821  last_epoch t=63.93s, total_time_elapsed t=581.0s\n",
      "723/723 [==============================] - 64s 88ms/step - loss: 3.3098 - val_loss: 3.3575\n",
      "Epoch 10/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.2892Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2892 - val_loss: 3.3643 val_score: 0.822  best_val_score: 0.822  last_epoch t=63.82s, total_time_elapsed t=645.0s\n",
      "723/723 [==============================] - 64s 88ms/step - loss: 3.2892 - val_loss: 3.3643\n",
      "Epoch 11/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.2628Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2628 - val_loss: 3.3488 val_score: 0.821  best_val_score: 0.822  last_epoch t=62.08s, total_time_elapsed t=707.0s\n",
      "723/723 [==============================] - 62s 86ms/step - loss: 3.2628 - val_loss: 3.3488\n",
      "Epoch 12/14\n",
      "722/723 [============================>.] - ETA: 0s - loss: 3.2575Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.2571 - val_loss: 3.3316 val_score: 0.822  best_val_score: 0.822  last_epoch t=62.82s, total_time_elapsed t=770.0s\n",
      "723/723 [==============================] - 63s 87ms/step - loss: 3.2571 - val_loss: 3.3316\n",
      "Epoch 13/14\n",
      "723/723 [==============================] - ETA: 0s - loss: 3.2485Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.2485 - val_loss: 3.3130 val_score: 0.824  best_val_score: 0.824  last_epoch t=64.89s, total_time_elapsed t=835.0s\n",
      "723/723 [==============================] - 65s 90ms/step - loss: 3.2485 - val_loss: 3.3130\n",
      "Epoch 14/14\n",
      "722/723 [============================>.] - ETA: 0s - loss: 3.2286Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.2301 - val_loss: 3.4164 val_score: 0.816  best_val_score: 0.824  last_epoch t=62.14s, total_time_elapsed t=897.0s\n",
      "723/723 [==============================] - 62s 86ms/step - loss: 3.2301 - val_loss: 3.4164\n",
      "Model finished with val loss: 3.416410446166992\n",
      "45/45 [==============================] - 6s 54ms/step\n",
      "Val Score: 0.8238811262484698\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:04<00:00, 106.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 4-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 211 series for training and 53 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211/211 [00:08<00:00, 24.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5820, 17280, 32), y: (5820, 17280, 2), series_ids: (5820,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:01<00:00, 34.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1384, 17280, 32), y: (1384, 17280, 2), series_ids: (1384,)\n",
      "Total model parameters: 35792652\n",
      "Epoch 1/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 4.3513Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.3513 - val_loss: 3.8239 val_score: 0.764  best_val_score: 0.764  last_epoch t=74.24s, total_time_elapsed t=74.0s\n",
      "728/728 [==============================] - 74s 94ms/step - loss: 4.3513 - val_loss: 3.8239\n",
      "Epoch 2/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.6691Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6691 - val_loss: 3.7467 val_score: 0.792  best_val_score: 0.792  last_epoch t=62.40s, total_time_elapsed t=137.0s\n",
      "728/728 [==============================] - 62s 86ms/step - loss: 3.6691 - val_loss: 3.7467\n",
      "Epoch 3/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.5306Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.5306 - val_loss: 3.6201 val_score: 0.793  best_val_score: 0.793  last_epoch t=63.14s, total_time_elapsed t=200.0s\n",
      "728/728 [==============================] - 63s 87ms/step - loss: 3.5306 - val_loss: 3.6201\n",
      "Epoch 4/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.4921Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4921 - val_loss: 3.6033 val_score: 0.797  best_val_score: 0.797  last_epoch t=62.90s, total_time_elapsed t=263.0s\n",
      "728/728 [==============================] - 63s 86ms/step - loss: 3.4921 - val_loss: 3.6033\n",
      "Epoch 5/14\n",
      "727/728 [============================>.] - ETA: 0s - loss: 3.4243Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.4252 - val_loss: 3.5611 val_score: 0.804  best_val_score: 0.804  last_epoch t=62.66s, total_time_elapsed t=325.0s\n",
      "728/728 [==============================] - 63s 86ms/step - loss: 3.4252 - val_loss: 3.5611\n",
      "Epoch 6/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.3720Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3720 - val_loss: 3.4848 val_score: 0.807  best_val_score: 0.807  last_epoch t=63.14s, total_time_elapsed t=388.0s\n",
      "728/728 [==============================] - 63s 87ms/step - loss: 3.3720 - val_loss: 3.4848\n",
      "Epoch 7/14\n",
      "727/728 [============================>.] - ETA: 0s - loss: 3.3553Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3564 - val_loss: 3.5293 val_score: 0.799  best_val_score: 0.807  last_epoch t=60.76s, total_time_elapsed t=449.0s\n",
      "728/728 [==============================] - 61s 83ms/step - loss: 3.3564 - val_loss: 3.5293\n",
      "Epoch 8/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.3104Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.3104 - val_loss: 3.4593 val_score: 0.804  best_val_score: 0.807  last_epoch t=60.81s, total_time_elapsed t=510.0s\n",
      "728/728 [==============================] - 61s 84ms/step - loss: 3.3104 - val_loss: 3.4593\n",
      "Epoch 9/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.2988Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.2988 - val_loss: 3.6007 val_score: 0.802  best_val_score: 0.807  last_epoch t=59.58s, total_time_elapsed t=570.0s\n",
      "728/728 [==============================] - 60s 82ms/step - loss: 3.2988 - val_loss: 3.6007\n",
      "Epoch 10/14\n",
      "727/728 [============================>.] - ETA: 0s - loss: 3.2644Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2653 - val_loss: 3.5177 val_score: 0.808  best_val_score: 0.808  last_epoch t=63.37s, total_time_elapsed t=633.0s\n",
      "728/728 [==============================] - 63s 87ms/step - loss: 3.2653 - val_loss: 3.5177\n",
      "Epoch 11/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.2516Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2516 - val_loss: 3.4933 val_score: 0.809  best_val_score: 0.809  last_epoch t=62.11s, total_time_elapsed t=695.0s\n",
      "728/728 [==============================] - 62s 85ms/step - loss: 3.2516 - val_loss: 3.4933\n",
      "Epoch 12/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.2303Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.2303 - val_loss: 3.5626 val_score: 0.812  best_val_score: 0.812  last_epoch t=62.20s, total_time_elapsed t=757.0s\n",
      "728/728 [==============================] - 62s 85ms/step - loss: 3.2303 - val_loss: 3.5626\n",
      "Epoch 13/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.2225Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.2225 - val_loss: 3.5090 val_score: 0.806  best_val_score: 0.812  last_epoch t=60.17s, total_time_elapsed t=818.0s\n",
      "728/728 [==============================] - 60s 83ms/step - loss: 3.2225 - val_loss: 3.5090\n",
      "Epoch 14/14\n",
      "728/728 [==============================] - ETA: 0s - loss: 3.2132Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.2132 - val_loss: 3.5162 val_score: 0.812  best_val_score: 0.812  last_epoch t=60.82s, total_time_elapsed t=878.0s\n",
      "728/728 [==============================] - 61s 84ms/step - loss: 3.2132 - val_loss: 3.5162\n",
      "Model finished with val loss: 3.5161869525909424\n",
      "44/44 [==============================] - 6s 60ms/step\n",
      "Val Score: 0.8123507485248058\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:04<00:00, 108.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 5-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 212 series for training and 52 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 212/212 [00:08<00:00, 26.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5645, 17280, 32), y: (5645, 17280, 2), series_ids: (5645,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:01<00:00, 29.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1559, 17280, 32), y: (1559, 17280, 2), series_ids: (1559,)\n",
      "Total model parameters: 35792652\n",
      "Epoch 1/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 4.4727Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.4727 - val_loss: 3.6533 val_score: 0.775  best_val_score: 0.775  last_epoch t=78.62s, total_time_elapsed t=79.0s\n",
      "706/706 [==============================] - 79s 103ms/step - loss: 4.4727 - val_loss: 3.6533\n",
      "Epoch 2/14\n",
      "705/706 [============================>.] - ETA: 0s - loss: 3.7218Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.7217 - val_loss: 3.5463 val_score: 0.778  best_val_score: 0.778  last_epoch t=63.19s, total_time_elapsed t=142.0s\n",
      "706/706 [==============================] - 63s 90ms/step - loss: 3.7217 - val_loss: 3.5463\n",
      "Epoch 3/14\n",
      "705/706 [============================>.] - ETA: 0s - loss: 3.6087Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.6077 - val_loss: 3.5841 val_score: 0.790  best_val_score: 0.790  last_epoch t=64.83s, total_time_elapsed t=207.0s\n",
      "706/706 [==============================] - 65s 92ms/step - loss: 3.6077 - val_loss: 3.5841\n",
      "Epoch 4/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.5168Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.5168 - val_loss: 3.3857 val_score: 0.804  best_val_score: 0.804  last_epoch t=64.61s, total_time_elapsed t=271.0s\n",
      "706/706 [==============================] - 65s 92ms/step - loss: 3.5168 - val_loss: 3.3857\n",
      "Epoch 5/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.4612Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.4612 - val_loss: 3.2507 val_score: 0.818  best_val_score: 0.818  last_epoch t=65.01s, total_time_elapsed t=336.0s\n",
      "706/706 [==============================] - 65s 92ms/step - loss: 3.4612 - val_loss: 3.2507\n",
      "Epoch 6/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.4302Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.4302 - val_loss: 3.2572 val_score: 0.812  best_val_score: 0.818  last_epoch t=62.63s, total_time_elapsed t=399.0s\n",
      "706/706 [==============================] - 63s 89ms/step - loss: 3.4302 - val_loss: 3.2572\n",
      "Epoch 7/14\n",
      "705/706 [============================>.] - ETA: 0s - loss: 3.3915Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3919 - val_loss: 3.3175 val_score: 0.812  best_val_score: 0.818  last_epoch t=61.17s, total_time_elapsed t=460.0s\n",
      "706/706 [==============================] - 61s 87ms/step - loss: 3.3919 - val_loss: 3.3175\n",
      "Epoch 8/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.3730Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.3730 - val_loss: 3.3124 val_score: 0.817  best_val_score: 0.818  last_epoch t=62.91s, total_time_elapsed t=523.0s\n",
      "706/706 [==============================] - 63s 89ms/step - loss: 3.3730 - val_loss: 3.3124\n",
      "Epoch 9/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.3292Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.3292 - val_loss: 3.2376 val_score: 0.822  best_val_score: 0.822  last_epoch t=63.65s, total_time_elapsed t=587.0s\n",
      "706/706 [==============================] - 64s 90ms/step - loss: 3.3292 - val_loss: 3.2376\n",
      "Epoch 10/14\n",
      "705/706 [============================>.] - ETA: 0s - loss: 3.3123Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.3130 - val_loss: 3.2629 val_score: 0.814  best_val_score: 0.822  last_epoch t=61.58s, total_time_elapsed t=648.0s\n",
      "706/706 [==============================] - 62s 87ms/step - loss: 3.3130 - val_loss: 3.2629\n",
      "Epoch 11/14\n",
      "705/706 [============================>.] - ETA: 0s - loss: 3.3093Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.3083 - val_loss: 3.5323 val_score: 0.803  best_val_score: 0.822  last_epoch t=61.59s, total_time_elapsed t=710.0s\n",
      "706/706 [==============================] - 62s 87ms/step - loss: 3.3083 - val_loss: 3.5323\n",
      "Epoch 12/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.2805Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.2805 - val_loss: 3.2587 val_score: 0.822  best_val_score: 0.822  last_epoch t=62.15s, total_time_elapsed t=772.0s\n",
      "706/706 [==============================] - 62s 88ms/step - loss: 3.2805 - val_loss: 3.2587\n",
      "Epoch 13/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.2738Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.2738 - val_loss: 3.1992 val_score: 0.818  best_val_score: 0.822  last_epoch t=63.28s, total_time_elapsed t=835.0s\n",
      "706/706 [==============================] - 63s 90ms/step - loss: 3.2738 - val_loss: 3.1992\n",
      "Epoch 14/14\n",
      "706/706 [==============================] - ETA: 0s - loss: 3.2574Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.2574 - val_loss: 3.2310 val_score: 0.821  best_val_score: 0.822  last_epoch t=62.17s, total_time_elapsed t=897.0s\n",
      "706/706 [==============================] - 62s 88ms/step - loss: 3.2574 - val_loss: 3.2310\n",
      "Model finished with val loss: 3.2310428619384766\n",
      "49/49 [==============================] - 6s 64ms/step\n",
      "Val Score: 0.8216766286875696\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Avg val score: 0.8179173251046212 and std val score: 0.0064308798271079425\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "fold_run_order = [2, 1, 3, 4, 5]\n",
    "\n",
    "val_preds_lst = []\n",
    "val_series_lst = []\n",
    "val_starts_splits_lst = []\n",
    "\n",
    "val_y_lst = []\n",
    "models_lst = []\n",
    "val_scores = []\n",
    "\n",
    "model_dct = {}\n",
    "\n",
    "for fold_num in fold_run_order:\n",
    "    \n",
    "    X_s, y_s, series_ids = load_data(cfg.processed_data_path)\n",
    "    \n",
    "\n",
    "    print(f'\\n\\n\\n-----------Starting fold: {fold_num}-----------\\n\\n\\n')\n",
    "    \n",
    "    series_ids_val = splits_df.filter(pl.col(f'{n_folds}_fold') == fold_num)['series_id'].to_numpy()\n",
    "\n",
    "    val_idxs = [series_ids.index(s) for s in series_ids_val]\n",
    "    trn_idxs = np.setdiff1d(np.arange(len(series_ids)), val_idxs)\n",
    "    \n",
    "    print(f'Using {len(trn_idxs)} series for training and {len(val_idxs)} series for validation')\n",
    "\n",
    "    X_s_trn, y_s_trn, series_ids_trn = [X_s[i] for i in trn_idxs], [y_s[i] for i in trn_idxs], [series_ids[i] for i in trn_idxs]\n",
    "    X_s_val, y_s_val, series_ids_val = [X_s[i] for i in val_idxs], [y_s[i] for i in val_idxs], [series_ids[i] for i in val_idxs]\n",
    "\n",
    "    trn_ds = SleepDataset(X_s_trn, y_s_trn, series_ids_trn, cfg.samp_freq, remove_no_dets=False, is_train=True)\n",
    "    norm_params = trn_ds.norm_params\n",
    "    model_dct[fold_num] = norm_params\n",
    "    \n",
    "    val_ds = SleepDataset(X_s_val, y_s_val, series_ids_val, cfg.samp_freq, remove_no_dets=False, is_train=False, norm_params=norm_params)\n",
    "\n",
    "    del X_s_trn, y_s_trn,  X_s_val, y_s_val, series_ids_trn, X_s, y_s, series_ids\n",
    "    _ = gc.collect()\n",
    "\n",
    "\n",
    "    model = get_model(trn_ds.X.shape[1:])\n",
    "    \n",
    "    print(f'Total model parameters: {model.count_params()}')\n",
    "\n",
    "    val_events_df = train_events.filter(pl.col('series_id').is_in(series_ids_val))\n",
    "    inter_eval = IntervalEvaluation(val_ds, val_events_df, cfg.samp_freq, (trn_ds.X.shape[0]//model_cfg.batch_size)*model_cfg.n_epochs, model_cfg.start_lr, model_cfg.end_lr)\n",
    "    \n",
    "    model.fit(trn_ds.X, trn_ds.y,\n",
    "          epochs=model_cfg.n_epochs,\n",
    "          batch_size=model_cfg.batch_size,\n",
    "          callbacks=[inter_eval],\n",
    "          verbose=1,\n",
    "          validation_data=(val_ds.X, val_ds.y))\n",
    "    \n",
    "    \n",
    "    \n",
    "    last_loss = model.history.history['val_loss'][-1]\n",
    "    print(f'Model finished with val loss: {last_loss}')\n",
    "    \n",
    "    model = inter_eval.best_model\n",
    "    val_preds = model.predict(val_ds.X)\n",
    "    \n",
    "    val_score = post_process_preds(val_events_df, val_preds, val_ds.series_ids, val_ds.starts_splits, cfg.samp_freq, get_score=True)\n",
    "    print(f'Val Score: {val_score}')\n",
    "    \n",
    "    val_scores.append(val_score)\n",
    "    \n",
    "    val_y_lst.append(val_ds.y)\n",
    "    val_preds_lst.append(val_preds)\n",
    "    val_series_lst.append(val_ds.series_ids)\n",
    "    val_starts_splits_lst.append(val_ds.starts_splits)\n",
    "    \n",
    "    tf.keras.models.save_model(model, os.path.join(cfg.output_dir, cfg.ver, f'tf_model_fold_{fold_num}.h5'))\n",
    "    \n",
    "    del trn_ds, val_ds, model, inter_eval\n",
    "    _ = gc.collect()\n",
    "\n",
    "avg_val_score, std_val_score = np.mean(val_scores), np.std(val_scores)\n",
    "print(f'Avg val score: {avg_val_score} and std val score: {std_val_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4bad85",
   "metadata": {},
   "source": [
    "###### series_ids_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b6461b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7204, 17280, 2) (7204, 17280, 2) (7204,)\n"
     ]
    }
   ],
   "source": [
    "val_starts_splits_all = np.concatenate(val_starts_splits_lst)\n",
    "val_preds_all = np.concatenate(val_preds_lst)\n",
    "val_series_all = np.concatenate(val_series_lst)\n",
    "val_y_all = np.concatenate(val_y_lst)\n",
    "\n",
    "print(val_preds_all.shape, val_y_all.shape, val_series_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aca30735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127946340, 5)\n",
      "(122539680, 5)\n",
      "(124485120, 4)\n",
      "(122539680, 4)\n"
     ]
    }
   ],
   "source": [
    "# The code is creating a DataFrame `oof_preds_df` that contains the predicted values for the \"onset\" and \"wakeup\" columns.\n",
    "\n",
    "res_steps = []\n",
    "res_preds_onsets = []\n",
    "res_preds_wakeups = []\n",
    "\n",
    "res_series_ids = []\n",
    "\n",
    "start = 0\n",
    "while start < len(val_preds_all):\n",
    "    \n",
    "    end = start+1\n",
    "    while end < len(val_preds_all) and val_series_all[end] == val_series_all[start]:\n",
    "        end += 1\n",
    "        \n",
    "    preds = val_preds_all[start:end]\n",
    "    \n",
    "    steps = np.concatenate([val_starts_splits_all[idx] + np.arange(len(val_preds_all[idx])) for idx in range(start, end)])\n",
    "    preds = preds.reshape((preds.shape[0]*preds.shape[1]), 2)\n",
    "    \n",
    "    res_preds_onsets.append(preds[:, 0])\n",
    "    res_preds_wakeups.append(preds[:, 1])\n",
    "    \n",
    "    res_steps.append(steps)\n",
    "    ser_id = val_series_all[start]\n",
    "    \n",
    "    res_series_ids.append([ser_id for _ in range(len(preds))])\n",
    "    \n",
    "    start=end\n",
    "    \n",
    "    \n",
    "oof_preds_df = pl.DataFrame().with_columns([\n",
    "    pl.Series(np.concatenate(res_series_ids)).alias('series_id'),\n",
    "    pl.Series(np.concatenate(res_steps)).alias('step'),\n",
    "    pl.Series(np.concatenate(res_preds_onsets)).alias('onset'),\n",
    "    pl.Series(np.concatenate(res_preds_wakeups)).alias('wakeup')\n",
    "\n",
    "])\n",
    "\n",
    "train_series = pl.read_parquet(cfg.train_series_path)\n",
    "print(train_series.shape)\n",
    "train_series = train_series.filter(pl.col('series_id').is_in(list(np.unique(val_series_all))))\n",
    "train_series = train_series.with_columns(pl.col('step').cast(pl.Int64))\n",
    "print(train_series.shape)\n",
    "\n",
    "\n",
    "\n",
    "print(oof_preds_df.shape)\n",
    "oof_preds_df = train_series[['series_id', 'step']].join(oof_preds_df, on=['series_id', 'step'], how='left')\n",
    "print(oof_preds_df.shape)\n",
    "\n",
    "oof_preds_df.write_parquet(os.path.join(cfg.output_dir, cfg.ver, 'oof_preds.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r ../data_processed/{cfg.ver}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd441fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03d92c9f6f8a\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 2\n",
    "# sid = val_series_all.filter(pl.col())\n",
    "plt.plot(val_y_all[i], label=\"Actual Values\")\n",
    "print(val_series_all[i])\n",
    "\n",
    "plt.plot(val_preds_all[i], label=\"Predicted Values\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2708f144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_nikhil(preds_orig, k_orig=150, max_thresh=0.1, max_count=1000):\n",
    "    \n",
    "    preds=preds_orig.copy()\n",
    "    preds = np.convolve(preds, np.array([0.15, 0.7, 0.15]), mode='same')\n",
    "\n",
    "    count = 0\n",
    "    orig_base=8\n",
    "    \n",
    "    k = k_orig\n",
    "    prev_max = None\n",
    "\n",
    "    scores = []\n",
    "    indices = []\n",
    "    \n",
    "    while True:\n",
    "        base = orig_base\n",
    "        \n",
    "        supress_k = 20\n",
    "        curr_max_idx = np.argmax(preds)\n",
    "        curr_max = preds[curr_max_idx] \n",
    "\n",
    "        if curr_max >= 16:\n",
    "            k = 300   \n",
    "            orig_base = 18\n",
    "            supress_k = 20\n",
    "        \n",
    "        elif curr_max >= 15:\n",
    "            k = 250   \n",
    "            orig_base = 15\n",
    "            supress_k = 20\n",
    "        \n",
    "        elif curr_max >= 14:\n",
    "            k = 225\n",
    "            orig_base = 12\n",
    "            supress_k = 20\n",
    "            \n",
    "        elif curr_max >= 12:\n",
    "            k = 210\n",
    "            orig_base = 10\n",
    "            supress_k = 20\n",
    "        \n",
    "        elif curr_max > 10:\n",
    "            k = 190\n",
    "            orig_base = 7\n",
    "        \n",
    "        elif curr_max > 8:\n",
    "            k = 175\n",
    "            orig_base = 6\n",
    "            \n",
    "        elif curr_max > 4:\n",
    "            k = 150\n",
    "            orig_base = 5\n",
    "            \n",
    "        elif curr_max >= 2:\n",
    "            k = 120\n",
    "            orig_base = 4.5\n",
    "            \n",
    "        elif curr_max >= 1:\n",
    "            k = 80\n",
    "            orig_base = 4\n",
    "            \n",
    "        elif curr_max >= 0.2:\n",
    "            k = 40\n",
    "            orig_base = 3\n",
    "        else:\n",
    "            k = 30\n",
    "            orig_base = 2\n",
    "            \n",
    "        if (curr_max < max_thresh) or count > max_count:\n",
    "            break\n",
    "        \n",
    "        indices.append(curr_max_idx)\n",
    "        scores.append(curr_max)\n",
    "        \n",
    "        preds[curr_max_idx] = 0\n",
    "        \n",
    "        supress_rates = np.logspace(start=0, stop=1, num=k, base=base)/base\n",
    "        supress_rates[:supress_k] = 0\n",
    "        # supress_rates[20:] += 0.1\n",
    "\n",
    "        preds[max(curr_max_idx-k, 0):curr_max_idx] *= supress_rates[:min(k, curr_max_idx-0)][::-1]\n",
    "\n",
    "        preds[curr_max_idx+1:min(curr_max_idx+k+1, preds.shape[0])] *= supress_rates[:min(k, preds.shape[0]-curr_max_idx-1)]\n",
    "\n",
    "        prev_max = curr_max\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    return indices, scores\n",
    "\n",
    "def get_actual_preds(val_preds, val_series_ids, val_steps, type_):\n",
    "    times = []\n",
    "    series_ids = []\n",
    "    scores = []\n",
    "    scores_y = []\n",
    "    \n",
    "    for i in np.arange(len(val_preds)):\n",
    "        \n",
    "        vp_i = val_preds[i]\n",
    "        ser_id = val_series_ids[i]\n",
    "        \n",
    "        col_index = 0 if type_ == \"onset\" else 1\n",
    "        other_col_index = 1 if type_ == \"onset\" else 0\n",
    "        \n",
    "        preds = vp_i[:, col_index]\n",
    "        preds_other = vp_i[:, other_col_index]\n",
    "\n",
    "        height_thresh = 0.05\n",
    "        \n",
    "        peaks, peak_scores = nms_nikhil(preds)\n",
    "\n",
    "        times.extend(val_steps[i][peaks])\n",
    "        scores.extend(list(peak_scores))\n",
    "        series_ids.extend([ser_id] * len(peaks))\n",
    "\n",
    "    return np.array(series_ids), np.array(times), np.array(scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def post_process_preds(val_events_df, val_preds, val_series_ids, val_starts_splits, samp_freq, get_score=False):\n",
    "    \n",
    "#     val_steps = np.concatenate([val_starts_splits[idx] + np.arange(len(val_preds[idx])) for idx in range(val_preds.shape[0])])\n",
    "\n",
    "    val_res = []\n",
    "    \n",
    "    prev_ser_id = None\n",
    "    \n",
    "    res_steps = []\n",
    "    res_preds = []\n",
    "    res_series_ids = []\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(val_preds):\n",
    "        \n",
    "        end = start+1\n",
    "        while end < len(val_preds) and val_series_ids[end] == val_series_ids[start]:\n",
    "            end += 1\n",
    "            \n",
    "        preds = val_preds[start:end]\n",
    "        \n",
    "        steps = np.concatenate([val_starts_splits[idx] + np.arange(len(val_preds[idx])) for idx in range(start, end)])\n",
    "        preds = preds.reshape((preds.shape[0]*preds.shape[1]), 2)\n",
    "        \n",
    "        res_preds.append(preds)\n",
    "        res_steps.append(steps)\n",
    "        res_series_ids.append(val_series_ids[start])\n",
    "        \n",
    "        start=end\n",
    "        \n",
    "        \n",
    "#     print(len(res_series_ids), len(res_steps), len(res_preds))\n",
    "\n",
    "    series_ids_onsets, onsets,  scores_onsets = get_actual_preds(res_preds, res_series_ids, res_steps, 'onset')\n",
    "    series_ids_wakeups, wakeups,  scores_wakeups =get_actual_preds(res_preds, res_series_ids, res_steps, 'wakeup')\n",
    "    \n",
    "    \n",
    "    onset_preds = pl.DataFrame().with_columns([pl.Series(series_ids_onsets).alias('series_id'),\n",
    "                                           pl.Series(onsets).cast(pl.Int64).alias('step'),\n",
    "                                           pl.lit('onset').alias('event'),\n",
    "                                           pl.Series(scores_onsets).alias('score')])\n",
    "\n",
    "    wakeup_preds = pl.DataFrame().with_columns([pl.Series(series_ids_wakeups).alias('series_id'),\n",
    "                                               pl.Series(wakeups).cast(pl.Int64).cast(pl.Int64).alias('step'),\n",
    "                                               pl.lit('wakeup').alias('event'),\n",
    "                                               pl.Series(scores_wakeups).alias('score')])\n",
    "    \n",
    "    val_preds_df = pl.concat([onset_preds, wakeup_preds]).sort(by=['series_id', 'step'])\n",
    "    print(val_preds_df.shape, end=' ')\n",
    "    \n",
    "    if get_score:\n",
    "        toleranaces = {'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]}\n",
    "        comp_score = metric_fast.comp_scorer(\n",
    "        val_events_df,\n",
    "        val_preds_df,\n",
    "        tolerances = toleranaces,\n",
    "        )\n",
    "        return comp_score\n",
    "    \n",
    "    else:\n",
    "        return val_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98191, 4) 0.8090413455380376\n",
      "(103571, 4) 0.8255364502605911\n",
      "(104647, 4) 0.8243587294606393\n",
      "(92868, 4) 0.8148486436146469\n",
      "(102604, 4) 0.824185073542435\n",
      "CPU times: user 1min, sys: 39.9 s, total: 1min 40s\n",
      "Wall time: 1min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.81959404848327, 0.0065264115961309295)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "scores = []\n",
    "\n",
    "for fold_num in range(1, 5+1):\n",
    "    \n",
    "    test_ser_ids = list(np.unique(val_series_lst[fold_num-1]))\n",
    "\n",
    "\n",
    "    val_events_df = train_events.filter(pl.col('series_id').is_in(test_ser_ids))\n",
    "    score = post_process_preds(val_events_df,\n",
    "                               val_preds_lst[fold_num-1],\n",
    "                               val_series_lst[fold_num-1],\n",
    "                               val_starts_splits_lst[fold_num-1],\n",
    "                               cfg.samp_freq,\n",
    "                               get_score=True)\n",
    "    print(score)\n",
    "    scores.append(score)\n",
    "    \n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(501881, 4) "
     ]
    }
   ],
   "source": [
    "val_preds_df = post_process_preds(val_events_df, val_preds_all, val_series_all, val_starts_splits_all, cfg.samp_freq, get_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_nearest_multiple(df, column_name='step'):\n",
    "    df[column_name] = df[column_name].apply(lambda x: x+1 if x%12==0  else x)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds_df = round_to_nearest_multiple(val_preds_df.to_pandas())\n",
    "val_preds_df = pl.DataFrame(val_preds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerances = {'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "               'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8161625023973504"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_fast.comp_scorer(train_events, val_preds_df, tolerances=tolerances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(501881, 4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a2d0d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/52 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:12<00:00,  4.25it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_partitions  = val_preds_df.partition_by(by=['series_id'], maintain_order=True, as_dict=True)\n",
    "events_partitions  = val_events_df.partition_by(by=['series_id'], maintain_order=True, as_dict=True)\n",
    "\n",
    "tolerances = {'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "               'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]}\n",
    "\n",
    "scores_dict = {}\n",
    "for ser_id in tqdm(events_partitions.keys()):\n",
    "    scores_dict[ser_id] = metric_fast.comp_scorer(events_partitions[ser_id], preds_partitions[ser_id],\n",
    "                                                  tolerances)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e6719b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f56824b503a0    0.085576\n",
       "854206f602d0    0.390577\n",
       "703b5efa9bc1    0.567381\n",
       "0a96f4993bd7    0.587142\n",
       "6ca4f4fca6a2    0.616823\n",
       "280e08693c6d    0.648960\n",
       "1762ab70ec76    0.723442\n",
       "cf13ed7e457a    0.725000\n",
       "72bbd1ac3edf    0.764289\n",
       "ccdee561ee5d    0.778035\n",
       "dfc3ccebfdc9    0.808320\n",
       "0402a003dae9    0.808367\n",
       "8b8b9e29171c    0.808392\n",
       "a167532acca2    0.817819\n",
       "038441c925bb    0.817928\n",
       "6bf95a3cf91c    0.818692\n",
       "694faf956ebf    0.827259\n",
       "b84960841a75    0.831605\n",
       "fe90110788d2    0.834450\n",
       "91cb6c98201f    0.843056\n",
       "3d53bfea61d6    0.843383\n",
       "51b23d177971    0.844073\n",
       "483d6545417f    0.849707\n",
       "c38707ef76df    0.866309\n",
       "d2d6b9af0553    0.868193\n",
       "f88e18cb4100    0.868923\n",
       "a81f4472c637    0.881591\n",
       "ce85771a714c    0.886299\n",
       "d150801f3145    0.891119\n",
       "e0d7b0dcf9f3    0.891239\n",
       "ebd76e93ec7d    0.894720\n",
       "8898e6db816d    0.898577\n",
       "b7188813d58a    0.900307\n",
       "062cae666e2a    0.902381\n",
       "91127c2b0e60    0.904921\n",
       "d043c0ca71cd    0.906928\n",
       "18b61dd5aae8    0.908136\n",
       "1955d568d987    0.909878\n",
       "40dce6018935    0.910843\n",
       "72d2234e84e4    0.911787\n",
       "804594bb1f06    0.913989\n",
       "c3072a759efb    0.914286\n",
       "599ca4ed791b    0.916179\n",
       "ce9164297046    0.917364\n",
       "3664fe9233f9    0.919925\n",
       "78569a801a38    0.922540\n",
       "29d3469bd15d    0.925354\n",
       "08db4255286f    0.931322\n",
       "25e2b3dd9c3b    0.933767\n",
       "927dd0c35dfd    0.947846\n",
       "bb5612895813    0.951043\n",
       "76237b9406d5    0.966977\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(scores_dict).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8fde599",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_attributes_dict = {k: v for k, v in cfg.__dict__.items() if not k.startswith('__') and not callable(v)}\n",
    "joblib.dump(cfg_attributes_dict, os.path.join(cfg.output_dir, cfg.ver, 'cfg.pkl'))\n",
    "joblib.dump(model_dct, os.path.join(cfg.output_dir, cfg.ver, 'model_dct.pkl'))\n",
    "\n",
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_series_all.npy'), val_series_all)\n",
    "\n",
    "\n",
    "val_preds_df.to_pandas().to_csv(os.path.join(cfg.output_dir, cfg.ver, 'val_preds_df.csv'), index=False)\n",
    "\n",
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_preds_all.npy'), val_preds_all)\n",
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_y_all.npy'), val_y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56787c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "meta_json = {\n",
    "  \"title\": f\"sleep-model-{cfg.ver}\",\n",
    "  \"id\": f\"nikhilmishradev/sleep-model-{cfg.ver}\",\n",
    "  \"licenses\": [\n",
    "    {\n",
    "      \"name\": \"CC0-1.0\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "json.dump(meta_json, open(os.path.join(cfg.output_dir, cfg.ver, 'dataset-metadata.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5308a572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/.kaggle/kaggle.json'\n",
      "Skipping folder: .ipynb_checkpoints; use '--dir-mode' to upload folders\n",
      "Starting upload for file cfg.pkl\n",
      "100%|███████████████████████████████████████████| 119/119 [00:01<00:00, 59.9B/s]\n",
      "Upload successful: cfg.pkl (119B)\n",
      "Starting upload for file model_dct.pkl\n",
      "100%|████████████████████████████████████████| 1.99k/1.99k [00:02<00:00, 957B/s]\n",
      "Upload successful: model_dct.pkl (2KB)\n",
      "Starting upload for file oof_preds.parquet\n",
      "100%|██████████████████████████████████████| 1.03G/1.03G [00:59<00:00, 18.5MB/s]\n",
      "Upload successful: oof_preds.parquet (1GB)\n",
      "Starting upload for file tf_model_fold_1.h5\n",
      "100%|████████████████████████████████████████| 137M/137M [00:18<00:00, 7.79MB/s]\n",
      "Upload successful: tf_model_fold_1.h5 (137MB)\n",
      "Starting upload for file tf_model_fold_2.h5\n",
      "100%|████████████████████████████████████████| 137M/137M [00:11<00:00, 12.8MB/s]\n",
      "Upload successful: tf_model_fold_2.h5 (137MB)\n",
      "Starting upload for file tf_model_fold_3.h5\n",
      "100%|████████████████████████████████████████| 137M/137M [00:10<00:00, 13.7MB/s]\n",
      "Upload successful: tf_model_fold_3.h5 (137MB)\n",
      "Starting upload for file tf_model_fold_4.h5\n",
      "100%|████████████████████████████████████████| 137M/137M [00:11<00:00, 12.4MB/s]\n",
      "Upload successful: tf_model_fold_4.h5 (137MB)\n",
      "Starting upload for file tf_model_fold_5.h5\n",
      "100%|████████████████████████████████████████| 137M/137M [00:11<00:00, 12.6MB/s]\n",
      "Upload successful: tf_model_fold_5.h5 (137MB)\n",
      "Starting upload for file val_preds_df.csv\n",
      "100%|██████████████████████████████████████| 21.8M/21.8M [00:04<00:00, 4.66MB/s]\n",
      "Upload successful: val_preds_df.csv (22MB)\n",
      "Starting upload for file val_series_all.npy\n",
      "100%|█████████████████████████████████████████| 338k/338k [00:02<00:00, 135kB/s]\n",
      "Upload successful: val_series_all.npy (338KB)\n",
      "Your private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/nikhilmishradev/sleep-model-fm-v15-final\n"
     ]
    }
   ],
   "source": [
    "# !rm -r ../outputs/vx/*\n",
    "# !cp -r {os.path.join(cfg.output_dir, cfg.ver)}/* ../outputs/vx\n",
    "# !rm ../outputs/vx/val_preds_all.npy ../outputs/vx/val_y_all.npy\n",
    "# !pip install -q kaggle\n",
    "# !kaggle datasets create -p ../outputs/vx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af417359",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_preds_all.npy'), val_preds_all)\n",
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_y_all.npy'), val_y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fm-v15-final'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd043dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
