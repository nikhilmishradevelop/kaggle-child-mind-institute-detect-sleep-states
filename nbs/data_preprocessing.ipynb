{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436e5dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "### To be required once per session\n",
    "\n",
    "!pip install --upgrade -q numpy numba polars lightgbm tensorflow-addons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6439b69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-16 06:03:00.761977: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from sklearn.model_selection import KFold\n",
    "sys.path.append('..')\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numba as nb\n",
    "import lightgbm as lgb\n",
    "import sklearn as sk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f39506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy==1.24.4\n",
      "pandas==1.4.3\n",
      "numba==0.58.1\n",
      "polars==0.19.19\n",
      "lightgbm==4.1.0\n",
      "tensorflow==2.10.0\n",
      "tensorflow_addons==0.21.0\n",
      "scikit-learn==0.24.2\n"
     ]
    }
   ],
   "source": [
    "packages = ['numpy', 'pandas', 'numba', 'polars', 'lightgbm', 'tensorflow', 'tensorflow_addons', 'scikit-learn']\n",
    "versions = [np.__version__, pd.__version__, nb.__version__, pl.__version__, lgb.__version__, tf.__version__, tfa.__version__, sk.__version__]\n",
    "\n",
    "for package, version in zip(packages, versions):\n",
    "    print(f'{package}=={version}')\n",
    "    # print(f'{package}=={versions.pop(0)}')\n",
    "    # print(f'{package}=={versions.pop(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab5563d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10 (default, Jun 22 2022, 20:18:18) \n",
      "[GCC 9.4.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "912a62a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec 16 06:03:03 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    On  | 00000000:23:00.0 Off |                  Off |\n",
      "| 30%   39C    P8              26W / 300W |      3MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d3469c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__\n",
    "nb.__version__\n",
    "pl.__version__\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fd5869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    samp_freq=1\n",
    "    gaussian_overlay_len = 60\n",
    "    std_dev_num = 2400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e4beb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_events_path': '../data/train_events.csv', 'train_series_path': '../data/train_series.parquet', 'processed_data_path': '../data_processed_models', 'output_dir': '../outputs'}\n",
      "{'__module__': '__main__', 'samp_freq': 1, 'gaussian_overlay_len': 60, 'std_dev_num': 2400, '__dict__': <attribute '__dict__' of 'cfg' objects>, '__weakref__': <attribute '__weakref__' of 'cfg' objects>, '__doc__': None, 'train_events_path': '../data/train_events.csv', 'train_series_path': '../data/train_series.parquet', 'processed_data_path': '../data_processed_models', 'output_dir': '../outputs'}\n"
     ]
    }
   ],
   "source": [
    "settings_json = json.load(open('../settings.json', 'r'))\n",
    "print(settings_json)\n",
    "\n",
    "for k,v in settings_json.items():\n",
    "    setattr(cfg, k, v)\n",
    "    \n",
    "print(cfg.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7b4ac57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': '__main__',\n",
       "              'samp_freq': 1,\n",
       "              'gaussian_overlay_len': 60,\n",
       "              'std_dev_num': 2400,\n",
       "              '__dict__': <attribute '__dict__' of 'cfg' objects>,\n",
       "              '__weakref__': <attribute '__weakref__' of 'cfg' objects>,\n",
       "              '__doc__': None,\n",
       "              'train_events_path': '../data/train_events.csv',\n",
       "              'train_series_path': '../data/train_series.parquet',\n",
       "              'processed_data_path': '../data_processed_models',\n",
       "              'output_dir': '../outputs'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "253ea1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data \n",
    "# Column transformations\n",
    "dt_transforms = [\n",
    "    pl.col('timestamp').str.strptime(pl.Datetime, format='%Y-%m-%dT%H:%M:%S%Z'), \n",
    "]\n",
    "\n",
    "data_transforms = [\n",
    "    pl.col('anglez').cast(pl.Int16), # Casting anglez to 16 bit integer\n",
    "    (pl.col('enmo')*1000).cast(pl.UInt16), # Convert enmo to 16 bit uint\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d6b71a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df, night_offset=20):\n",
    "    return (\n",
    "        df.with_columns(\n",
    "            [\n",
    "                pl.col(\"timestamp\")\n",
    "                .str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S%Z\")\n",
    "                .alias(\"timestamp\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                (pl.col(\"timestamp\").dt.year() - 2000).cast(pl.Int8).alias(\"year\"),\n",
    "                pl.col(\"timestamp\").dt.month().cast(pl.Int8).alias(\"month\"),\n",
    "                pl.col(\"timestamp\").dt.day().cast(pl.Int8).alias(\"day\"),\n",
    "                pl.col(\"timestamp\").dt.hour().cast(pl.Int8).alias(\"hour\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(  # 正午をまたいで日付を調整\n",
    "            pl.when(pl.col(\"hour\") < night_offset)\n",
    "            .then(pl.col(\"timestamp\"))\n",
    "            .otherwise(pl.col(\"timestamp\") + pl.duration(days=1))\n",
    "            .dt.date()\n",
    "            .alias(\"night_group\"),\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                (\n",
    "                    pl.col(\"series_id\")\n",
    "                    + pl.lit(\"_\")\n",
    "                    + pl.col(\"night_group\").cast(pl.Datetime).dt.strftime(\"%Y%m%d\")\n",
    "                ).alias(\"group_id\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"timestamp\").cumcount().over(\"group_id\").alias(\"norm_step\"),\n",
    "            ]\n",
    "        )\n",
    "        .drop([\"night_group\"])\n",
    "    )\n",
    "\n",
    "def transform_series(df):\n",
    "    return transform(df).with_columns(\n",
    "        [\n",
    "            (pl.col(\"enmo\") == 0).alias(\"is_enmo_clipped\"),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(\n",
    "    df,\n",
    "    group_col=\"series_id\",\n",
    "    day_group_col=\"group_id\",\n",
    "    term1=(5 * 60) // 5,\n",
    "    term2=(30 * 60) // 5,\n",
    "    term3=(60 * 60) // 5,\n",
    "    min_threshold=0.005,\n",
    "    max_threshold=0.04,\n",
    "    center=True,\n",
    "):\n",
    "    return (\n",
    "        df.with_columns(\n",
    "            [\n",
    "                pl.col(\"anglez\").diff(1).abs().over(group_col).alias(\"anglez_diff\"),\n",
    "                pl.col(\"enmo\").diff(1).abs().over(group_col).alias(\"enmo_diff\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"anglez_diff\")\n",
    "                .rolling_median(term1, center=center)  # 5 min window\n",
    "                .over(group_col)\n",
    "                .alias(\"anglez_diff_median_5min\"),\n",
    "                pl.col(\"enmo_diff\")\n",
    "                .rolling_median(term1, center=center)  # 5 min window\n",
    "                .over(group_col)\n",
    "                .alias(\"enmo_diff_median_5min\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"anglez_diff_median_5min\")\n",
    "                .quantile(0.1)\n",
    "                .clip(min_threshold, max_threshold)\n",
    "                .over(day_group_col)\n",
    "                .alias(\"critical_threshold\")\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                (pl.col(\"anglez_diff_median_5min\") < pl.col(\"critical_threshold\") * 15)\n",
    "                .over(group_col)\n",
    "                .alias(\"is_static\")\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"is_static\")\n",
    "                .cast(pl.Int32)\n",
    "                .rolling_sum(term2, center=center)\n",
    "                .over(group_col)\n",
    "                .alias(\"is_static_sum_30min\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [(pl.col(\"is_static_sum_30min\") == ((30 * 60) // 5)).over(group_col).alias(\"tmp\")]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"tmp\").shift(term2 // 2).over(group_col).alias(\"tmp_left\"),\n",
    "                pl.col(\"tmp\").shift(-(term2 // 2)).over(group_col).alias(\"tmp_right\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                (pl.col(\"tmp_left\") | pl.col(\"tmp_right\")).alias(\"is_sleep_block\"),\n",
    "            ]\n",
    "        )\n",
    "        .drop([\"tmp\", \"tmp_left\", \"tmp_right\"])\n",
    "        .with_columns([pl.col(\"is_sleep_block\").not_().alias(\"is_gap\")])\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"is_gap\")\n",
    "                .cast(pl.Int32)\n",
    "                .rolling_sum(term3, center=center)\n",
    "                .over(group_col)\n",
    "                .alias(\"gap_length\")\n",
    "            ]\n",
    "        )\n",
    "        .with_columns([(pl.col(\"gap_length\") == term3).over(group_col).alias(\"tmp\")])\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"tmp\").shift(term3 // 2).over(group_col).alias(\"tmp_left\"),\n",
    "                pl.col(\"tmp\").shift(-(term3 // 2)).over(group_col).alias(\"tmp_right\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                (pl.col(\"tmp_left\") | pl.col(\"tmp_right\")).alias(\"is_large_gap\"),\n",
    "            ]\n",
    "        )\n",
    "        .drop([\"tmp\", \"tmp_left\", \"tmp_right\"])\n",
    "        .with_columns([pl.col(\"is_large_gap\").not_().alias(\"is_sleep_episode\")])\n",
    "        #\n",
    "        # extract longest sleep episode\n",
    "        #\n",
    "        .with_columns(\n",
    "            [\n",
    "                # extract false->true transition\n",
    "                (\n",
    "                    (\n",
    "                        pl.col(\"is_sleep_episode\")\n",
    "                        & pl.col(\"is_sleep_episode\")\n",
    "                        .shift_and_fill(pl.lit(False), periods=1)\n",
    "                        .not_()\n",
    "                    )\n",
    "                    .cumsum()\n",
    "                    .over(\"group_id\")\n",
    "                ).alias(\"sleep_episode_id\")\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"is_sleep_episode\")\n",
    "                .sum()\n",
    "                .over([\"group_id\", \"sleep_episode_id\"])\n",
    "                .alias(\"sleep_episode_length\")\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"sleep_episode_length\")\n",
    "                .max()\n",
    "                .over([\"group_id\"])\n",
    "                .alias(\"max_sleep_episode_length\")\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                (\n",
    "                    pl.col(\"is_sleep_episode\")\n",
    "                    & (pl.col(\"sleep_episode_length\") == pl.col(\"max_sleep_episode_length\"))\n",
    "                ).alias(\"is_longest_sleep_episode\")\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4d47b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.39 s, sys: 16.1 s, total: 24.5 s\n",
      "Wall time: 5.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_events = pl.read_csv(cfg.train_events_path).with_columns(dt_transforms)\n",
    "train_series = pl.read_parquet(cfg.train_series_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3855/107349838.py:36: DeprecationWarning: `cumcount` is deprecated. It has been renamed to `cum_count`.\n",
      "  pl.col(\"timestamp\").cumcount().over(\"group_id\").alias(\"norm_step\"),\n",
      "/tmp/ipykernel_3855/2612547189.py:104: DeprecationWarning: `shift_and_fill` is deprecated. Use `shift` instead.\n",
      "  & pl.col(\"is_sleep_episode\")\n",
      "/tmp/ipykernel_3855/2612547189.py:104: DeprecationWarning: `the argument periods` for `shift_and_fill` is deprecated. It has been renamed to `n`.\n",
      "  & pl.col(\"is_sleep_episode\")\n",
      "/tmp/ipykernel_3855/2612547189.py:102: DeprecationWarning: `cumsum` is deprecated. It has been renamed to `cum_sum`.\n",
      "  (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 38s, sys: 1min 39s, total: 7min 18s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "train_series = transform_series(train_series)\n",
    "train_series = add_feature(train_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7d0fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_series = train_series[['series_id', 'step', 'timestamp', 'anglez', 'enmo',\n",
    "                             'is_longest_sleep_episode', 'is_sleep_episode', 'is_large_gap',\n",
    "                             'is_gap', 'is_sleep_block', 'is_static']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_series = train_series.with_columns([pl.col(c).fill_null(False) for c in train_series.columns if c != 'series_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "858124f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_series_ids = np.unique(train_events['series_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce671dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a total of 269 series\n",
      "269 269\n"
     ]
    }
   ],
   "source": [
    "use_series = train_events.drop_nulls()['series_id'].unique()\n",
    "print(f'Using a total of {len(use_series)} series')\n",
    "\n",
    "train_series = train_series.filter(pl.col('series_id').is_in(use_series))\n",
    "train_events = train_events.filter(pl.col('series_id').is_in(use_series))\n",
    "\n",
    "print(train_series['series_id'].n_unique(), train_events['series_id'].n_unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73c1a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_series = train_series.with_columns([pl.col('timestamp').dt.date().alias('date')])\n",
    "train_series = train_series.with_columns(pl.col('step').cast(pl.Int64))\n",
    "\n",
    "train_events = train_events.with_columns([pl.col('timestamp').dt.date().alias('date')])\n",
    "train_events = train_events.drop_nulls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01e12b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_events_pd = train_events.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64ae4987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fltr_series = train_events_pd[train_events_pd['event'] == 'onset'].groupby('series_id').size() == train_events_pd[train_events_pd['event'] == 'wakeup'].groupby('series_id').size()\n",
    "fltr_series.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b25717e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_series = fltr_series[fltr_series == True].index.tolist()\n",
    "len(use_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6439d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a total of 264 series\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264 264\n"
     ]
    }
   ],
   "source": [
    "print(f'Using a total of {len(use_series)} series')\n",
    "\n",
    "train_series = train_series.filter(pl.col('series_id').is_in(use_series))\n",
    "train_events = train_events.filter(pl.col('series_id').is_in(use_series))\n",
    "\n",
    "print(train_series['series_id'].n_unique(), train_events['series_id'].n_unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf440f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_nunique(arr, window=5):\n",
    "    n = len(arr)\n",
    "    nunique = np.zeros_like(arr)\n",
    "    if window > n:\n",
    "        return nunique\n",
    "    \n",
    "    # Initialize the dictionary with the first window\n",
    "    window_counts = {}\n",
    "    for i in arr[:window]:\n",
    "        window_counts[i] = window_counts.get(i, 0) + 1\n",
    "\n",
    "    # Set the unique count for the first window\n",
    "    nunique[window-1] = len(window_counts)\n",
    "\n",
    "    for i in range(window, n):\n",
    "        # Element leaving the window\n",
    "        leaving = arr[i - window]\n",
    "        window_counts[leaving] -= 1\n",
    "        if window_counts[leaving] == 0:\n",
    "            del window_counts[leaving]\n",
    "\n",
    "        # Element entering the window\n",
    "        entering = arr[i]\n",
    "        window_counts[entering] = window_counts.get(entering, 0) + 1\n",
    "\n",
    "        # Update the unique count\n",
    "        nunique[i] = len(window_counts)\n",
    "\n",
    "    return nunique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e371f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_eng(df, add_target=False):\n",
    "    \n",
    "    df = df.with_columns([pl.col('timestamp').dt.hour().alias('hour'),\n",
    "                                         (pl.col('timestamp').dt.hour() * 60 + pl.col('timestamp').dt.minute()).alias('hour_minute'),\n",
    "                                          (pl.col('timestamp').dt.hour()*60*60 + pl.col('timestamp').dt.minute()*60 + pl.col('timestamp').dt.second()).alias('hms'),\n",
    "                                          pl.col('timestamp').dt.weekday().alias('dayofweek'),\n",
    "                                          pl.col('enmo').diff().fill_null(0).alias('enmo_diff'),\n",
    "                                          pl.col('anglez').diff().abs().alias('anglez_diff').fill_null(0),\n",
    "                                          (pl.col('step') % (24*60*12)).alias('step_mod')]).with_columns([\\\n",
    "    ((pl.col('anglez_diff') > 5)*1.0).alias('large_anglez_diff'),\n",
    "     pl.col('enmo').mean().over(['hour']).alias('enmo_mean_hour'),\n",
    "    pl.col('enmo').std().over(['hour']).fill_null(0).alias('enmo_std_hour'),\n",
    "    pl.col('enmo').std().over(['hms']).fill_null(0).alias('anglez_std_stepmod'),\n",
    "    ((pl.col('enmo') == pl.col('enmo').median())*1).over(['hms']).alias('enmo_eq_median_stepmod'),\n",
    "    ((pl.col('enmo') == pl.col('enmo').mean())*1).over(['hms']).alias('enmo_eq_stepmod')\n",
    "    ]).with_columns([\n",
    "        pl.col('enmo_eq_stepmod').sum().over(['hms']).alias('enmo_eq_cnt_stepmod'),\n",
    "        pl.col('enmo_eq_stepmod').mean().over(['hms']).alias('enmo_eq_pct_stepmod')\n",
    "    ]).with_columns([\n",
    "        pl.col('enmo_eq_pct_stepmod').diff().fill_null(0).alias('enmo_eq_pct_stepmod_diff'),\n",
    "    ])\n",
    "    \n",
    "    windows = [12, 60]\n",
    "    \n",
    "    df = df.with_columns([pl.Series(rolling_nunique(df['anglez'].to_numpy().astype('int'), window=window)).alias(f'rolling_nunique_anglez_win_{window}')\\\n",
    "                            for window in windows])\n",
    "    \n",
    "    aggs_col_wise = {\n",
    "        'enmo': ['mean', 'std'],\n",
    "        'anglez_diff': ['max'],\n",
    "        'large_anglez_diff': ['max', 'mean']\n",
    "    }\n",
    "\n",
    "    df = df.with_columns([getattr(pl.col(col), f'rolling_{agg}')(window_size=win_size, min_periods=1, center=True)\\\n",
    "        .alias(f'{col}_rolling_{agg}_win_{win_size}')\\\n",
    "        for col in aggs_col_wise.keys() for agg in aggs_col_wise[col] for win_size in windows])\n",
    "    \n",
    "    ### Rolling range by calculating max - min\n",
    "    \n",
    "    df = df.with_columns([(pl.col('anglez').rolling_max(window_size=win_size, min_periods=1, center=True) -\\\n",
    "        pl.col('anglez').rolling_min(window_size=win_size, min_periods=1, center=True)).fill_null(0).alias(f'anglez_rolling_range_win_{win_size}')\\\n",
    "            for win_size in windows])\n",
    "\n",
    "    df = df.sort(by='step', descending=False)\n",
    "\n",
    "    if add_target == True:\n",
    "        df = df.join(train_events[['series_id', 'event', 'step']], on=['series_id', 'step'], how='left')\n",
    "        df = df.with_columns(pl.col('event').replace({'onset': 1, 'wakeup': 0}, default=None).fill_null(0).alias('is_onset'))\n",
    "        df = df.with_columns(pl.col('event').replace({'onset': 0, 'wakeup': 1}, default=None).fill_null(0).alias('is_wakeup'))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b64f9ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264/264 [01:06<00:00,  3.94it/s]\n"
     ]
    }
   ],
   "source": [
    "partitions = train_series.partition_by(by='series_id', maintain_order=True)\n",
    "\n",
    "def process_partition(df):\n",
    "    df = feat_eng(df, add_target=True)\n",
    "    return df\n",
    "\n",
    "# Parallel execution\n",
    "results = Parallel(n_jobs=64)(delayed(process_partition)(df) for df in tqdm(partitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3974c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_series = pl.concat(results)\n",
    "del partitions, results\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f94b3c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_cols = ['enmo', 'hour', 'anglez', 'dayofweek', 'enmo_diff', 'anglez_diff',\n",
    "             'anglez_std_stepmod', 'enmo_eq_stepmod',  'enmo_eq_pct_stepmod','enmo_eq_pct_stepmod_diff', 'large_anglez_diff',\n",
    "             'enmo_eq_median_stepmod',\n",
    "             'is_longest_sleep_episode', 'is_sleep_episode', 'is_large_gap', 'is_gap', 'is_sleep_block', 'is_static'\n",
    "            ] + [c for c in train_series.columns if 'rolling' in c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61313a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_added_cols = ['is_longest_sleep_episode', 'is_sleep_episode', 'is_large_gap', 'is_gap', 'is_sleep_block', 'is_static']\n",
    "\n",
    "train_series = train_series.with_columns([(pl.col(c)*1).alias(c) for c in is_added_cols])\n",
    "train_series = train_series.drop(['event_dup', 'timestamp', 'date', 'event'])\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "710a693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_arr_1d(arr, samp_freq=12, samp_method='mean'):\n",
    "    \n",
    "    if len(arr) % samp_freq != 0:\n",
    "        n_rem = samp_freq - len(arr) % samp_freq\n",
    "        arr = np.concatenate([arr, np.zeros(n_rem)])\n",
    "    \n",
    "    arr_samp = arr.reshape(-1, samp_freq)\n",
    "    \n",
    "    if samp_method == 'last':\n",
    "        arr_samp = arr_samp[:, -1]\n",
    "    elif samp_method in ['argmax', 'argmin']:\n",
    "        arr_samp =getattr(arr_samp, samp_method)(axis=1)/samp_freq\n",
    "    \n",
    "    elif samp_method == 'first':\n",
    "        arr_samp = arr_samp[:, 0]\n",
    "        \n",
    "    elif samp_method == 'aux':\n",
    "        argmax_idx = np.argmax(arr_samp, axis=1)\n",
    "        return np.where(argmax_idx > samp_freq//2, 0, 1)\n",
    "        \n",
    "    else:\n",
    "        arr_samp = getattr(arr_samp, samp_method)(axis=1)\n",
    "\n",
    "    return arr_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f48bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(position, length, sigma):\n",
    "    \"\"\"Generate a Gaussian curve centered at 'position' with a given 'sigma'.\"\"\"\n",
    "    x = np.arange(0, length)\n",
    "    return 1 / (sigma * np.sqrt(2*np.pi)) * np.exp(-(x - position)**2 / (2*sigma**2))\n",
    "\n",
    "def get_y_gaussian(series_df):\n",
    "    \n",
    "    onset_steps = series_df.filter(pl.col('is_onset') == 1)['step'].to_numpy()\n",
    "    wakeup_steps = series_df.filter(pl.col('is_wakeup') == 1)['step'].to_numpy()\n",
    "\n",
    "    assert len(onset_steps) == len(wakeup_steps)\n",
    "\n",
    "    y_sub = np.zeros((len(series_df), 2))\n",
    "\n",
    "    for onset_step, wakeup_step in zip(onset_steps, wakeup_steps):\n",
    "\n",
    "        ### Onsets\n",
    "        s_onset, e_onset = max(0, onset_step-cfg.gaussian_overlay_len//2), onset_step+cfg.gaussian_overlay_len//2+1\n",
    "\n",
    "        y_sub[s_onset: e_onset, 0] = gaussian(position=cfg.gaussian_overlay_len//2,\n",
    "                                              length=cfg.gaussian_overlay_len+1,\n",
    "                                              sigma=cfg.std_dev_num/cfg.samp_freq)[s_onset-(onset_step-cfg.gaussian_overlay_len//2):]\n",
    "\n",
    "        ### Wakeups\n",
    "        s_wakeup, e_wakeup = wakeup_step-cfg.gaussian_overlay_len//2, min(len(y_sub), wakeup_step+cfg.gaussian_overlay_len//2+1)\n",
    "        \n",
    "        \n",
    "        y_sub[s_wakeup: e_wakeup:, 1] = gaussian(position=cfg.gaussian_overlay_len//2,\n",
    "                                                 length=cfg.gaussian_overlay_len+1,\n",
    "                                                 sigma=cfg.std_dev_num/cfg.samp_freq)[:cfg.gaussian_overlay_len+1-((wakeup_step+cfg.gaussian_overlay_len//2+1)-e_wakeup)]\n",
    "        \n",
    "    return y_sub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "536775d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/264 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264/264 [00:47<00:00,  5.52it/s]\n"
     ]
    }
   ],
   "source": [
    "def partition_series_data(partitions, ft_cols, cfg, target_cols=None, norm_agg_dict=None):\n",
    "    \n",
    "    # If target_cols is not provided or is empty, skip y_s related computation\n",
    "    compute_y_s = target_cols is not None and len(target_cols) > 0\n",
    "\n",
    "    X_s, series_ids = [], []\n",
    "    if compute_y_s:\n",
    "        y_s = []\n",
    "        \n",
    "    for ser_id in tqdm(partitions.keys()):\n",
    "        \n",
    "        df_ser_sub = partitions[ser_id]\n",
    "\n",
    "        X_sub = df_ser_sub[ft_cols].to_numpy()\n",
    "        X_sub_samp = np.concatenate([np.array([resample_arr_1d(X_sub[:, i], cfg.samp_freq, agg) for agg in norm_agg_dict[ft_cols[i]]]).T for i in range(X_sub.shape[1])], axis=1).astype('float32')\n",
    "        \n",
    "        X_s.append(X_sub_samp)\n",
    "        series_ids.append(ser_id)\n",
    "        \n",
    "        if compute_y_s:\n",
    "            y_sub = get_y_gaussian(df_ser_sub)\n",
    "            y_sub_samp = np.array([resample_arr_1d(y_sub[:, i], cfg.samp_freq, 'max') for i in range(len(target_cols))]).T.astype('float32')\n",
    "            y_s.append(y_sub_samp)\n",
    "            \n",
    "    \n",
    "    if compute_y_s:\n",
    "        return X_s, y_s, series_ids\n",
    "    else:\n",
    "        return X_s, series_ids\n",
    "    \n",
    "norm_agg_dict = {c: ['mean'] for c in norm_cols}\n",
    "\n",
    "partitions = train_series.partition_by(by='series_id', maintain_order=True, as_dict=True)\n",
    "del train_series\n",
    "_ = gc.collect()\n",
    "\n",
    "X_s, y_s, series_ids = partition_series_data(partitions, norm_cols, cfg, target_cols=['is_onset', 'is_wakeup'], norm_agg_dict=norm_agg_dict)\n",
    "del partitions\n",
    "_ = gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a28c7070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(X_s, y_s, series_ids, directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    for X, y, series_id in tqdm(zip(X_s, y_s, series_ids), total=len(X_s)):\n",
    "        np.save(os.path.join(directory, f'{series_id}_X.npy'), X)\n",
    "        np.save(os.path.join(directory, f'{series_id}_y.npy'), y)\n",
    "\n",
    "def load_data(directory):\n",
    "    X_s = []\n",
    "    y_s = []\n",
    "    series_ids = []\n",
    "\n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        if filename.endswith('_X.npy'):\n",
    "            series_id = filename.split('_X.npy')[0]\n",
    "            X = np.load(os.path.join(directory, filename))\n",
    "            y = np.load(os.path.join(directory, f'{series_id}_y.npy'))\n",
    "\n",
    "            X_s.append(X)\n",
    "            y_s.append(y)\n",
    "            series_ids.append(series_id)\n",
    "\n",
    "    return X_s, y_s, series_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41032974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_y(y):\n",
    "    \n",
    "    for i in range(y.shape[1]):\n",
    "        \n",
    "        mean = y[:,i].mean()\n",
    "        std = y[:,i].std()\n",
    "        y[:,i] = (y[:,i]-mean)/(std+1e-16)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8d3840d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264/264 [00:11<00:00, 23.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create directory if doesn't exist\n",
    "\n",
    "if not os.path.exists(cfg.processed_data_path):\n",
    "    os.makedirs(cfg.processed_data_path)\n",
    "\n",
    "save_data(X_s, y_s, series_ids, cfg.processed_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c089eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_events.write_ipc(os.path.join(cfg.processed_data_path, 'train_events.ipc'), compression='zstd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb48d8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((389880, 32), (389880, 2), 264)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_s[0].shape, y_s[0].shape, len(series_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3f3bd6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['enmo',\n",
       " 'hour',\n",
       " 'anglez',\n",
       " 'dayofweek',\n",
       " 'enmo_diff',\n",
       " 'anglez_diff',\n",
       " 'anglez_std_stepmod',\n",
       " 'enmo_eq_stepmod',\n",
       " 'enmo_eq_pct_stepmod',\n",
       " 'enmo_eq_pct_stepmod_diff',\n",
       " 'large_anglez_diff',\n",
       " 'enmo_eq_median_stepmod',\n",
       " 'is_longest_sleep_episode',\n",
       " 'is_sleep_episode',\n",
       " 'is_large_gap',\n",
       " 'is_gap',\n",
       " 'is_sleep_block',\n",
       " 'is_static',\n",
       " 'rolling_nunique_anglez_win_12',\n",
       " 'rolling_nunique_anglez_win_60',\n",
       " 'enmo_rolling_mean_win_12',\n",
       " 'enmo_rolling_mean_win_60',\n",
       " 'enmo_rolling_std_win_12',\n",
       " 'enmo_rolling_std_win_60',\n",
       " 'anglez_diff_rolling_max_win_12',\n",
       " 'anglez_diff_rolling_max_win_60',\n",
       " 'large_anglez_diff_rolling_max_win_12',\n",
       " 'large_anglez_diff_rolling_max_win_60',\n",
       " 'large_anglez_diff_rolling_mean_win_12',\n",
       " 'large_anglez_diff_rolling_mean_win_60',\n",
       " 'anglez_rolling_range_win_12',\n",
       " 'anglez_rolling_range_win_60']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9014c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
