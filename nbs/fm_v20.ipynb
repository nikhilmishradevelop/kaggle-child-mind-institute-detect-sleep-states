{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b4ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The line `# !pip install --upgrade -q numpy numba polars lightgbm tensorflow-addons` is a command that installs or upgrades several Python packages.\n",
    "# !pip install --upgrade -q numpy numba polars lightgbm tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b94e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 11:20:40.721750: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'src.metric_fast' from '/home/sleep-kaggle/kaggle_final_solution/../src/metric_fast.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src import metric_fast\n",
    "import joblib\n",
    "import time\n",
    "import importlib\n",
    "importlib.reload(metric_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    samp_freq=1\n",
    "    gaussian_overlay_len = 60\n",
    "    std_dev_num = 2400\n",
    "    ver='fm-v20-final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e70e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_json = json.load(open('../settings.json', 'r'))\n",
    "print(settings_json)\n",
    "\n",
    "for k,v in settings_json.items():\n",
    "    setattr(cfg, k, v)\n",
    "    \n",
    "print(cfg.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd65f06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_events = pl.read_ipc(os.path.join(cfg.processed_data_path, 'train_events.ipc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_df = train_events[['series_id']].unique(maintain_order=True)\n",
    "splits_df = splits_df.to_pandas()\n",
    "\n",
    "for n_splits in [5, 7, 10]:\n",
    "    folds = KFold(n_splits, shuffle=True, random_state=55125)\n",
    "\n",
    "    splits_df[f'{n_splits}_fold'] = 0\n",
    "    for i, (trn_idx, val_idx) in enumerate(folds.split(splits_df['series_id'], splits_df['series_id'])):\n",
    "        \n",
    "        splits_df.loc[val_idx, f'{n_splits}_fold'] = i+1\n",
    "                                       \n",
    "    \n",
    "splits_df = pl.DataFrame(splits_df)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85190e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory):\n",
    "    X_s = []\n",
    "    y_s = []\n",
    "    series_ids = []\n",
    "\n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        if filename.endswith('_X.npy'):\n",
    "            series_id = filename.split('_X.npy')[0]\n",
    "            X = np.load(os.path.join(directory, filename))\n",
    "            y = np.load(os.path.join(directory, f'{series_id}_y.npy'))\n",
    "\n",
    "            X_s.append(X)\n",
    "            y_s.append(y)\n",
    "            series_ids.append(series_id)\n",
    "\n",
    "    return X_s, y_s, series_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ad2da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_y(y):\n",
    "    \n",
    "    for i in range(y.shape[1]):\n",
    "        \n",
    "        mean = y[:,i].mean()\n",
    "        std = y[:,i].std()\n",
    "        y[:,i] = (y[:,i]-mean)/(std+1e-16)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c515709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_pad(X, y=None, split_length=1440, stride=1440):\n",
    "    \"\"\"\n",
    "    Splits and pads the arrays X and y using a sliding window.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.array): Array with shape (timesteps, features)\n",
    "    - y (np.array): Array with shape (timesteps, 2)\n",
    "    - split_length (int): Desired timestep length for the resulting arrays.\n",
    "    - stride (int): Step size for sliding window.\n",
    "\n",
    "    Returns:\n",
    "    - List of arrays for X and y, each with shape (split_length, features)\n",
    "    \"\"\"\n",
    "    \n",
    "    if y is not None and len(X) != len(y):\n",
    "        raise ValueError(\"X and y should have the same number of timesteps.\")\n",
    "    \n",
    "    timesteps, features = X.shape\n",
    "    \n",
    "    # Create empty lists to store split segments\n",
    "    X_splits = []\n",
    "    y_splits = []\n",
    "    starts = []\n",
    "    \n",
    "    # Use sliding window to extract segments\n",
    "    \n",
    "    for start in range(0, timesteps, stride):\n",
    "        end = start + split_length\n",
    "        if end <= timesteps:\n",
    "            starts.append(start)\n",
    "            X_splits.append(X[start:end].copy())\n",
    "            if y is not None:\n",
    "                y_splits.append(y[start:end].copy())\n",
    "        else:\n",
    "            # If the segment is shorter than split_length, pad it\n",
    "            starts.append(start)\n",
    "            padding_length = end - timesteps\n",
    "            X_segment_padded = np.pad(X[start:], ((0, padding_length), (0, 0)), mode='constant', constant_values=-9)\n",
    "            X_splits.append(X_segment_padded)\n",
    "            \n",
    "            if y is not None:\n",
    "                y_segment_padded = np.pad(y[start:], ((0, padding_length), (0, 0)), mode='constant', constant_values=0)\n",
    "                y_splits.append(y_segment_padded)\n",
    "                \n",
    "            break\n",
    "            \n",
    "            \n",
    "    if y is not None:\n",
    "        return X_splits, y_splits, starts\n",
    "    \n",
    "    return X_splits, starts\n",
    "\n",
    "\n",
    "\n",
    "class SleepDataset:\n",
    "    \n",
    "    def __init__(self, X_s, y_s=None, series_ids=None, samp_freq=None, remove_no_dets=True, is_train=False, split_factor=1, norm_params=None):\n",
    "        \n",
    "        self.split_len = (24*60*12) // cfg.samp_freq\n",
    "        self.split_strides = self.split_len\n",
    "        self.remove_no_dets = remove_no_dets\n",
    "        self.is_train = is_train\n",
    "\n",
    "        print(f'Using a split len of {self.split_len}')\n",
    "        self.create_dataset(X_s, y_s, series_ids)\n",
    "\n",
    "        if self.is_train:\n",
    "            self.norm_params = self.calculate_norm_params()\n",
    "        else:\n",
    "            if norm_params is None:\n",
    "                raise ValueError(\"Normalization parameters must be provided for non-training data.\")\n",
    "            self.norm_params = norm_params\n",
    "\n",
    "        self.normalize_data()\n",
    "\n",
    "    def calculate_norm_params(self):\n",
    "        mean = np.mean(self.X, axis=(0, 1))\n",
    "        std = np.std(self.X, axis=(0, 1))\n",
    "        return {'mean': mean, 'std': std}\n",
    "\n",
    "    def normalize_data(self):\n",
    "        self.X = (self.X - self.norm_params['mean']) / (1e-6 + self.norm_params['std'])\n",
    "        \n",
    "        \n",
    "    def create_dataset(self, X_s, y_s=None, series_ids=None):\n",
    "        X_s_splits, y_s_splits, series_splits, starts_splits = [], [], [], []\n",
    "\n",
    "        for i in tqdm(range(len(X_s))):\n",
    "            x_splits, starts = split_and_pad(X_s[i].copy(), split_length = self.split_len, stride=self.split_strides)\n",
    "            X_s_splits.extend(x_splits)\n",
    "            starts_splits.extend(starts)\n",
    "\n",
    "            if y_s is not None:\n",
    "                _, y_splits, _ = split_and_pad(X_s[i].copy(), y_s[i].copy(), split_length=self.split_len, stride=self.split_strides)\n",
    "                y_s_splits.extend(y_splits)\n",
    "\n",
    "            if series_ids is not None:\n",
    "                series_splits.extend([series_ids[i] for _ in range(len(x_splits))])\n",
    "            \n",
    "        self.X = np.array(X_s_splits)\n",
    "        self.starts_splits = np.array(starts_splits)\n",
    "        \n",
    "        if y_s is not None:\n",
    "            self.y = np.array(y_s_splits)\n",
    "\n",
    "            if self.remove_no_dets:\n",
    "                fltr = (self.y[:, :, 1].sum(axis=1) + self.y[:, :, 0].sum(axis=1)) != 0\n",
    "                self.X = self.X[fltr]\n",
    "                self.y = self.y[fltr]\n",
    "                if series_ids is not None:\n",
    "                    self.series_ids = np.array(series_splits)[fltr]\n",
    "                else:\n",
    "                    self.series_ids = None\n",
    "            \n",
    "            self.y = np.array([normalize_y(yts) for yts in self.y])\n",
    "\n",
    "        else:\n",
    "            self.y = None\n",
    "\n",
    "        if series_ids is not None:\n",
    "            if not hasattr(self, 'series_ids'):\n",
    "                self.series_ids = np.array(series_splits)\n",
    "            print(f'X: {self.X.shape}, y: {self.y.shape if self.y is not None else \"Not provided\"}, series_ids: {self.series_ids.shape}')\n",
    "        else:\n",
    "            print(f'X: {self.X.shape}, y: {self.y.shape if self.y is not None else \"Not provided\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20edbd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_nikhil(preds_orig, k_orig=125, max_thresh=0.05, max_count=1000):\n",
    "    \n",
    "    preds=preds_orig.copy()\n",
    "\n",
    "    count = 0\n",
    "    base=6.75\n",
    "    \n",
    "    k = k_orig\n",
    "    prev_max = None\n",
    "\n",
    "    scores = []\n",
    "    indices = []\n",
    "    \n",
    "    while True:\n",
    "            \n",
    "        curr_max_idx = np.argmax(preds)\n",
    "        curr_max = preds[curr_max_idx] \n",
    "        \n",
    "        if (curr_max < max_thresh) or count > max_count:\n",
    "            break\n",
    "        \n",
    "        indices.append(curr_max_idx)\n",
    "        scores.append(curr_max)\n",
    "        \n",
    "        preds[curr_max_idx] = 0\n",
    "        \n",
    "        supress_rates = np.logspace(start=0, stop=1, num=k, base=base)/base\n",
    "\n",
    "        preds[max(curr_max_idx-k, 0):curr_max_idx] *= supress_rates[:min(k, curr_max_idx-0)][::-1]\n",
    "        preds[curr_max_idx+1:min(curr_max_idx+k+1, preds.shape[0])] *= supress_rates[:min(k, preds.shape[0]-curr_max_idx-1)]\n",
    "\n",
    "        prev_max = curr_max\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    return indices, scores\n",
    "\n",
    "def get_actual_preds(val_preds, val_series_ids, val_steps, type_):\n",
    "    times = []\n",
    "    series_ids = []\n",
    "    scores = []\n",
    "    scores_y = []\n",
    "    \n",
    "    for i in np.arange(len(val_preds)):\n",
    "        \n",
    "        vp_i = val_preds[i]\n",
    "        ser_id = val_series_ids[i]\n",
    "        \n",
    "        col_index = 0 if type_ == \"onset\" else 1\n",
    "        other_col_index = 1 if type_ == \"onset\" else 0\n",
    "        \n",
    "        preds = vp_i[:, col_index]\n",
    "        preds_other = vp_i[:, other_col_index]\n",
    "\n",
    "        height_thresh = 0.05\n",
    "        \n",
    "        peaks, peak_scores = nms_nikhil(preds)\n",
    "\n",
    "        times.extend(val_steps[i][peaks])\n",
    "        scores.extend(list(peak_scores))\n",
    "        series_ids.extend([ser_id] * len(peaks))\n",
    "\n",
    "    return np.array(series_ids), np.array(times), np.array(scores)\n",
    "\n",
    "\n",
    "def post_process_preds(val_events_df, val_preds, val_series_ids, val_starts_splits, samp_freq, get_score=False):\n",
    "    \n",
    "#     val_steps = np.concatenate([val_starts_splits[idx] + np.arange(len(val_preds[idx])) for idx in range(val_preds.shape[0])])\n",
    "\n",
    "    val_res = []\n",
    "    \n",
    "    prev_ser_id = None\n",
    "    \n",
    "    res_steps = []\n",
    "    res_preds = []\n",
    "    res_series_ids = []\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(val_preds):\n",
    "        \n",
    "        end = start+1\n",
    "        while end < len(val_preds) and val_series_ids[end] == val_series_ids[start]:\n",
    "            end += 1\n",
    "            \n",
    "        preds = val_preds[start:end]\n",
    "        \n",
    "        steps = np.concatenate([val_starts_splits[idx] + np.arange(len(val_preds[idx])) for idx in range(start, end)])\n",
    "        preds = preds.reshape((preds.shape[0]*preds.shape[1]), 2)\n",
    "        \n",
    "        res_preds.append(preds)\n",
    "        res_steps.append(steps)\n",
    "        res_series_ids.append(val_series_ids[start])\n",
    "        \n",
    "        start=end\n",
    "        \n",
    "        \n",
    "#     print(len(res_series_ids), len(res_steps), len(res_preds))\n",
    "\n",
    "    series_ids_onsets, onsets,  scores_onsets = get_actual_preds(res_preds, res_series_ids, res_steps, 'onset')\n",
    "    series_ids_wakeups, wakeups,  scores_wakeups =get_actual_preds(res_preds, res_series_ids, res_steps, 'wakeup')\n",
    "    \n",
    "    \n",
    "    onset_preds = pl.DataFrame().with_columns([pl.Series(series_ids_onsets).alias('series_id'),\n",
    "                                           pl.Series(onsets).cast(pl.Int64).alias('step'),\n",
    "                                           pl.lit('onset').alias('event'),\n",
    "                                           pl.Series(scores_onsets).alias('score')])\n",
    "\n",
    "    wakeup_preds = pl.DataFrame().with_columns([pl.Series(series_ids_wakeups).alias('series_id'),\n",
    "                                               pl.Series(wakeups).cast(pl.Int64).cast(pl.Int64).alias('step'),\n",
    "                                               pl.lit('wakeup').alias('event'),\n",
    "                                               pl.Series(scores_wakeups).alias('score')])\n",
    "    \n",
    "    val_preds_df = pl.concat([onset_preds, wakeup_preds]).sort(by=['series_id', 'step'])\n",
    "    \n",
    "    if get_score:\n",
    "        toleranaces = {'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]}\n",
    "        comp_score = metric_fast.comp_scorer(\n",
    "        val_events_df,\n",
    "        val_preds_df,\n",
    "        tolerances = toleranaces,\n",
    "        )\n",
    "        return comp_score\n",
    "    \n",
    "    else:\n",
    "        return val_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a630548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_exponential_lr(start_lr, end_lr, num_steps, decay_rate=None):\n",
    "    \"\"\"\n",
    "    Calculate the exponentially decreasing learning rates.\n",
    "\n",
    "    Parameters:\n",
    "    start_lr (float): Initial learning rate.\n",
    "    end_lr (float): Final learning rate.\n",
    "    num_steps (int): Total number of steps over which the learning rate should decay.\n",
    "    decay_rate (float): Decay rate per step. If None, it will be computed based on start_lr, end_lr, and num_steps.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing the learning rate for each step.\n",
    "    \"\"\"\n",
    "    if decay_rate is None:\n",
    "        # Calculate decay rate based on the start_lr, end_lr, and num_steps\n",
    "        decay_rate = (end_lr / start_lr) ** (1 / (num_steps - 1))\n",
    "\n",
    "    learning_rates = [start_lr * (decay_rate ** step) for step in range(num_steps)]\n",
    "    return learning_rates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cf0fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntervalEvaluation(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_ds, val_events_df, samp_freq, n_steps, start_lr, end_lr):\n",
    "\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.val_ds = val_ds\n",
    "        self.val_events_df = val_events_df\n",
    "        self.samp_freq = samp_freq\n",
    "        \n",
    "        warmup_pct_steps = 0.25\n",
    "        warmup_steps = int(n_steps * warmup_pct_steps)\n",
    "        self.learning_rates = [start_lr] * (warmup_steps) + calculate_exponential_lr(start_lr, end_lr, n_steps-warmup_steps)\n",
    "        self.best_score = -np.inf\n",
    "        self.best_model = None\n",
    "        self.step_count=0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        if epoch == 0:\n",
    "            self.first_epoch_start_time = self.start_time\n",
    "        \n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        if self.step_count < len(self.learning_rates):\n",
    "\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, self.learning_rates[self.step_count])\n",
    "            self.curr_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        self.step_count += 1\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        val_preds = self.model.predict(self.val_ds.X, batch_size=8, verbose=0)[:, :, :2]\n",
    "        val_score = post_process_preds(self.val_events_df, val_preds, self.val_ds.series_ids, self.val_ds.starts_splits, self.samp_freq, get_score=True)\n",
    "        \n",
    "        if val_score > self.best_score:\n",
    "            self.best_score = val_score\n",
    "            self.best_model = tf.keras.models.clone_model(self.model)\n",
    "            self.best_model.set_weights(self.model.get_weights()) \n",
    "        \n",
    "        total_time = round(time.time() - self.start_time, 2)\n",
    "        total_seconds_till_now = round(time.time() - self.first_epoch_start_time, 0)\n",
    "        \n",
    "        print(f\"Epoch: {epoch:03d} curr_lr: {self.curr_lr:.1e} - train_loss: {logs['loss']:.04f} - val_loss: {logs['val_loss']:.04f} val_score: {val_score:.03f}  best_val_score: {self.best_score:.03f}  last_epoch t={total_time:.02f}s, total_time_elapsed t={total_seconds_till_now}s\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaf8b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "\n",
    "    # MultiHead Attention\n",
    "    x = tfa.layers.MultiHeadAttention(\n",
    "        head_size=head_size,\n",
    "        num_heads=num_heads,\n",
    "        use_projection_bias=True,\n",
    "        dropout=dropout\n",
    "    )([inputs, inputs, inputs])\n",
    "\n",
    "    # Residual connection with LayerNormalization and Scaling\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + inputs) * (0.5 ** 0.5)\n",
    "    \n",
    "    # Feed Forward Part\n",
    "    ff = tf.keras.layers.Dense(ff_dim, activation='relu')(x)\n",
    "    # ff = tf.keras.layers.Dropout(dropout)(ff)\n",
    "    ff = tf.keras.layers.Dense(inputs.shape[-1])(ff)\n",
    "    # ff = tf.keras.layers.Dropout(dropout)(ff)\n",
    "    \n",
    "    # Residual connection with LayerNormalization and Scaling for FFN\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + ff) * (0.5 ** 0.5)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10d34cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mce_loss(y_true, y_pred):\n",
    "    # Clip the ground truth and predictions to the range (-100, 100)\n",
    "    y_true_clipped = tf.clip_by_value(y_true, -100, 100)\n",
    "    y_pred_clipped = tf.clip_by_value(y_pred, -100, 100)\n",
    "\n",
    "    # Calculate the mean cubed error\n",
    "    loss = tf.reduce_mean(tf.abs(y_true_clipped - y_pred_clipped) ** 3)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 11:20:43.079098: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 11:20:43.199956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46413 MB memory:  -> device: 0, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:23:00.0, compute capability: 8.9\n",
      "/usr/local/lib/python3.8/dist-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36630024"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conv_block(inputs, num_filters):\n",
    "    x = tf.keras.layers.Conv1D(num_filters, 8, padding=\"same\")(inputs)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.Conv1D(num_filters, 8, padding=\"same\")(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def encoder_block(inputs, num_filters):\n",
    "    x = conv_block(inputs, num_filters)\n",
    "    p = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(inputs, concat_tensors, num_filters):\n",
    "    x = tf.keras.layers.Conv1DTranspose(num_filters, 4, strides=2, padding=\"same\")(inputs)\n",
    "    i = len(concat_tensors)-1\n",
    "    for concat_tensor in concat_tensors:\n",
    "        concat_tensor_max = tf.keras.layers.MaxPool1D(pool_size=2**i)(concat_tensor)\n",
    "        x = tf.keras.layers.Concatenate()([x, concat_tensor_max])\n",
    "        i -= 1\n",
    "        \n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def get_model(input_shape, num_blocks=4):\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    orig_n_channels = x.shape[-1]\n",
    "    \n",
    "    x_t = []\n",
    "    n_split_fact = 4\n",
    "    \n",
    "    \n",
    "    for i in range(x.shape[-1]):\n",
    "        x_sp = tf.reshape(x[:, :, i], (-1, x.shape[1]//n_split_fact, n_split_fact))\n",
    "        \n",
    "        if i < orig_n_channels:\n",
    "        \n",
    "            # Calculating mean, max, and standard deviation\n",
    "            mean = tf.reduce_mean(x_sp, axis=-1, keepdims=True)\n",
    "            max_val = tf.reduce_max(x_sp, axis=-1, keepdims=True)\n",
    "            std_dev = tf.math.reduce_std(x_sp, axis=-1, keepdims=True)\n",
    "            min_val = tf.reduce_min(x_sp, axis=-1, keepdims=True)\n",
    "\n",
    "            x_sp = tf.keras.layers.Concatenate()([x_sp, mean, max_val, std_dev, min_val])\n",
    "        \n",
    "        x_sp = tf.keras.layers.Dense(n_split_fact*16, activation='relu')(x_sp)\n",
    "        \n",
    "        x_t.append(x_sp)\n",
    "        \n",
    "    \n",
    "    x_c1d = tf.keras.layers.Conv1D(64, kernel_size=12, strides=n_split_fact, padding=\"same\")(x)\n",
    "    x_c1d = tf.keras.layers.ReLU()(x_c1d)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()(x_t + [x_c1d])# tf.reshape(x, (-1, -1, 3))\n",
    "    \n",
    "    # Initial filter size\n",
    "    fsz = 64\n",
    "    \n",
    "    # Lists to hold the encoder and pooling outputs\n",
    "    encoder_outputs = []\n",
    "    pooling_outputs = []\n",
    "    \n",
    "    # Encoder\n",
    "    for i in range(num_blocks):\n",
    "        if i == 0:\n",
    "            # First block receives the model input\n",
    "            enc_out, pool_out = encoder_block(x, fsz * (2 ** i))\n",
    "        else:\n",
    "            # Subsequent blocks receive the pooling output of the previous block\n",
    "            enc_out, pool_out = encoder_block(pooling_outputs[-1], fsz * (2 ** i))\n",
    "        \n",
    "        if i == 0:\n",
    "            pool_out_copy = pool_out\n",
    "            encoder_out_copy = enc_out\n",
    "            \n",
    "        else:\n",
    "            pool_out_mp = tf.keras.layers.MaxPooling1D(pool_size=2**i)(pool_out_copy)\n",
    "            encoder_out_mp = tf.keras.layers.MaxPooling1D(pool_size=2**i)(encoder_out_copy)\n",
    "            \n",
    "            pool_out = tf.keras.layers.Concatenate()([pool_out, pool_out_mp])\n",
    "            enc_out = tf.keras.layers.Concatenate()([enc_out, encoder_out_mp])\n",
    "        \n",
    "        \n",
    "        encoder_outputs.append(enc_out)\n",
    "        pooling_outputs.append(pool_out)\n",
    "    \n",
    "    # Bottleneck\n",
    "    bottleneck = conv_block(pooling_outputs[-1], fsz * (2 ** num_blocks))\n",
    "    \n",
    "    \n",
    "    def gru_conv(x):\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(x.shape[-1]//4, return_sequences=True))(x)\n",
    "        for i in range(1):\n",
    "            x_conv = tf.keras.layers.Conv1D(x.shape[-1], kernel_size=(4,), padding='same', activation='relu')(x)\n",
    "            x = tf.keras.layers.Dense(x.shape[-1], activation='relu')(x_conv)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    for _ in range(1):\n",
    "        bottleneck = transformer_encoder(bottleneck, head_size=16, num_heads=8, ff_dim=256, dropout=0)\n",
    "\n",
    "    bottleneck = gru_conv(bottleneck)\n",
    "\n",
    "    decoder_input = bottleneck\n",
    "    for i in range(num_blocks - 1, -1, -1):\n",
    "        decoder_output = decoder_block(decoder_input, encoder_outputs[:i+1], fsz * (2 ** i))\n",
    "        decoder_input = decoder_output\n",
    "    \n",
    "    # Output Layer\n",
    "    x = tf.keras.layers.Conv1D(2*n_split_fact, 1, padding=\"same\", activation=\"linear\")(decoder_output)\n",
    "    x = tf.reshape(x, shape=(-1, x.shape[1]*n_split_fact, 2))\n",
    "    \n",
    "    outputs = x\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "    model.compile(optimizer=tfa.optimizers.AdamW(weight_decay=1e-4), loss=custom_mce_loss)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Usage:\n",
    "input_shape = (17280, 32)  # replace with your input shape\n",
    "num_blocks = 4  # specify the number of encoder/decoder blocks\n",
    "model = get_model(input_shape, num_blocks)\n",
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e61eaf0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 17280, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be712eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_cfg:\n",
    "    n_epochs = 14\n",
    "    batch_size = 8\n",
    "    start_lr = 6e-5\n",
    "    end_lr = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:04<00:00, 119.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 1-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 237 series for training and 27 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 237/237 [00:14<00:00, 16.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6508, 17280, 32), y: (6508, 17280, 2), series_ids: (6508,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 67.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (696, 17280, 32), y: (696, 17280, 2), series_ids: (696,)\n",
      "Total model parameters: 36630024\n",
      "Epoch 1/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 11:22:11.438232: I tensorflow/stream_executor/cuda/cuda_blas.cc:1633] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-12-14 11:22:11.464558: I tensorflow/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814/814 [==============================] - ETA: 0s - loss: 4.2365Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.2365 - val_loss: 3.3259 val_score: 0.825  best_val_score: 0.825  last_epoch t=78.71s, total_time_elapsed t=79.0s\n",
      "814/814 [==============================] - 79s 87ms/step - loss: 4.2365 - val_loss: 3.3259\n",
      "Epoch 2/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.6141Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6141 - val_loss: 3.3394 val_score: 0.839  best_val_score: 0.839  last_epoch t=66.09s, total_time_elapsed t=145.0s\n",
      "814/814 [==============================] - 66s 81ms/step - loss: 3.6141 - val_loss: 3.3394\n",
      "Epoch 3/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.5292Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.5292 - val_loss: 3.2376 val_score: 0.842  best_val_score: 0.842  last_epoch t=65.55s, total_time_elapsed t=210.0s\n",
      "814/814 [==============================] - 66s 81ms/step - loss: 3.5292 - val_loss: 3.2376\n",
      "Epoch 4/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.4725Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4725 - val_loss: 3.3848 val_score: 0.816  best_val_score: 0.842  last_epoch t=62.81s, total_time_elapsed t=273.0s\n",
      "814/814 [==============================] - 63s 77ms/step - loss: 3.4725 - val_loss: 3.3848\n",
      "Epoch 5/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.4149Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.4149 - val_loss: 3.3272 val_score: 0.815  best_val_score: 0.842  last_epoch t=64.11s, total_time_elapsed t=337.0s\n",
      "814/814 [==============================] - 64s 79ms/step - loss: 3.4149 - val_loss: 3.3272\n",
      "Epoch 6/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.3860Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3860 - val_loss: 3.1186 val_score: 0.841  best_val_score: 0.842  last_epoch t=64.76s, total_time_elapsed t=402.0s\n",
      "814/814 [==============================] - 65s 80ms/step - loss: 3.3860 - val_loss: 3.1186\n",
      "Epoch 7/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.3557Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3557 - val_loss: 3.2693 val_score: 0.814  best_val_score: 0.842  last_epoch t=65.12s, total_time_elapsed t=467.0s\n",
      "814/814 [==============================] - 65s 80ms/step - loss: 3.3557 - val_loss: 3.2693\n",
      "Epoch 8/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.3223Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.3223 - val_loss: 3.1043 val_score: 0.845  best_val_score: 0.845  last_epoch t=67.64s, total_time_elapsed t=535.0s\n",
      "814/814 [==============================] - 68s 83ms/step - loss: 3.3223 - val_loss: 3.1043\n",
      "Epoch 9/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.3011Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.3011 - val_loss: 3.0926 val_score: 0.838  best_val_score: 0.845  last_epoch t=63.87s, total_time_elapsed t=599.0s\n",
      "814/814 [==============================] - 64s 78ms/step - loss: 3.3011 - val_loss: 3.0926\n",
      "Epoch 10/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.2658Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2658 - val_loss: 3.1529 val_score: 0.834  best_val_score: 0.845  last_epoch t=63.45s, total_time_elapsed t=662.0s\n",
      "814/814 [==============================] - 63s 78ms/step - loss: 3.2658 - val_loss: 3.1529\n",
      "Epoch 11/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.2575Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2575 - val_loss: 3.1725 val_score: 0.832  best_val_score: 0.845  last_epoch t=64.40s, total_time_elapsed t=727.0s\n",
      "814/814 [==============================] - 64s 79ms/step - loss: 3.2575 - val_loss: 3.1725\n",
      "Epoch 12/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.2333Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.2333 - val_loss: 3.1868 val_score: 0.825  best_val_score: 0.845  last_epoch t=64.44s, total_time_elapsed t=791.0s\n",
      "814/814 [==============================] - 64s 79ms/step - loss: 3.2333 - val_loss: 3.1868\n",
      "Epoch 13/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.2081Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.2081 - val_loss: 3.1306 val_score: 0.835  best_val_score: 0.845  last_epoch t=65.61s, total_time_elapsed t=857.0s\n",
      "814/814 [==============================] - 66s 81ms/step - loss: 3.2081 - val_loss: 3.1306\n",
      "Epoch 14/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.1857Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.1857 - val_loss: 3.1447 val_score: 0.838  best_val_score: 0.845  last_epoch t=65.35s, total_time_elapsed t=922.0s\n",
      "814/814 [==============================] - 65s 80ms/step - loss: 3.1857 - val_loss: 3.1447\n",
      "Model finished with val loss: 3.1447253227233887\n",
      "22/22 [==============================] - 4s 74ms/step\n",
      "Val Score: 0.8446148803164868\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:05<00:00, 95.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 2-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 237 series for training and 27 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 237/237 [00:13<00:00, 18.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6506, 17280, 32), y: (6506, 17280, 2), series_ids: (6506,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 27.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (698, 17280, 32), y: (698, 17280, 2), series_ids: (698,)\n",
      "Total model parameters: 36630024\n",
      "Epoch 1/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 4.2188Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.2188 - val_loss: 3.5369 val_score: 0.800  best_val_score: 0.800  last_epoch t=77.61s, total_time_elapsed t=78.0s\n",
      "814/814 [==============================] - 78s 86ms/step - loss: 4.2188 - val_loss: 3.5369\n",
      "Epoch 2/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.6344Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6344 - val_loss: 3.4403 val_score: 0.817  best_val_score: 0.817  last_epoch t=66.41s, total_time_elapsed t=144.0s\n",
      "814/814 [==============================] - 66s 82ms/step - loss: 3.6344 - val_loss: 3.4403\n",
      "Epoch 3/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.5043Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.5043 - val_loss: 3.5846 val_score: 0.812  best_val_score: 0.817  last_epoch t=64.57s, total_time_elapsed t=209.0s\n",
      "814/814 [==============================] - 65s 79ms/step - loss: 3.5043 - val_loss: 3.5846\n",
      "Epoch 4/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.4785Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4785 - val_loss: 3.4251 val_score: 0.813  best_val_score: 0.817  last_epoch t=65.84s, total_time_elapsed t=274.0s\n",
      "814/814 [==============================] - 66s 81ms/step - loss: 3.4785 - val_loss: 3.4251\n",
      "Epoch 5/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.4215Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.4215 - val_loss: 3.3718 val_score: 0.820  best_val_score: 0.820  last_epoch t=67.70s, total_time_elapsed t=342.0s\n",
      "814/814 [==============================] - 68s 83ms/step - loss: 3.4215 - val_loss: 3.3718\n",
      "Epoch 6/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.3831Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3831 - val_loss: 3.3354 val_score: 0.822  best_val_score: 0.822  last_epoch t=68.01s, total_time_elapsed t=410.0s\n",
      "814/814 [==============================] - 68s 84ms/step - loss: 3.3831 - val_loss: 3.3354\n",
      "Epoch 7/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.3480Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3480 - val_loss: 3.2971 val_score: 0.826  best_val_score: 0.826  last_epoch t=68.48s, total_time_elapsed t=479.0s\n",
      "814/814 [==============================] - 68s 84ms/step - loss: 3.3480 - val_loss: 3.2971\n",
      "Epoch 8/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.3269Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.3269 - val_loss: 3.2795 val_score: 0.828  best_val_score: 0.828  last_epoch t=66.75s, total_time_elapsed t=545.0s\n",
      "814/814 [==============================] - 67s 82ms/step - loss: 3.3269 - val_loss: 3.2795\n",
      "Epoch 9/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.2999Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.2999 - val_loss: 3.2810 val_score: 0.834  best_val_score: 0.834  last_epoch t=66.01s, total_time_elapsed t=611.0s\n",
      "814/814 [==============================] - 66s 81ms/step - loss: 3.2999 - val_loss: 3.2810\n",
      "Epoch 10/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.2898Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2898 - val_loss: 3.2998 val_score: 0.823  best_val_score: 0.834  last_epoch t=64.08s, total_time_elapsed t=675.0s\n",
      "814/814 [==============================] - 64s 79ms/step - loss: 3.2898 - val_loss: 3.2998\n",
      "Epoch 11/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.2585Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2585 - val_loss: 3.3408 val_score: 0.825  best_val_score: 0.834  last_epoch t=65.35s, total_time_elapsed t=741.0s\n",
      "814/814 [==============================] - 65s 80ms/step - loss: 3.2585 - val_loss: 3.3408\n",
      "Epoch 12/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.2336Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.2336 - val_loss: 3.3242 val_score: 0.827  best_val_score: 0.834  last_epoch t=66.05s, total_time_elapsed t=807.0s\n",
      "814/814 [==============================] - 66s 81ms/step - loss: 3.2336 - val_loss: 3.3242\n",
      "Epoch 13/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.2138Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.2138 - val_loss: 3.2763 val_score: 0.841  best_val_score: 0.841  last_epoch t=67.64s, total_time_elapsed t=875.0s\n",
      "814/814 [==============================] - 68s 83ms/step - loss: 3.2138 - val_loss: 3.2763\n",
      "Epoch 14/14\n",
      "814/814 [==============================] - ETA: 0s - loss: 3.1964Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.1964 - val_loss: 3.2803 val_score: 0.827  best_val_score: 0.841  last_epoch t=64.62s, total_time_elapsed t=939.0s\n",
      "814/814 [==============================] - 65s 79ms/step - loss: 3.1964 - val_loss: 3.2803\n",
      "Model finished with val loss: 3.280275583267212\n",
      "22/22 [==============================] - 4s 72ms/step\n",
      "Val Score: 0.8404970302714813\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:05<00:00, 93.32it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 3-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 237 series for training and 27 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 237/237 [00:10<00:00, 23.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6438, 17280, 32), y: (6438, 17280, 2), series_ids: (6438,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 33.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (766, 17280, 32), y: (766, 17280, 2), series_ids: (766,)\n",
      "Total model parameters: 36630024\n",
      "Epoch 1/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 4.3073Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.3073 - val_loss: 3.6976 val_score: 0.671  best_val_score: 0.671  last_epoch t=79.47s, total_time_elapsed t=79.0s\n",
      "805/805 [==============================] - 79s 90ms/step - loss: 4.3073 - val_loss: 3.6976\n",
      "Epoch 2/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.6274Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6274 - val_loss: 3.5764 val_score: 0.705  best_val_score: 0.705  last_epoch t=68.11s, total_time_elapsed t=148.0s\n",
      "805/805 [==============================] - 68s 85ms/step - loss: 3.6274 - val_loss: 3.5764\n",
      "Epoch 3/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.5248Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.5248 - val_loss: 3.7643 val_score: 0.695  best_val_score: 0.705  last_epoch t=66.63s, total_time_elapsed t=214.0s\n",
      "805/805 [==============================] - 67s 83ms/step - loss: 3.5248 - val_loss: 3.7643\n",
      "Epoch 4/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.4648Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4648 - val_loss: 3.5406 val_score: 0.705  best_val_score: 0.705  last_epoch t=67.87s, total_time_elapsed t=282.0s\n",
      "805/805 [==============================] - 68s 84ms/step - loss: 3.4648 - val_loss: 3.5406\n",
      "Epoch 5/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.4049Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.4049 - val_loss: 3.6074 val_score: 0.697  best_val_score: 0.705  last_epoch t=64.88s, total_time_elapsed t=347.0s\n",
      "805/805 [==============================] - 65s 81ms/step - loss: 3.4049 - val_loss: 3.6074\n",
      "Epoch 6/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.3634Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3634 - val_loss: 3.5569 val_score: 0.698  best_val_score: 0.705  last_epoch t=65.90s, total_time_elapsed t=413.0s\n",
      "805/805 [==============================] - 66s 82ms/step - loss: 3.3634 - val_loss: 3.5569\n",
      "Epoch 7/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.3198Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3198 - val_loss: 3.4951 val_score: 0.712  best_val_score: 0.712  last_epoch t=68.84s, total_time_elapsed t=482.0s\n",
      "805/805 [==============================] - 69s 86ms/step - loss: 3.3198 - val_loss: 3.4951\n",
      "Epoch 8/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.3024Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.3024 - val_loss: 3.8044 val_score: 0.663  best_val_score: 0.712  last_epoch t=65.74s, total_time_elapsed t=547.0s\n",
      "805/805 [==============================] - 66s 82ms/step - loss: 3.3024 - val_loss: 3.8044\n",
      "Epoch 9/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.2850Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.2850 - val_loss: 3.5131 val_score: 0.722  best_val_score: 0.722  last_epoch t=68.25s, total_time_elapsed t=616.0s\n",
      "805/805 [==============================] - 68s 85ms/step - loss: 3.2850 - val_loss: 3.5131\n",
      "Epoch 10/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.2491Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2491 - val_loss: 3.4959 val_score: 0.716  best_val_score: 0.722  last_epoch t=65.88s, total_time_elapsed t=682.0s\n",
      "805/805 [==============================] - 66s 82ms/step - loss: 3.2491 - val_loss: 3.4959\n",
      "Epoch 11/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.2308Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2308 - val_loss: 3.4776 val_score: 0.715  best_val_score: 0.722  last_epoch t=65.80s, total_time_elapsed t=747.0s\n",
      "805/805 [==============================] - 66s 82ms/step - loss: 3.2308 - val_loss: 3.4776\n",
      "Epoch 12/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.2082Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.2082 - val_loss: 3.5885 val_score: 0.708  best_val_score: 0.722  last_epoch t=65.93s, total_time_elapsed t=813.0s\n",
      "805/805 [==============================] - 66s 82ms/step - loss: 3.2082 - val_loss: 3.5885\n",
      "Epoch 13/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.1883Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.1883 - val_loss: 3.4564 val_score: 0.716  best_val_score: 0.722  last_epoch t=66.49s, total_time_elapsed t=880.0s\n",
      "805/805 [==============================] - 66s 83ms/step - loss: 3.1883 - val_loss: 3.4564\n",
      "Epoch 14/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.1635Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.1635 - val_loss: 3.4657 val_score: 0.715  best_val_score: 0.722  last_epoch t=65.52s, total_time_elapsed t=945.0s\n",
      "805/805 [==============================] - 66s 81ms/step - loss: 3.1635 - val_loss: 3.4657\n",
      "Model finished with val loss: 3.4657084941864014\n",
      "24/24 [==============================] - 5s 81ms/step\n",
      "Val Score: 0.7215648380602935\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:05<00:00, 104.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 4-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 237 series for training and 27 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 237/237 [00:10<00:00, 23.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6382, 17280, 32), y: (6382, 17280, 2), series_ids: (6382,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:01<00:00, 22.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (822, 17280, 32), y: (822, 17280, 2), series_ids: (822,)\n",
      "Total model parameters: 36630024\n",
      "Epoch 1/14\n",
      "798/798 [==============================] - ETA: 0s - loss: 4.2574Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.2574 - val_loss: 4.0319 val_score: 0.773  best_val_score: 0.773  last_epoch t=80.61s, total_time_elapsed t=81.0s\n",
      "798/798 [==============================] - 81s 92ms/step - loss: 4.2574 - val_loss: 4.0319\n",
      "Epoch 2/14\n",
      "798/798 [==============================] - ETA: 0s - loss: 3.6002Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6002 - val_loss: 3.8715 val_score: 0.785  best_val_score: 0.785  last_epoch t=68.71s, total_time_elapsed t=149.0s\n",
      "798/798 [==============================] - 69s 86ms/step - loss: 3.6002 - val_loss: 3.8715\n",
      "Epoch 3/14\n",
      "798/798 [==============================] - ETA: 0s - loss: 3.4853Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.4853 - val_loss: 3.7834 val_score: 0.789  best_val_score: 0.789  last_epoch t=68.77s, total_time_elapsed t=218.0s\n",
      "798/798 [==============================] - 69s 86ms/step - loss: 3.4853 - val_loss: 3.7834\n",
      "Epoch 4/14\n",
      "798/798 [==============================] - ETA: 0s - loss: 3.4645Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4645 - val_loss: 3.8841 val_score: 0.787  best_val_score: 0.789  last_epoch t=66.46s, total_time_elapsed t=285.0s\n",
      "798/798 [==============================] - 66s 83ms/step - loss: 3.4645 - val_loss: 3.8841\n",
      "Epoch 5/14\n",
      "798/798 [==============================] - ETA: 0s - loss: 3.3803Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.3803 - val_loss: 3.8142 val_score: 0.797  best_val_score: 0.797  last_epoch t=68.66s, total_time_elapsed t=353.0s\n",
      "798/798 [==============================] - 69s 86ms/step - loss: 3.3803 - val_loss: 3.8142\n",
      "Epoch 6/14\n",
      "798/798 [==============================] - ETA: 0s - loss: 3.3337Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3337 - val_loss: 3.7905 val_score: 0.797  best_val_score: 0.797  last_epoch t=66.74s, total_time_elapsed t=420.0s\n",
      "798/798 [==============================] - 67s 84ms/step - loss: 3.3337 - val_loss: 3.7905\n",
      "Epoch 7/14\n",
      "798/798 [==============================] - ETA: 0s - loss: 3.3079Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3079 - val_loss: 3.6768 val_score: 0.807  best_val_score: 0.807  last_epoch t=68.67s, total_time_elapsed t=489.0s\n",
      "798/798 [==============================] - 69s 86ms/step - loss: 3.3079 - val_loss: 3.6768\n",
      "Epoch 8/14\n",
      "798/798 [==============================] - ETA: 0s - loss: 3.2754Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.2754 - val_loss: 3.7213 val_score: 0.799  best_val_score: 0.807  last_epoch t=66.75s, total_time_elapsed t=555.0s\n",
      "798/798 [==============================] - 67s 84ms/step - loss: 3.2754 - val_loss: 3.7213\n",
      "Epoch 9/14\n",
      "798/798 [==============================] - ETA: 0s - loss: 3.2485Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.2485 - val_loss: 3.7983 val_score: 0.812  best_val_score: 0.812  last_epoch t=68.89s, total_time_elapsed t=624.0s\n",
      "798/798 [==============================] - 69s 86ms/step - loss: 3.2485 - val_loss: 3.7983\n",
      "Epoch 10/14\n",
      "798/798 [==============================] - ETA: 0s - loss: 3.2285Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2285 - val_loss: 3.6957 val_score: 0.807  best_val_score: 0.812  last_epoch t=67.19s, total_time_elapsed t=691.0s\n",
      "798/798 [==============================] - 67s 84ms/step - loss: 3.2285 - val_loss: 3.6957\n",
      "Epoch 11/14\n",
      "798/798 [==============================] - ETA: 0s - loss: 3.2060Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2060 - val_loss: 3.6794 val_score: 0.810  best_val_score: 0.812  last_epoch t=67.04s, total_time_elapsed t=759.0s\n",
      "798/798 [==============================] - 67s 84ms/step - loss: 3.2060 - val_loss: 3.6794\n",
      "Epoch 12/14\n",
      "798/798 [==============================] - ETA: 0s - loss: 3.1935Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.1935 - val_loss: 3.6260 val_score: 0.816  best_val_score: 0.816  last_epoch t=69.48s, total_time_elapsed t=828.0s\n",
      "798/798 [==============================] - 69s 87ms/step - loss: 3.1935 - val_loss: 3.6260\n",
      "Epoch 13/14\n",
      "798/798 [==============================] - ETA: 0s - loss: 3.1684Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.1684 - val_loss: 3.6693 val_score: 0.812  best_val_score: 0.816  last_epoch t=67.04s, total_time_elapsed t=895.0s\n",
      "798/798 [==============================] - 67s 84ms/step - loss: 3.1684 - val_loss: 3.6693\n",
      "Epoch 14/14\n",
      "798/798 [==============================] - ETA: 0s - loss: 3.1550Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.1550 - val_loss: 3.6717 val_score: 0.811  best_val_score: 0.816  last_epoch t=66.82s, total_time_elapsed t=962.0s\n",
      "798/798 [==============================] - 67s 84ms/step - loss: 3.1550 - val_loss: 3.6717\n",
      "Model finished with val loss: 3.671682357788086\n",
      "26/26 [==============================] - 4s 77ms/step\n",
      "Val Score: 0.8163103730462202\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:04<00:00, 109.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 5-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 238 series for training and 26 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:09<00:00, 25.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6434, 17280, 32), y: (6434, 17280, 2), series_ids: (6434,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 32.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (770, 17280, 32), y: (770, 17280, 2), series_ids: (770,)\n",
      "Total model parameters: 36630024\n",
      "Epoch 1/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 4.3112Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.3112 - val_loss: 3.5145 val_score: 0.788  best_val_score: 0.788  last_epoch t=79.83s, total_time_elapsed t=80.0s\n",
      "805/805 [==============================] - 80s 92ms/step - loss: 4.3112 - val_loss: 3.5145\n",
      "Epoch 2/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.6581Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6581 - val_loss: 3.2617 val_score: 0.807  best_val_score: 0.807  last_epoch t=68.59s, total_time_elapsed t=148.0s\n",
      "805/805 [==============================] - 69s 85ms/step - loss: 3.6581 - val_loss: 3.2617\n",
      "Epoch 3/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.5666Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.5666 - val_loss: 3.3493 val_score: 0.798  best_val_score: 0.807  last_epoch t=67.03s, total_time_elapsed t=215.0s\n",
      "805/805 [==============================] - 67s 83ms/step - loss: 3.5666 - val_loss: 3.3493\n",
      "Epoch 4/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.4848Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4848 - val_loss: 3.4231 val_score: 0.814  best_val_score: 0.814  last_epoch t=68.93s, total_time_elapsed t=284.0s\n",
      "805/805 [==============================] - 69s 86ms/step - loss: 3.4848 - val_loss: 3.4231\n",
      "Epoch 5/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.4451Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.4451 - val_loss: 3.2810 val_score: 0.804  best_val_score: 0.814  last_epoch t=66.46s, total_time_elapsed t=351.0s\n",
      "805/805 [==============================] - 66s 83ms/step - loss: 3.4451 - val_loss: 3.2810\n",
      "Epoch 6/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.4028Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.4028 - val_loss: 3.1206 val_score: 0.825  best_val_score: 0.825  last_epoch t=68.07s, total_time_elapsed t=419.0s\n",
      "805/805 [==============================] - 68s 85ms/step - loss: 3.4028 - val_loss: 3.1206\n",
      "Epoch 7/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.3636Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3636 - val_loss: 3.1260 val_score: 0.828  best_val_score: 0.828  last_epoch t=68.98s, total_time_elapsed t=488.0s\n",
      "805/805 [==============================] - 69s 86ms/step - loss: 3.3636 - val_loss: 3.1260\n",
      "Epoch 8/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.3403Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.3403 - val_loss: 3.1480 val_score: 0.822  best_val_score: 0.828  last_epoch t=65.13s, total_time_elapsed t=553.0s\n",
      "805/805 [==============================] - 65s 81ms/step - loss: 3.3403 - val_loss: 3.1480\n",
      "Epoch 9/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.3154Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.3154 - val_loss: 3.1216 val_score: 0.824  best_val_score: 0.828  last_epoch t=66.34s, total_time_elapsed t=619.0s\n",
      "805/805 [==============================] - 66s 82ms/step - loss: 3.3154 - val_loss: 3.1216\n",
      "Epoch 10/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.2955Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2955 - val_loss: 3.1694 val_score: 0.820  best_val_score: 0.828  last_epoch t=65.54s, total_time_elapsed t=685.0s\n",
      "805/805 [==============================] - 66s 81ms/step - loss: 3.2955 - val_loss: 3.1694\n",
      "Epoch 11/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.2795Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2795 - val_loss: 3.1348 val_score: 0.826  best_val_score: 0.828  last_epoch t=65.68s, total_time_elapsed t=751.0s\n",
      "805/805 [==============================] - 66s 82ms/step - loss: 3.2795 - val_loss: 3.1348\n",
      "Epoch 12/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.2550Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.2550 - val_loss: 3.0999 val_score: 0.831  best_val_score: 0.831  last_epoch t=68.28s, total_time_elapsed t=819.0s\n",
      "805/805 [==============================] - 68s 85ms/step - loss: 3.2550 - val_loss: 3.0999\n",
      "Epoch 13/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.2290Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.2290 - val_loss: 3.1114 val_score: 0.830  best_val_score: 0.831  last_epoch t=64.56s, total_time_elapsed t=883.0s\n",
      "805/805 [==============================] - 65s 80ms/step - loss: 3.2290 - val_loss: 3.1114\n",
      "Epoch 14/14\n",
      "805/805 [==============================] - ETA: 0s - loss: 3.2081Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.2081 - val_loss: 3.1723 val_score: 0.826  best_val_score: 0.831  last_epoch t=65.29s, total_time_elapsed t=949.0s\n",
      "805/805 [==============================] - 65s 81ms/step - loss: 3.2081 - val_loss: 3.1723\n",
      "Model finished with val loss: 3.172273874282837\n",
      "25/25 [==============================] - 4s 68ms/step\n",
      "Val Score: 0.8307696930838797\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:05<00:00, 100.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 6-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 238 series for training and 26 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:10<00:00, 23.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6472, 17280, 32), y: (6472, 17280, 2), series_ids: (6472,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 33.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (732, 17280, 32), y: (732, 17280, 2), series_ids: (732,)\n",
      "Total model parameters: 36630024\n",
      "Epoch 1/14\n",
      "809/809 [==============================] - ETA: 0s - loss: 4.3250Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.3250 - val_loss: 3.4169 val_score: 0.806  best_val_score: 0.806  last_epoch t=70.90s, total_time_elapsed t=71.0s\n",
      "809/809 [==============================] - 71s 80ms/step - loss: 4.3250 - val_loss: 3.4169\n",
      "Epoch 2/14\n",
      "809/809 [==============================] - ETA: 0s - loss: 3.6556Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6556 - val_loss: 3.3625 val_score: 0.819  best_val_score: 0.819  last_epoch t=61.59s, total_time_elapsed t=132.0s\n",
      "809/809 [==============================] - 62s 76ms/step - loss: 3.6556 - val_loss: 3.3625\n",
      "Epoch 3/14\n",
      "809/809 [==============================] - ETA: 0s - loss: 3.5590Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.5590 - val_loss: 3.2505 val_score: 0.833  best_val_score: 0.833  last_epoch t=59.09s, total_time_elapsed t=192.0s\n",
      "809/809 [==============================] - 59s 73ms/step - loss: 3.5590 - val_loss: 3.2505\n",
      "Epoch 4/14\n",
      "809/809 [==============================] - ETA: 0s - loss: 3.4806Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4806 - val_loss: 3.2053 val_score: 0.832  best_val_score: 0.833  last_epoch t=58.90s, total_time_elapsed t=250.0s\n",
      "809/809 [==============================] - 59s 73ms/step - loss: 3.4806 - val_loss: 3.2053\n",
      "Epoch 5/14\n",
      "809/809 [==============================] - ETA: 0s - loss: 3.4371Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.4371 - val_loss: 3.2626 val_score: 0.828  best_val_score: 0.833  last_epoch t=58.63s, total_time_elapsed t=309.0s\n",
      "809/809 [==============================] - 59s 72ms/step - loss: 3.4371 - val_loss: 3.2626\n",
      "Epoch 6/14\n",
      "809/809 [==============================] - ETA: 0s - loss: 3.3985Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3985 - val_loss: 3.2604 val_score: 0.830  best_val_score: 0.833  last_epoch t=59.79s, total_time_elapsed t=369.0s\n",
      "809/809 [==============================] - 60s 74ms/step - loss: 3.3985 - val_loss: 3.2604\n",
      "Epoch 7/14\n",
      "809/809 [==============================] - ETA: 0s - loss: 3.3674Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3674 - val_loss: 3.2110 val_score: 0.845  best_val_score: 0.845  last_epoch t=61.17s, total_time_elapsed t=430.0s\n",
      "809/809 [==============================] - 61s 76ms/step - loss: 3.3674 - val_loss: 3.2110\n",
      "Epoch 8/14\n",
      "809/809 [==============================] - ETA: 0s - loss: 3.3442Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.3442 - val_loss: 3.1194 val_score: 0.842  best_val_score: 0.845  last_epoch t=59.46s, total_time_elapsed t=490.0s\n",
      "809/809 [==============================] - 59s 74ms/step - loss: 3.3442 - val_loss: 3.1194\n",
      "Epoch 9/14\n",
      "809/809 [==============================] - ETA: 0s - loss: 3.3175Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.3175 - val_loss: 3.1751 val_score: 0.841  best_val_score: 0.845  last_epoch t=59.55s, total_time_elapsed t=549.0s\n",
      "809/809 [==============================] - 60s 74ms/step - loss: 3.3175 - val_loss: 3.1751\n",
      "Epoch 10/14\n",
      "809/809 [==============================] - ETA: 0s - loss: 3.2916Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2916 - val_loss: 3.1309 val_score: 0.849  best_val_score: 0.849  last_epoch t=60.96s, total_time_elapsed t=610.0s\n",
      "809/809 [==============================] - 61s 75ms/step - loss: 3.2916 - val_loss: 3.1309\n",
      "Epoch 11/14\n",
      "809/809 [==============================] - ETA: 0s - loss: 3.2670Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2670 - val_loss: 3.1850 val_score: 0.835  best_val_score: 0.849  last_epoch t=58.82s, total_time_elapsed t=669.0s\n",
      "809/809 [==============================] - 59s 73ms/step - loss: 3.2670 - val_loss: 3.1850\n",
      "Epoch 12/14\n",
      "809/809 [==============================] - ETA: 0s - loss: 3.2473Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.2473 - val_loss: 3.2153 val_score: 0.831  best_val_score: 0.849  last_epoch t=59.22s, total_time_elapsed t=728.0s\n",
      "809/809 [==============================] - 59s 73ms/step - loss: 3.2473 - val_loss: 3.2153\n",
      "Epoch 13/14\n",
      "809/809 [==============================] - ETA: 0s - loss: 3.2250Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.2250 - val_loss: 3.1545 val_score: 0.846  best_val_score: 0.849  last_epoch t=59.15s, total_time_elapsed t=787.0s\n",
      "809/809 [==============================] - 59s 73ms/step - loss: 3.2250 - val_loss: 3.1545\n",
      "Epoch 14/14\n",
      "809/809 [==============================] - ETA: 0s - loss: 3.1973Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.1973 - val_loss: 3.0751 val_score: 0.848  best_val_score: 0.849  last_epoch t=59.67s, total_time_elapsed t=847.0s\n",
      "809/809 [==============================] - 60s 74ms/step - loss: 3.1973 - val_loss: 3.0751\n",
      "Model finished with val loss: 3.0750925540924072\n",
      "23/23 [==============================] - 6s 80ms/step\n",
      "Val Score: 0.8491317976307752\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:04<00:00, 109.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 7-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 238 series for training and 26 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:10<00:00, 21.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6533, 17280, 32), y: (6533, 17280, 2), series_ids: (6533,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 27.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (671, 17280, 32), y: (671, 17280, 2), series_ids: (671,)\n",
      "Total model parameters: 36630024\n",
      "Epoch 1/14\n",
      "817/817 [==============================] - ETA: 0s - loss: 4.3116Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.3116 - val_loss: 3.7540 val_score: 0.807  best_val_score: 0.807  last_epoch t=78.10s, total_time_elapsed t=78.0s\n",
      "817/817 [==============================] - 78s 87ms/step - loss: 4.3116 - val_loss: 3.7540\n",
      "Epoch 2/14\n",
      "817/817 [==============================] - ETA: 0s - loss: 3.6107Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6107 - val_loss: 3.5824 val_score: 0.826  best_val_score: 0.826  last_epoch t=65.30s, total_time_elapsed t=143.0s\n",
      "817/817 [==============================] - 65s 80ms/step - loss: 3.6107 - val_loss: 3.5824\n",
      "Epoch 3/14\n",
      "817/817 [==============================] - ETA: 0s - loss: 3.5013Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.5013 - val_loss: 3.4933 val_score: 0.844  best_val_score: 0.844  last_epoch t=65.74s, total_time_elapsed t=209.0s\n",
      "817/817 [==============================] - 66s 80ms/step - loss: 3.5013 - val_loss: 3.4933\n",
      "Epoch 4/14\n",
      "817/817 [==============================] - ETA: 0s - loss: 3.4491Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4491 - val_loss: 3.4970 val_score: 0.838  best_val_score: 0.844  last_epoch t=65.11s, total_time_elapsed t=274.0s\n",
      "817/817 [==============================] - 65s 80ms/step - loss: 3.4491 - val_loss: 3.4970\n",
      "Epoch 5/14\n",
      "817/817 [==============================] - ETA: 0s - loss: 3.3829Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.3829 - val_loss: 3.4633 val_score: 0.839  best_val_score: 0.844  last_epoch t=64.16s, total_time_elapsed t=338.0s\n",
      "817/817 [==============================] - 64s 79ms/step - loss: 3.3829 - val_loss: 3.4633\n",
      "Epoch 6/14\n",
      "817/817 [==============================] - ETA: 0s - loss: 3.3695Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3695 - val_loss: 3.5494 val_score: 0.836  best_val_score: 0.844  last_epoch t=64.74s, total_time_elapsed t=403.0s\n",
      "817/817 [==============================] - 65s 79ms/step - loss: 3.3695 - val_loss: 3.5494\n",
      "Epoch 7/14\n",
      "817/817 [==============================] - ETA: 0s - loss: 3.3392Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3392 - val_loss: 3.6072 val_score: 0.839  best_val_score: 0.844  last_epoch t=63.93s, total_time_elapsed t=467.0s\n",
      "817/817 [==============================] - 64s 78ms/step - loss: 3.3392 - val_loss: 3.6072\n",
      "Epoch 8/14\n",
      "817/817 [==============================] - ETA: 0s - loss: 3.3033Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.3033 - val_loss: 3.7375 val_score: 0.840  best_val_score: 0.844  last_epoch t=65.03s, total_time_elapsed t=532.0s\n",
      "817/817 [==============================] - 65s 80ms/step - loss: 3.3033 - val_loss: 3.7375\n",
      "Epoch 9/14\n",
      "817/817 [==============================] - ETA: 0s - loss: 3.2788Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.2788 - val_loss: 3.4368 val_score: 0.847  best_val_score: 0.847  last_epoch t=68.17s, total_time_elapsed t=600.0s\n",
      "817/817 [==============================] - 68s 83ms/step - loss: 3.2788 - val_loss: 3.4368\n",
      "Epoch 10/14\n",
      "817/817 [==============================] - ETA: 0s - loss: 3.2496Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2496 - val_loss: 3.4960 val_score: 0.839  best_val_score: 0.847  last_epoch t=65.92s, total_time_elapsed t=666.0s\n",
      "817/817 [==============================] - 66s 81ms/step - loss: 3.2496 - val_loss: 3.4960\n",
      "Epoch 11/14\n",
      "817/817 [==============================] - ETA: 0s - loss: 3.2383Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2383 - val_loss: 3.4567 val_score: 0.843  best_val_score: 0.847  last_epoch t=64.35s, total_time_elapsed t=731.0s\n",
      "817/817 [==============================] - 64s 79ms/step - loss: 3.2383 - val_loss: 3.4567\n",
      "Epoch 12/14\n",
      "817/817 [==============================] - ETA: 0s - loss: 3.2117Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.2117 - val_loss: 3.4517 val_score: 0.846  best_val_score: 0.847  last_epoch t=66.34s, total_time_elapsed t=797.0s\n",
      "817/817 [==============================] - 66s 81ms/step - loss: 3.2117 - val_loss: 3.4517\n",
      "Epoch 13/14\n",
      "817/817 [==============================] - ETA: 0s - loss: 3.2023Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.2023 - val_loss: 3.4998 val_score: 0.851  best_val_score: 0.851  last_epoch t=68.49s, total_time_elapsed t=865.0s\n",
      "817/817 [==============================] - 68s 84ms/step - loss: 3.2023 - val_loss: 3.4998\n",
      "Epoch 14/14\n",
      "817/817 [==============================] - ETA: 0s - loss: 3.1808Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.1808 - val_loss: 3.4421 val_score: 0.847  best_val_score: 0.851  last_epoch t=64.51s, total_time_elapsed t=930.0s\n",
      "817/817 [==============================] - 65s 79ms/step - loss: 3.1808 - val_loss: 3.4421\n",
      "Model finished with val loss: 3.4421274662017822\n",
      "21/21 [==============================] - 3s 82ms/step\n",
      "Val Score: 0.8511523974776616\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:04<00:00, 127.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 8-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 238 series for training and 26 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:09<00:00, 25.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6478, 17280, 32), y: (6478, 17280, 2), series_ids: (6478,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 29.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (726, 17280, 32), y: (726, 17280, 2), series_ids: (726,)\n",
      "Total model parameters: 36630024\n",
      "Epoch 1/14\n",
      "810/810 [==============================] - ETA: 0s - loss: 4.1204Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.1204 - val_loss: 4.2012 val_score: 0.801  best_val_score: 0.801  last_epoch t=78.86s, total_time_elapsed t=79.0s\n",
      "810/810 [==============================] - 79s 90ms/step - loss: 4.1204 - val_loss: 4.2012\n",
      "Epoch 2/14\n",
      "810/810 [==============================] - ETA: 0s - loss: 3.5640Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.5640 - val_loss: 3.7737 val_score: 0.809  best_val_score: 0.809  last_epoch t=66.83s, total_time_elapsed t=146.0s\n",
      "810/810 [==============================] - 67s 83ms/step - loss: 3.5640 - val_loss: 3.7737\n",
      "Epoch 3/14\n",
      "810/810 [==============================] - ETA: 0s - loss: 3.4744Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.4744 - val_loss: 3.8030 val_score: 0.806  best_val_score: 0.809  last_epoch t=66.30s, total_time_elapsed t=212.0s\n",
      "810/810 [==============================] - 66s 82ms/step - loss: 3.4744 - val_loss: 3.8030\n",
      "Epoch 4/14\n",
      "810/810 [==============================] - ETA: 0s - loss: 3.4312Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4312 - val_loss: 3.8762 val_score: 0.816  best_val_score: 0.816  last_epoch t=67.91s, total_time_elapsed t=280.0s\n",
      "810/810 [==============================] - 68s 84ms/step - loss: 3.4312 - val_loss: 3.8762\n",
      "Epoch 5/14\n",
      "810/810 [==============================] - ETA: 0s - loss: 3.3809Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.3809 - val_loss: 3.8834 val_score: 0.828  best_val_score: 0.828  last_epoch t=67.52s, total_time_elapsed t=347.0s\n",
      "810/810 [==============================] - 68s 83ms/step - loss: 3.3809 - val_loss: 3.8834\n",
      "Epoch 6/14\n",
      "810/810 [==============================] - ETA: 0s - loss: 3.3284Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3284 - val_loss: 3.8367 val_score: 0.818  best_val_score: 0.828  last_epoch t=66.37s, total_time_elapsed t=414.0s\n",
      "810/810 [==============================] - 66s 82ms/step - loss: 3.3284 - val_loss: 3.8367\n",
      "Epoch 7/14\n",
      "810/810 [==============================] - ETA: 0s - loss: 3.3016Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3016 - val_loss: 3.6612 val_score: 0.826  best_val_score: 0.828  last_epoch t=64.72s, total_time_elapsed t=479.0s\n",
      "810/810 [==============================] - 65s 80ms/step - loss: 3.3016 - val_loss: 3.6612\n",
      "Epoch 8/14\n",
      "810/810 [==============================] - ETA: 0s - loss: 3.2883Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.2883 - val_loss: 3.6743 val_score: 0.831  best_val_score: 0.831  last_epoch t=69.00s, total_time_elapsed t=548.0s\n",
      "810/810 [==============================] - 69s 85ms/step - loss: 3.2883 - val_loss: 3.6743\n",
      "Epoch 9/14\n",
      "810/810 [==============================] - ETA: 0s - loss: 3.2531Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.2531 - val_loss: 3.6758 val_score: 0.836  best_val_score: 0.836  last_epoch t=66.92s, total_time_elapsed t=614.0s\n",
      "810/810 [==============================] - 67s 83ms/step - loss: 3.2531 - val_loss: 3.6758\n",
      "Epoch 10/14\n",
      "810/810 [==============================] - ETA: 0s - loss: 3.2353Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2353 - val_loss: 3.5841 val_score: 0.836  best_val_score: 0.836  last_epoch t=65.66s, total_time_elapsed t=680.0s\n",
      "810/810 [==============================] - 66s 81ms/step - loss: 3.2353 - val_loss: 3.5841\n",
      "Epoch 11/14\n",
      "810/810 [==============================] - ETA: 0s - loss: 3.2151Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2151 - val_loss: 3.6092 val_score: 0.839  best_val_score: 0.839  last_epoch t=68.10s, total_time_elapsed t=748.0s\n",
      "810/810 [==============================] - 68s 84ms/step - loss: 3.2151 - val_loss: 3.6092\n",
      "Epoch 12/14\n",
      "810/810 [==============================] - ETA: 0s - loss: 3.1955Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.1955 - val_loss: 3.5877 val_score: 0.836  best_val_score: 0.839  last_epoch t=66.40s, total_time_elapsed t=815.0s\n",
      "810/810 [==============================] - 66s 82ms/step - loss: 3.1955 - val_loss: 3.5877\n",
      "Epoch 13/14\n",
      "810/810 [==============================] - ETA: 0s - loss: 3.1598Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.1598 - val_loss: 3.6473 val_score: 0.833  best_val_score: 0.839  last_epoch t=65.46s, total_time_elapsed t=880.0s\n",
      "810/810 [==============================] - 65s 81ms/step - loss: 3.1598 - val_loss: 3.6473\n",
      "Epoch 14/14\n",
      "810/810 [==============================] - ETA: 0s - loss: 3.1470Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.1470 - val_loss: 3.6148 val_score: 0.837  best_val_score: 0.839  last_epoch t=65.52s, total_time_elapsed t=946.0s\n",
      "810/810 [==============================] - 66s 81ms/step - loss: 3.1470 - val_loss: 3.6148\n",
      "Model finished with val loss: 3.614832878112793\n",
      "23/23 [==============================] - 4s 69ms/step\n",
      "Val Score: 0.8386028637850245\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:04<00:00, 111.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 9-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 238 series for training and 26 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:09<00:00, 24.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6545, 17280, 32), y: (6545, 17280, 2), series_ids: (6545,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 34.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (659, 17280, 32), y: (659, 17280, 2), series_ids: (659,)\n",
      "Total model parameters: 36630024\n",
      "Epoch 1/14\n",
      "819/819 [==============================] - ETA: 0s - loss: 4.3432Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.3432 - val_loss: 3.9902 val_score: 0.761  best_val_score: 0.761  last_epoch t=77.58s, total_time_elapsed t=78.0s\n",
      "819/819 [==============================] - 78s 87ms/step - loss: 4.3432 - val_loss: 3.9902\n",
      "Epoch 2/14\n",
      "819/819 [==============================] - ETA: 0s - loss: 3.6080Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6080 - val_loss: 3.8300 val_score: 0.781  best_val_score: 0.781  last_epoch t=67.66s, total_time_elapsed t=145.0s\n",
      "819/819 [==============================] - 68s 83ms/step - loss: 3.6080 - val_loss: 3.8300\n",
      "Epoch 3/14\n",
      "818/819 [============================>.] - ETA: 0s - loss: 3.4941Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.4935 - val_loss: 3.7840 val_score: 0.779  best_val_score: 0.781  last_epoch t=64.84s, total_time_elapsed t=210.0s\n",
      "819/819 [==============================] - 65s 79ms/step - loss: 3.4935 - val_loss: 3.7840\n",
      "Epoch 4/14\n",
      "818/819 [============================>.] - ETA: 0s - loss: 3.4264Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4262 - val_loss: 3.6718 val_score: 0.796  best_val_score: 0.796  last_epoch t=68.03s, total_time_elapsed t=278.0s\n",
      "819/819 [==============================] - 68s 83ms/step - loss: 3.4262 - val_loss: 3.6718\n",
      "Epoch 5/14\n",
      "819/819 [==============================] - ETA: 0s - loss: 3.3861Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.3861 - val_loss: 3.6792 val_score: 0.797  best_val_score: 0.797  last_epoch t=68.01s, total_time_elapsed t=346.0s\n",
      "819/819 [==============================] - 68s 83ms/step - loss: 3.3861 - val_loss: 3.6792\n",
      "Epoch 6/14\n",
      "819/819 [==============================] - ETA: 0s - loss: 3.3446Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3446 - val_loss: 3.6152 val_score: 0.802  best_val_score: 0.802  last_epoch t=67.31s, total_time_elapsed t=413.0s\n",
      "819/819 [==============================] - 67s 82ms/step - loss: 3.3446 - val_loss: 3.6152\n",
      "Epoch 7/14\n",
      "819/819 [==============================] - ETA: 0s - loss: 3.3208Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3208 - val_loss: 3.7230 val_score: 0.798  best_val_score: 0.802  last_epoch t=64.72s, total_time_elapsed t=478.0s\n",
      "819/819 [==============================] - 65s 79ms/step - loss: 3.3208 - val_loss: 3.7230\n",
      "Epoch 8/14\n",
      "819/819 [==============================] - ETA: 0s - loss: 3.2817Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.2817 - val_loss: 3.8168 val_score: 0.787  best_val_score: 0.802  last_epoch t=64.72s, total_time_elapsed t=543.0s\n",
      "819/819 [==============================] - 65s 79ms/step - loss: 3.2817 - val_loss: 3.8168\n",
      "Epoch 9/14\n",
      "819/819 [==============================] - ETA: 0s - loss: 3.2513Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.2513 - val_loss: 3.6428 val_score: 0.802  best_val_score: 0.802  last_epoch t=64.12s, total_time_elapsed t=607.0s\n",
      "819/819 [==============================] - 64s 78ms/step - loss: 3.2513 - val_loss: 3.6428\n",
      "Epoch 10/14\n",
      "819/819 [==============================] - ETA: 0s - loss: 3.2323Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2323 - val_loss: 3.6938 val_score: 0.799  best_val_score: 0.802  last_epoch t=65.36s, total_time_elapsed t=672.0s\n",
      "819/819 [==============================] - 65s 80ms/step - loss: 3.2323 - val_loss: 3.6938\n",
      "Epoch 11/14\n",
      "819/819 [==============================] - ETA: 0s - loss: 3.2179Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2179 - val_loss: 3.6869 val_score: 0.789  best_val_score: 0.802  last_epoch t=65.79s, total_time_elapsed t=738.0s\n",
      "819/819 [==============================] - 66s 80ms/step - loss: 3.2179 - val_loss: 3.6869\n",
      "Epoch 12/14\n",
      "819/819 [==============================] - ETA: 0s - loss: 3.1869Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.1869 - val_loss: 3.6006 val_score: 0.801  best_val_score: 0.802  last_epoch t=65.69s, total_time_elapsed t=804.0s\n",
      "819/819 [==============================] - 66s 80ms/step - loss: 3.1869 - val_loss: 3.6006\n",
      "Epoch 13/14\n",
      "818/819 [============================>.] - ETA: 0s - loss: 3.1675Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.1681 - val_loss: 3.6527 val_score: 0.793  best_val_score: 0.802  last_epoch t=64.62s, total_time_elapsed t=868.0s\n",
      "819/819 [==============================] - 65s 79ms/step - loss: 3.1681 - val_loss: 3.6527\n",
      "Epoch 14/14\n",
      "819/819 [==============================] - ETA: 0s - loss: 3.1458Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.1458 - val_loss: 3.7064 val_score: 0.793  best_val_score: 0.802  last_epoch t=64.98s, total_time_elapsed t=933.0s\n",
      "819/819 [==============================] - 65s 79ms/step - loss: 3.1458 - val_loss: 3.7064\n",
      "Model finished with val loss: 3.706366539001465\n",
      "21/21 [==============================] - 4s 76ms/step\n",
      "Val Score: 0.802324398870539\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 529/529 [00:05<00:00, 104.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------Starting fold: 10-----------\n",
      "\n",
      "\n",
      "\n",
      "Using 238 series for training and 26 series for validation\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:10<00:00, 22.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (6540, 17280, 32), y: (6540, 17280, 2), series_ids: (6540,)\n",
      "Using a split len of 17280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 34.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (664, 17280, 32), y: (664, 17280, 2), series_ids: (664,)\n",
      "Total model parameters: 36630024\n",
      "Epoch 1/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 4.2497Epoch: 000 curr_lr: 6.0e-05 - train_loss: 4.2497 - val_loss: 3.5067 val_score: 0.809  best_val_score: 0.809  last_epoch t=77.98s, total_time_elapsed t=78.0s\n",
      "818/818 [==============================] - 78s 86ms/step - loss: 4.2497 - val_loss: 3.5067\n",
      "Epoch 2/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.6427Epoch: 001 curr_lr: 6.0e-05 - train_loss: 3.6427 - val_loss: 3.4584 val_score: 0.823  best_val_score: 0.823  last_epoch t=66.67s, total_time_elapsed t=145.0s\n",
      "818/818 [==============================] - 67s 82ms/step - loss: 3.6427 - val_loss: 3.4584\n",
      "Epoch 3/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.5296Epoch: 002 curr_lr: 6.0e-05 - train_loss: 3.5296 - val_loss: 3.3690 val_score: 0.830  best_val_score: 0.830  last_epoch t=66.34s, total_time_elapsed t=211.0s\n",
      "818/818 [==============================] - 66s 81ms/step - loss: 3.5296 - val_loss: 3.3690\n",
      "Epoch 4/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.4833Epoch: 003 curr_lr: 5.7e-05 - train_loss: 3.4833 - val_loss: 3.2919 val_score: 0.830  best_val_score: 0.830  last_epoch t=65.57s, total_time_elapsed t=277.0s\n",
      "818/818 [==============================] - 66s 80ms/step - loss: 3.4833 - val_loss: 3.2919\n",
      "Epoch 5/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.4370Epoch: 004 curr_lr: 5.1e-05 - train_loss: 3.4370 - val_loss: 3.3910 val_score: 0.833  best_val_score: 0.833  last_epoch t=66.35s, total_time_elapsed t=343.0s\n",
      "818/818 [==============================] - 66s 81ms/step - loss: 3.4370 - val_loss: 3.3910\n",
      "Epoch 6/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.3928Epoch: 005 curr_lr: 4.6e-05 - train_loss: 3.3928 - val_loss: 3.2308 val_score: 0.837  best_val_score: 0.837  last_epoch t=67.07s, total_time_elapsed t=410.0s\n",
      "818/818 [==============================] - 67s 82ms/step - loss: 3.3928 - val_loss: 3.2308\n",
      "Epoch 7/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.3508Epoch: 006 curr_lr: 4.2e-05 - train_loss: 3.3508 - val_loss: 3.2275 val_score: 0.839  best_val_score: 0.839  last_epoch t=66.51s, total_time_elapsed t=477.0s\n",
      "818/818 [==============================] - 67s 81ms/step - loss: 3.3508 - val_loss: 3.2275\n",
      "Epoch 8/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.3279Epoch: 007 curr_lr: 3.7e-05 - train_loss: 3.3279 - val_loss: 3.2596 val_score: 0.834  best_val_score: 0.839  last_epoch t=64.63s, total_time_elapsed t=541.0s\n",
      "818/818 [==============================] - 65s 79ms/step - loss: 3.3279 - val_loss: 3.2596\n",
      "Epoch 9/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.3011Epoch: 008 curr_lr: 3.4e-05 - train_loss: 3.3011 - val_loss: 3.1949 val_score: 0.841  best_val_score: 0.841  last_epoch t=67.57s, total_time_elapsed t=609.0s\n",
      "818/818 [==============================] - 68s 83ms/step - loss: 3.3011 - val_loss: 3.1949\n",
      "Epoch 10/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.2778Epoch: 009 curr_lr: 3.0e-05 - train_loss: 3.2778 - val_loss: 3.3740 val_score: 0.829  best_val_score: 0.841  last_epoch t=65.44s, total_time_elapsed t=674.0s\n",
      "818/818 [==============================] - 65s 80ms/step - loss: 3.2778 - val_loss: 3.3740\n",
      "Epoch 11/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.2514Epoch: 010 curr_lr: 2.7e-05 - train_loss: 3.2514 - val_loss: 3.2674 val_score: 0.838  best_val_score: 0.841  last_epoch t=65.43s, total_time_elapsed t=740.0s\n",
      "818/818 [==============================] - 65s 80ms/step - loss: 3.2514 - val_loss: 3.2674\n",
      "Epoch 12/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.2327Epoch: 011 curr_lr: 2.5e-05 - train_loss: 3.2327 - val_loss: 3.2447 val_score: 0.838  best_val_score: 0.841  last_epoch t=65.06s, total_time_elapsed t=805.0s\n",
      "818/818 [==============================] - 65s 80ms/step - loss: 3.2327 - val_loss: 3.2447\n",
      "Epoch 13/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.2014Epoch: 012 curr_lr: 2.2e-05 - train_loss: 3.2014 - val_loss: 3.2797 val_score: 0.834  best_val_score: 0.841  last_epoch t=64.55s, total_time_elapsed t=869.0s\n",
      "818/818 [==============================] - 65s 79ms/step - loss: 3.2014 - val_loss: 3.2797\n",
      "Epoch 14/14\n",
      "818/818 [==============================] - ETA: 0s - loss: 3.1933Epoch: 013 curr_lr: 2.0e-05 - train_loss: 3.1933 - val_loss: 3.2324 val_score: 0.840  best_val_score: 0.841  last_epoch t=65.12s, total_time_elapsed t=934.0s\n",
      "818/818 [==============================] - 65s 80ms/step - loss: 3.1933 - val_loss: 3.2324\n",
      "Model finished with val loss: 3.232435941696167\n",
      "21/21 [==============================] - 4s 61ms/step\n",
      "Val Score: 0.8410048084206766\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Avg val score: 0.8235973080963038 and std val score: 0.036924956282588725\n"
     ]
    }
   ],
   "source": [
    "n_folds = 10\n",
    "fold_run_order = list(np.arange(1, n_folds+1))\n",
    "\n",
    "val_preds_lst = []\n",
    "val_series_lst = []\n",
    "val_starts_splits_lst = []\n",
    "\n",
    "val_y_lst = []\n",
    "models_lst = []\n",
    "val_scores = []\n",
    "\n",
    "model_dct = {}\n",
    "\n",
    "for fold_num in fold_run_order:\n",
    "    \n",
    "    X_s, y_s, series_ids = load_data(cfg.processed_data_path)\n",
    "    \n",
    "\n",
    "    print(f'\\n\\n\\n-----------Starting fold: {fold_num}-----------\\n\\n\\n')\n",
    "    \n",
    "    series_ids_val = splits_df.filter(pl.col(f'{n_folds}_fold') == fold_num)['series_id'].to_numpy()\n",
    "\n",
    "    val_idxs = [series_ids.index(s) for s in series_ids_val]\n",
    "    trn_idxs = np.setdiff1d(np.arange(len(series_ids)), val_idxs)\n",
    "    \n",
    "    print(f'Using {len(trn_idxs)} series for training and {len(val_idxs)} series for validation')\n",
    "\n",
    "    X_s_trn, y_s_trn, series_ids_trn = [X_s[i] for i in trn_idxs], [y_s[i] for i in trn_idxs], [series_ids[i] for i in trn_idxs]\n",
    "    X_s_val, y_s_val, series_ids_val = [X_s[i] for i in val_idxs], [y_s[i] for i in val_idxs], [series_ids[i] for i in val_idxs]\n",
    "\n",
    "    trn_ds = SleepDataset(X_s_trn, y_s_trn, series_ids_trn, cfg.samp_freq, remove_no_dets=False, is_train=True)\n",
    "    norm_params = trn_ds.norm_params\n",
    "    model_dct[fold_num] = norm_params\n",
    "    \n",
    "    val_ds = SleepDataset(X_s_val, y_s_val, series_ids_val, cfg.samp_freq, remove_no_dets=False, is_train=False, norm_params=norm_params)\n",
    "\n",
    "    del X_s_trn, y_s_trn,  X_s_val, y_s_val, series_ids_trn, X_s, y_s, series_ids\n",
    "    _ = gc.collect()\n",
    "\n",
    "\n",
    "    model = get_model(trn_ds.X.shape[1:])\n",
    "    \n",
    "    print(f'Total model parameters: {model.count_params()}')\n",
    "\n",
    "    val_events_df = train_events.filter(pl.col('series_id').is_in(series_ids_val))\n",
    "    inter_eval = IntervalEvaluation(val_ds, val_events_df, cfg.samp_freq, (trn_ds.X.shape[0]//model_cfg.batch_size)*model_cfg.n_epochs, model_cfg.start_lr, model_cfg.end_lr)\n",
    "    \n",
    "    model.fit(trn_ds.X, trn_ds.y,\n",
    "          epochs=model_cfg.n_epochs,\n",
    "          batch_size=model_cfg.batch_size,\n",
    "          callbacks=[inter_eval],\n",
    "          verbose=1,\n",
    "          validation_data=(val_ds.X, val_ds.y))\n",
    "    \n",
    "    \n",
    "    \n",
    "    last_loss = model.history.history['val_loss'][-1]\n",
    "    print(f'Model finished with val loss: {last_loss}')\n",
    "    \n",
    "    model = inter_eval.best_model\n",
    "    val_preds = model.predict(val_ds.X)\n",
    "    \n",
    "    val_score = post_process_preds(val_events_df, val_preds, val_ds.series_ids, val_ds.starts_splits, cfg.samp_freq, get_score=True)\n",
    "    print(f'Val Score: {val_score}')\n",
    "    \n",
    "    val_scores.append(val_score)\n",
    "    \n",
    "    val_y_lst.append(val_ds.y)\n",
    "    val_preds_lst.append(val_preds)\n",
    "    val_series_lst.append(val_ds.series_ids)\n",
    "    val_starts_splits_lst.append(val_ds.starts_splits)\n",
    "    \n",
    "    tf.keras.models.save_model(model, os.path.join(cfg.output_dir, cfg.ver, f'tf_model_fold_{fold_num}.h5'))\n",
    "    \n",
    "    del trn_ds, val_ds, model, inter_eval\n",
    "    _ = gc.collect()\n",
    "\n",
    "avg_val_score, std_val_score = np.mean(val_scores), np.std(val_scores)\n",
    "print(f'Avg val score: {avg_val_score} and std val score: {std_val_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ff482f",
   "metadata": {},
   "source": [
    "###### series_ids_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6d9e397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7204, 17280, 2) (7204, 17280, 2) (7204,)\n"
     ]
    }
   ],
   "source": [
    "val_starts_splits_all = np.concatenate(val_starts_splits_lst)\n",
    "val_preds_all = np.concatenate(val_preds_lst)\n",
    "val_series_all = np.concatenate(val_series_lst)\n",
    "val_y_all = np.concatenate(val_y_lst)\n",
    "\n",
    "print(val_preds_all.shape, val_y_all.shape, val_series_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "990abfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127946340, 5)\n",
      "(122539680, 5)\n",
      "(124485120, 4)\n",
      "(122539680, 4)\n"
     ]
    }
   ],
   "source": [
    "# The code is creating a DataFrame `oof_preds_df` that contains the predicted values for the \"onset\" and \"wakeup\" columns.\n",
    "\n",
    "res_steps = []\n",
    "res_preds_onsets = []\n",
    "res_preds_wakeups = []\n",
    "\n",
    "res_series_ids = []\n",
    "\n",
    "start = 0\n",
    "while start < len(val_preds_all):\n",
    "    \n",
    "    end = start+1\n",
    "    while end < len(val_preds_all) and val_series_all[end] == val_series_all[start]:\n",
    "        end += 1\n",
    "        \n",
    "    preds = val_preds_all[start:end]\n",
    "    \n",
    "    steps = np.concatenate([val_starts_splits_all[idx] + np.arange(len(val_preds_all[idx])) for idx in range(start, end)])\n",
    "    preds = preds.reshape((preds.shape[0]*preds.shape[1]), 2)\n",
    "    \n",
    "    res_preds_onsets.append(preds[:, 0])\n",
    "    res_preds_wakeups.append(preds[:, 1])\n",
    "    \n",
    "    res_steps.append(steps)\n",
    "    ser_id = val_series_all[start]\n",
    "    \n",
    "    res_series_ids.append([ser_id for _ in range(len(preds))])\n",
    "    \n",
    "    start=end\n",
    "    \n",
    "    \n",
    "oof_preds_df = pl.DataFrame().with_columns([\n",
    "    pl.Series(np.concatenate(res_series_ids)).alias('series_id'),\n",
    "    pl.Series(np.concatenate(res_steps)).alias('step'),\n",
    "    pl.Series(np.concatenate(res_preds_onsets)).alias('onset'),\n",
    "    pl.Series(np.concatenate(res_preds_wakeups)).alias('wakeup')\n",
    "\n",
    "])\n",
    "\n",
    "train_series = pl.read_parquet(cfg.train_series_path)\n",
    "print(train_series.shape)\n",
    "train_series = train_series.filter(pl.col('series_id').is_in(list(np.unique(val_series_all))))\n",
    "train_series = train_series.with_columns(pl.col('step').cast(pl.Int64))\n",
    "print(train_series.shape)\n",
    "\n",
    "\n",
    "\n",
    "print(oof_preds_df.shape)\n",
    "oof_preds_df = train_series[['series_id', 'step']].join(oof_preds_df, on=['series_id', 'step'], how='left')\n",
    "print(oof_preds_df.shape)\n",
    "\n",
    "oof_preds_df.write_parquet(os.path.join(cfg.output_dir, cfg.ver, 'oof_preds.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc0cef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r ../data_processed/{cfg.ver}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "800ba20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25e2b3dd9c3b\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 2\n",
    "# sid = val_series_all.filter(pl.col())\n",
    "plt.plot(val_y_all[i], label=\"Actual Values\")\n",
    "print(val_series_all[i])\n",
    "\n",
    "plt.plot(val_preds_all[i], label=\"Predicted Values\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2a3f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_nikhil(preds_orig, k_orig=150, max_thresh=0.05, max_count=500):\n",
    "    \n",
    "    preds=preds_orig.copy()\n",
    "    preds = np.convolve(preds, np.array([0.2, 0.6, 0.2]), mode='same')\n",
    "\n",
    "    count = 0\n",
    "    base=6.75\n",
    "    \n",
    "    k = k_orig\n",
    "    prev_max = None\n",
    "\n",
    "    scores = []\n",
    "    indices = []\n",
    "    \n",
    "    while True:\n",
    "            \n",
    "        curr_max_idx = np.argmax(preds)\n",
    "        curr_max = preds[curr_max_idx] \n",
    "        \n",
    "        if (curr_max < max_thresh) or count > max_count:\n",
    "            break\n",
    "        \n",
    "        indices.append(curr_max_idx)\n",
    "        scores.append(curr_max)\n",
    "        \n",
    "        preds[curr_max_idx] = 0\n",
    "        \n",
    "        supress_rates = np.logspace(start=0, stop=1, num=k, base=base)/base\n",
    "        supress_rates[:20] = 0\n",
    "        # supress_rates[20:] += 0.1\n",
    "\n",
    "        preds[max(curr_max_idx-k, 0):curr_max_idx] *= supress_rates[:min(k, curr_max_idx-0)][::-1]\n",
    "\n",
    "        preds[curr_max_idx+1:min(curr_max_idx+k+1, preds.shape[0])] *= supress_rates[:min(k, preds.shape[0]-curr_max_idx-1)]\n",
    "\n",
    "        prev_max = curr_max\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    return indices, scores\n",
    "\n",
    "def get_actual_preds(val_preds, val_series_ids, val_steps, type_):\n",
    "    times = []\n",
    "    series_ids = []\n",
    "    scores = []\n",
    "    scores_y = []\n",
    "    \n",
    "    for i in np.arange(len(val_preds)):\n",
    "        \n",
    "        vp_i = val_preds[i]\n",
    "        ser_id = val_series_ids[i]\n",
    "        \n",
    "        col_index = 0 if type_ == \"onset\" else 1\n",
    "        other_col_index = 1 if type_ == \"onset\" else 0\n",
    "        \n",
    "        preds = vp_i[:, col_index]\n",
    "        preds_other = vp_i[:, other_col_index]\n",
    "\n",
    "        height_thresh = 0.05\n",
    "        \n",
    "        peaks, peak_scores = nms_nikhil(preds)\n",
    "\n",
    "        times.extend(val_steps[i][peaks])\n",
    "        scores.extend(list(peak_scores))\n",
    "        series_ids.extend([ser_id] * len(peaks))\n",
    "\n",
    "    return np.array(series_ids), np.array(times), np.array(scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def post_process_preds(val_events_df, val_preds, val_series_ids, val_starts_splits, samp_freq, get_score=False):\n",
    "    \n",
    "#     val_steps = np.concatenate([val_starts_splits[idx] + np.arange(len(val_preds[idx])) for idx in range(val_preds.shape[0])])\n",
    "\n",
    "    val_res = []\n",
    "    \n",
    "    prev_ser_id = None\n",
    "    \n",
    "    res_steps = []\n",
    "    res_preds = []\n",
    "    res_series_ids = []\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(val_preds):\n",
    "        \n",
    "        end = start+1\n",
    "        while end < len(val_preds) and val_series_ids[end] == val_series_ids[start]:\n",
    "            end += 1\n",
    "            \n",
    "        preds = val_preds[start:end]\n",
    "        \n",
    "        steps = np.concatenate([val_starts_splits[idx] + np.arange(len(val_preds[idx])) for idx in range(start, end)])\n",
    "        preds = preds.reshape((preds.shape[0]*preds.shape[1]), 2)\n",
    "        \n",
    "        res_preds.append(preds)\n",
    "        res_steps.append(steps)\n",
    "        res_series_ids.append(val_series_ids[start])\n",
    "        \n",
    "        start=end\n",
    "        \n",
    "        \n",
    "#     print(len(res_series_ids), len(res_steps), len(res_preds))\n",
    "\n",
    "    series_ids_onsets, onsets,  scores_onsets = get_actual_preds(res_preds, res_series_ids, res_steps, 'onset')\n",
    "    series_ids_wakeups, wakeups,  scores_wakeups =get_actual_preds(res_preds, res_series_ids, res_steps, 'wakeup')\n",
    "    \n",
    "    \n",
    "    onset_preds = pl.DataFrame().with_columns([pl.Series(series_ids_onsets).alias('series_id'),\n",
    "                                           pl.Series(onsets).cast(pl.Int64).alias('step'),\n",
    "                                           pl.lit('onset').alias('event'),\n",
    "                                           pl.Series(scores_onsets).alias('score')])\n",
    "\n",
    "    wakeup_preds = pl.DataFrame().with_columns([pl.Series(series_ids_wakeups).alias('series_id'),\n",
    "                                               pl.Series(wakeups).cast(pl.Int64).cast(pl.Int64).alias('step'),\n",
    "                                               pl.lit('wakeup').alias('event'),\n",
    "                                               pl.Series(scores_wakeups).alias('score')])\n",
    "    \n",
    "    val_preds_df = pl.concat([onset_preds, wakeup_preds]).sort(by=['series_id', 'step'])\n",
    "    \n",
    "    if get_score:\n",
    "        toleranaces = {'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]}\n",
    "        comp_score = metric_fast.comp_scorer(\n",
    "        val_events_df,\n",
    "        val_preds_df,\n",
    "        tolerances = toleranaces,\n",
    "        )\n",
    "        return comp_score\n",
    "    \n",
    "    else:\n",
    "        return val_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10354814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8444650558831941\n",
      "0.8416758383803757\n",
      "0.7230944421667282\n",
      "0.8170324102230753\n",
      "0.8322065516148098\n",
      "0.8504343515800763\n",
      "0.8522675854574908\n",
      "0.8385581752914013\n",
      "0.8032440013867315\n",
      "0.8422204112717888\n",
      "CPU times: user 29.7 s, sys: 37 s, total: 1min 6s\n",
      "Wall time: 1min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8245198823255672, 0.03674090916266876)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "scores = []\n",
    "\n",
    "for fold_num in range(1, n_folds+1):\n",
    "    \n",
    "    test_ser_ids = list(np.unique(val_series_lst[fold_num-1]))\n",
    "\n",
    "\n",
    "    val_events_df = train_events.filter(pl.col('series_id').is_in(test_ser_ids))\n",
    "    score = post_process_preds(val_events_df,\n",
    "                               val_preds_lst[fold_num-1],\n",
    "                               val_series_lst[fold_num-1],\n",
    "                               val_starts_splits_lst[fold_num-1],\n",
    "                               cfg.samp_freq,\n",
    "                               get_score=True)\n",
    "    print(score)\n",
    "    scores.append(score)\n",
    "    \n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e911c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds_df = post_process_preds(val_events_df, val_preds_all, val_series_all, val_starts_splits_all, cfg.samp_freq, get_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b93f28c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_nearest_multiple(df, column_name='step'):\n",
    "    df[column_name] = df[column_name].apply(lambda x: x+1 if x%12==0  else x)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b63863c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds_df = round_to_nearest_multiple(val_preds_df.to_pandas())\n",
    "val_preds_df = pl.DataFrame(val_preds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02c02a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerances = {'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "               'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5dcf3c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.814316353453655"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_fast.comp_scorer(train_events, val_preds_df, tolerances=tolerances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa0b2952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262222, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bbdae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:05<00:00,  5.07it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_partitions  = val_preds_df.partition_by(by=['series_id'], maintain_order=True, as_dict=True)\n",
    "events_partitions  = val_events_df.partition_by(by=['series_id'], maintain_order=True, as_dict=True)\n",
    "\n",
    "tolerances = {'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "               'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]}\n",
    "\n",
    "scores_dict = {}\n",
    "for ser_id in tqdm(events_partitions.keys()):\n",
    "    scores_dict[ser_id] = metric_fast.comp_scorer(events_partitions[ser_id], preds_partitions[ser_id],\n",
    "                                                  tolerances)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47012ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148471991ffb    0.450931\n",
       "c535634d7dcd    0.463728\n",
       "703b5efa9bc1    0.488174\n",
       "fbf33b1a2c10    0.697022\n",
       "35826366dfc7    0.747556\n",
       "4ac356361be9    0.759743\n",
       "ece2561f07e9    0.816578\n",
       "9b9cd7b7af8c    0.818099\n",
       "1319a1935f48    0.823714\n",
       "0cd1e3d0ed95    0.836974\n",
       "f88e18cb4100    0.868700\n",
       "eef041dd50aa    0.869135\n",
       "fcca183903b7    0.879574\n",
       "ce85771a714c    0.883841\n",
       "3be1545083b7    0.887776\n",
       "137771d19ca2    0.891360\n",
       "0ef7d94fde99    0.892744\n",
       "8b159a98f485    0.914702\n",
       "40dce6018935    0.916484\n",
       "9ddd40f2cb36    0.920464\n",
       "3df0da2e5966    0.925641\n",
       "d8de352c2657    0.934613\n",
       "927dd0c35dfd    0.936609\n",
       "2f7504d0f426    0.948389\n",
       "72ba4a8afff4    0.963179\n",
       "c7d693f24684    0.970733\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(scores_dict).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6a12ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_attributes_dict = {k: v for k, v in cfg.__dict__.items() if not k.startswith('__') and not callable(v)}\n",
    "joblib.dump(cfg_attributes_dict, os.path.join(cfg.output_dir, cfg.ver, 'cfg.pkl'))\n",
    "joblib.dump(model_dct, os.path.join(cfg.output_dir, cfg.ver, 'model_dct.pkl'))\n",
    "\n",
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_series_all.npy'), val_series_all)\n",
    "\n",
    "\n",
    "val_preds_df.to_pandas().to_csv(os.path.join(cfg.output_dir, cfg.ver, 'val_preds_df.csv'), index=False)\n",
    "\n",
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_preds_all.npy'), val_preds_all)\n",
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_y_all.npy'), val_y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89201dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "meta_json = {\n",
    "  \"title\": f\"sleep-model-{cfg.ver}\",\n",
    "  \"id\": f\"nikhilmishradev/sleep-model-{cfg.ver}\",\n",
    "  \"licenses\": [\n",
    "    {\n",
    "      \"name\": \"CC0-1.0\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "json.dump(meta_json, open(os.path.join(cfg.output_dir, cfg.ver, 'dataset-metadata.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "482e8991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/.kaggle/kaggle.json'\n",
      "Skipping folder: .ipynb_checkpoints; use '--dir-mode' to upload folders\n",
      "Starting upload for file cfg.pkl\n",
      "100%|███████████████████████████████████████████| 119/119 [00:02<00:00, 49.1B/s]\n",
      "Upload successful: cfg.pkl (119B)\n",
      "Starting upload for file model_dct.pkl\n",
      "100%|██████████████████████████████████████| 4.00k/4.00k [00:02<00:00, 1.98kB/s]\n",
      "Upload successful: model_dct.pkl (4KB)\n",
      "Starting upload for file oof_preds.parquet\n",
      "100%|██████████████████████████████████████| 1.02G/1.02G [00:58<00:00, 18.6MB/s]\n",
      "Upload successful: oof_preds.parquet (1GB)\n",
      "Starting upload for file tf_model_fold_1.h5\n",
      "100%|████████████████████████████████████████| 140M/140M [00:10<00:00, 14.1MB/s]\n",
      "Upload successful: tf_model_fold_1.h5 (140MB)\n",
      "Starting upload for file tf_model_fold_10.h5\n",
      "100%|████████████████████████████████████████| 140M/140M [00:11<00:00, 13.0MB/s]\n",
      "Upload successful: tf_model_fold_10.h5 (140MB)\n",
      "Starting upload for file tf_model_fold_2.h5\n",
      "100%|████████████████████████████████████████| 140M/140M [00:11<00:00, 13.1MB/s]\n",
      "Upload successful: tf_model_fold_2.h5 (140MB)\n",
      "Starting upload for file tf_model_fold_3.h5\n",
      "100%|████████████████████████████████████████| 140M/140M [00:10<00:00, 14.5MB/s]\n",
      "Upload successful: tf_model_fold_3.h5 (140MB)\n",
      "Starting upload for file tf_model_fold_4.h5\n",
      "100%|████████████████████████████████████████| 140M/140M [00:11<00:00, 12.4MB/s]\n",
      "Upload successful: tf_model_fold_4.h5 (140MB)\n",
      "Starting upload for file tf_model_fold_5.h5\n",
      "100%|████████████████████████████████████████| 140M/140M [00:10<00:00, 13.9MB/s]\n",
      "Upload successful: tf_model_fold_5.h5 (140MB)\n",
      "Starting upload for file tf_model_fold_6.h5\n",
      "100%|████████████████████████████████████████| 140M/140M [00:11<00:00, 13.1MB/s]\n",
      "Upload successful: tf_model_fold_6.h5 (140MB)\n",
      "Starting upload for file tf_model_fold_7.h5\n",
      "100%|████████████████████████████████████████| 140M/140M [00:10<00:00, 13.5MB/s]\n",
      "Upload successful: tf_model_fold_7.h5 (140MB)\n",
      "Starting upload for file tf_model_fold_8.h5\n",
      "100%|████████████████████████████████████████| 140M/140M [00:11<00:00, 13.4MB/s]\n",
      "Upload successful: tf_model_fold_8.h5 (140MB)\n",
      "Starting upload for file tf_model_fold_9.h5\n",
      "100%|████████████████████████████████████████| 140M/140M [00:11<00:00, 13.3MB/s]\n",
      "Upload successful: tf_model_fold_9.h5 (140MB)\n",
      "Starting upload for file val_preds_df.csv\n",
      "100%|██████████████████████████████████████| 11.4M/11.4M [00:04<00:00, 2.79MB/s]\n",
      "Upload successful: val_preds_df.csv (11MB)\n",
      "Starting upload for file val_series_all.npy\n",
      "100%|█████████████████████████████████████████| 338k/338k [00:02<00:00, 145kB/s]\n",
      "Upload successful: val_series_all.npy (338KB)\n",
      "Your private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/nikhilmishradev/sleep-model-fm-v20-final\n"
     ]
    }
   ],
   "source": [
    "# !rm -r ../outputs/vx/*\n",
    "# !cp -r {os.path.join(cfg.output_dir, cfg.ver)}/* ../outputs/vx\n",
    "# !rm ../outputs/vx/val_preds_all.npy ../outputs/vx/val_y_all.npy\n",
    "# !pip install -q kaggle\n",
    "# !kaggle datasets create -p ../outputs/vx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4ed9dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_preds_all.npy'), val_preds_all)\n",
    "np.save(os.path.join(cfg.output_dir, cfg.ver, 'val_y_all.npy'), val_y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61dd1def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fm-v20-final'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf4d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
