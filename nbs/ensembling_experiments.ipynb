{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna\n",
    "# import optuna\n",
    "# optuna.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from tqdm import tqdm\n",
    "from src import metric_fast\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_events_path': '../data/train_events.csv', 'train_series_path': '../data/train_series.parquet', 'processed_data_path': '../data_processed_models', 'output_dir': '../outputs'}\n",
      "{'__module__': '__main__', '__dict__': <attribute '__dict__' of 'cfg' objects>, '__weakref__': <attribute '__weakref__' of 'cfg' objects>, '__doc__': None, 'train_events_path': '../data/train_events.csv', 'train_series_path': '../data/train_series.parquet', 'processed_data_path': '../data_processed_models', 'output_dir': '../outputs'}\n"
     ]
    }
   ],
   "source": [
    "settings_json = json.load(open('../settings.json', 'r'))\n",
    "print(settings_json)\n",
    "\n",
    "for k,v in settings_json.items():\n",
    "    setattr(cfg, k, v)\n",
    "    \n",
    "print(cfg.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not mmap compressed IPC file, defaulting to normal read. Toggle off 'memory_map' to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "train_events = pl.read_ipc(os.path.join(settings_json['processed_data_path'], 'train_events.ipc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_events['series_id'].n_unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(os.path.join(cfg.output_dir, 'fm-v13-final', 'oof_preds.parquet'))\n",
    "df_v15 = pl.read_parquet(os.path.join(cfg.output_dir, 'fm-v15-final', 'oof_preds.parquet'))\n",
    "df_v20 = pl.read_parquet(os.path.join(cfg.output_dir, 'fm-v20-final', 'oof_preds.parquet'))\n",
    "df_v21 = pl.read_parquet(os.path.join(cfg.output_dir, 'fm-v21-final', 'oof_preds.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v15 = df[['series_id', 'step']].join(df_v15, on=['series_id', 'step'], how='left')\n",
    "df_v20 = df[['series_id', 'step']].join(df_v20, on=['series_id', 'step'], how='left')\n",
    "df_v21 = df[['series_id', 'step']].join(df_v21, on=['series_id', 'step'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns([pl.Series(((df[c]+ df_v15[c] + df_v20[c] + df_v21[c])/4), dtype=pl.Float32).alias(c) for c in ['onset', 'wakeup']])\n",
    "\n",
    "df = df.rename({'onset': 'onset_oof', 'wakeup': 'wakeup_oof'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_array(arr, new_min, new_max):\n",
    "    \"\"\"\n",
    "    Scale a numpy array to a new given range [new_min, new_max].\n",
    "\n",
    "    :param arr: numpy array to be scaled\n",
    "    :param new_min: new minimum value of the range\n",
    "    :param new_max: new maximum value of the range\n",
    "    :return: scaled numpy array\n",
    "    \"\"\"\n",
    "    min_val = np.min(arr)\n",
    "    max_val = np.max(arr)\n",
    "    \n",
    "    # Scale the array\n",
    "    scaled_arr = ((arr - min_val) / (max_val - min_val)) * (new_max - new_min) + new_min\n",
    "    \n",
    "    return scaled_arr\n",
    "\n",
    "\n",
    "def scale(arr, new_min, new_max, power=1.25):\n",
    "    \n",
    "    \n",
    "    arr = scale_array(arr, new_min, new_max)\n",
    "    arr = arr**power\n",
    "    arr = scale_array(arr, new_min, new_max)\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns([pl.Series(scale(df[c].to_numpy(), 0, 15, 1)).alias(f'{c}_t') for c in ['wakeup_oof', 'onset_oof']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.7398790717124939 15.790359497070312\n"
     ]
    }
   ],
   "source": [
    "print(df['onset_oof'].min(), df['wakeup_oof'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 15.0\n"
     ]
    }
   ],
   "source": [
    "print(df['onset_oof_t'].min(), df['wakeup_oof_t'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264/264 [00:00<00:00, 590.31it/s]\n"
     ]
    }
   ],
   "source": [
    "res_series_ids = []\n",
    "res_steps = []\n",
    "res_preds = []\n",
    "\n",
    "partitions = df.partition_by(by='series_id', maintain_order=True)\n",
    "for df_sub in tqdm(partitions):\n",
    "    series_id = df_sub['series_id'][0]\n",
    "    res_series_ids.append(series_id)\n",
    "    res_steps.append(df_sub['step'].to_numpy())\n",
    "    res_preds.append(df_sub[['onset_oof_t', 'wakeup_oof_t']].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wbf_nikhil(preds_orig, max_thresh=0.1, max_count=600, hyperparams=None):\n",
    "    k_dist = hyperparams['k_dist']\n",
    "    log_base = hyperparams['log_base']\n",
    "    log_scale = hyperparams['log_scale']\n",
    "    curr_max_power = hyperparams['curr_max_power']\n",
    "    weight_coeff = hyperparams['weight_coeff']\n",
    "    convolution_kernel = hyperparams['convolution_kernel']\n",
    "    section_weight_method = hyperparams['section_weight_method']\n",
    "    preds_reduction_power = hyperparams['preds_reduction_power']\n",
    "    overlap_coeff = hyperparams['overlap_coeff']\n",
    "    min_distance = hyperparams['min_distance']\n",
    "    \n",
    "    preds = preds_orig.copy()\n",
    "    preds = np.convolve(preds, convolution_kernel, mode='same')\n",
    "\n",
    "    count = 0\n",
    "    indices = []\n",
    "    scores = []\n",
    "\n",
    "    while count < max_count:\n",
    "        curr_max_idx = np.argmax(preds)\n",
    "        curr_max = preds[curr_max_idx]\n",
    "\n",
    "        if curr_max < max_thresh:\n",
    "            break\n",
    "\n",
    "        k = int(k_dist - max(min_distance, (curr_max**curr_max_power)))\n",
    "\n",
    "        start_idx = max(curr_max_idx - k, 0)\n",
    "        end_idx = min(curr_max_idx + k + 1, len(preds))\n",
    "\n",
    "        section = preds[start_idx:end_idx]\n",
    "\n",
    "        # Different weight calculation methods\n",
    "        distances = np.abs(np.arange(len(section)) - k)\n",
    "        if section_weight_method == 'logarithmic':\n",
    "            weights = 1 / (log_base ** (distances / (k * log_scale)))\n",
    "        elif section_weight_method == 'linear':\n",
    "            weights = 1 - (distances / k) * weight_coeff\n",
    "        # Add more methods as needed\n",
    "\n",
    "        weighted_avg = np.sum(section * weights) / np.sum(weights)\n",
    "\n",
    "        scores.append(weighted_avg)\n",
    "        indices.append(curr_max_idx)\n",
    "\n",
    "        preds[start_idx:end_idx] *= ((1 - weights * overlap_coeff))**preds_reduction_power\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    return indices, scores\n",
    "\n",
    "def get_actual_preds(val_preds, val_series_ids, val_steps, type_, hyperparams):\n",
    "    times = []\n",
    "    series_ids = []\n",
    "    scores = []\n",
    "\n",
    "    for i in np.arange(len(val_preds)):\n",
    "        \n",
    "        vp_i = val_preds[i]\n",
    "        ser_id = val_series_ids[i]\n",
    "        \n",
    "        col_index = 0 if type_ == \"onset\" else 1\n",
    "\n",
    "        preds = vp_i[:, col_index] \n",
    "        peaks, peak_scores = wbf_nikhil(preds, hyperparams=hyperparams)\n",
    "    # ...\n",
    "\n",
    "        times.extend(val_steps[i][peaks])\n",
    "        scores.extend(list(peak_scores))\n",
    "        series_ids.extend([ser_id] * len(peaks))\n",
    "\n",
    "    return np.array(series_ids), np.array(times), np.array(scores)\n",
    "\n",
    "def post_process_preds(val_events_df, res_series_ids, res_preds, res_steps, hyperparams, get_score=False):\n",
    "\n",
    "    series_ids_onsets, onsets, scores_onsets = get_actual_preds(res_preds, res_series_ids, res_steps, 'onset', hyperparams)\n",
    "    series_ids_wakeups, wakeups, scores_wakeups = get_actual_preds(res_preds, res_series_ids, res_steps, 'wakeup', hyperparams)\n",
    "    \n",
    "    \n",
    "    onset_preds = pl.DataFrame().with_columns([pl.Series(series_ids_onsets).alias('series_id'),\n",
    "                                           pl.Series(onsets).cast(pl.Int64).alias('step'),\n",
    "                                           pl.lit('onset').alias('event'),\n",
    "                                           pl.Series(scores_onsets).alias('score')])\n",
    "\n",
    "    wakeup_preds = pl.DataFrame().with_columns([pl.Series(series_ids_wakeups).alias('series_id'),\n",
    "                                               pl.Series(wakeups).cast(pl.Int64).cast(pl.Int64).alias('step'),\n",
    "                                               pl.lit('wakeup').alias('event'),\n",
    "                                               pl.Series(scores_wakeups).alias('score')])\n",
    "    \n",
    "    val_preds_df = pl.concat([onset_preds, wakeup_preds]).sort(by=['series_id', 'step'])\n",
    "    \n",
    "    if get_score:\n",
    "        toleranaces = {'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]}\n",
    "        comp_score = metric_fast.comp_scorer(\n",
    "        val_events_df,\n",
    "        val_preds_df,\n",
    "        tolerances = toleranaces,\n",
    "        )\n",
    "        return comp_score\n",
    "    \n",
    "    else:\n",
    "        return val_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8230877867171313"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'k_dist': 265,\n",
    " 'log_base': 14,\n",
    " 'log_scale': 0.4790984168498803,\n",
    " 'curr_max_power': 0.7379097334221248,\n",
    " 'weight_coeff': 1.4190240522794364,\n",
    " 'score_thresh_coeff': 1.3935907064707698,\n",
    " 'convolution_kernel': [0.15, 0.7, 0.15],\n",
    " 'section_weight_method': 'logarithmic',\n",
    " 'preds_reduction_power': 4.2185511941820355,\n",
    " 'overlap_coeff': 0.6879294472420623,\n",
    " 'min_distance': 21}\n",
    "post_process_preds(train_events, res_series_ids, res_preds, res_steps, params, get_score=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
